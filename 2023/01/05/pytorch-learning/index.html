<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>pytorch_learning | HaomingX的博客</title><meta name="keywords" content="pytorch"><meta name="author" content="HaomingX"><meta name="copyright" content="HaomingX"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本文参考：https:&#x2F;&#x2F;www.pytorchmaster.com&#x2F; torch函数torch.cat()函数目的： 在给定维度上对输入的张量序列seq 进行连接操作。 outputs &#x3D; torch.cat(inputs, dim&#x3D;?) → Tensor 参数  inputs : 待连接的张量序列，可以是任意相同Tensor类型的python 序列 dim : 选择的扩维, 必须在0到len(">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch_learning">
<meta property="og:url" content="https://haomingx.github.io/2023/01/05/pytorch-learning/index.html">
<meta property="og:site_name" content="HaomingX的博客">
<meta property="og:description" content="本文参考：https:&#x2F;&#x2F;www.pytorchmaster.com&#x2F; torch函数torch.cat()函数目的： 在给定维度上对输入的张量序列seq 进行连接操作。 outputs &#x3D; torch.cat(inputs, dim&#x3D;?) → Tensor 参数  inputs : 待连接的张量序列，可以是任意相同Tensor类型的python 序列 dim : 选择的扩维, 必须在0到len(">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209210056384.jpg">
<meta property="article:published_time" content="2023-01-05T02:51:16.000Z">
<meta property="article:modified_time" content="2023-02-05T11:06:01.477Z">
<meta property="article:author" content="HaomingX">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209210056384.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://haomingx.github.io/2023/01/05/pytorch-learning/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch_learning',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-05 19:06:01'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209202359712.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-frog"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fas fa-bug"></i><span> 自言自语</span></a></li><li><a class="site-page child" href="/message/"><i class="fa-fw fas fa-sms"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">HaomingX的博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-frog"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fas fa-bug"></i><span> 自言自语</span></a></li><li><a class="site-page child" href="/message/"><i class="fa-fw fas fa-sms"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">pytorch_learning</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-01-05T02:51:16.000Z" title="发表于 2023-01-05 10:51:16">2023-01-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-02-05T11:06:01.477Z" title="更新于 2023-02-05 19:06:01">2023-02-05</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="pytorch_learning"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><p>本文参考：<a target="_blank" rel="noopener" href="https://www.pytorchmaster.com/">https://www.pytorchmaster.com/</a></p>
<h3 id="torch函数"><a href="#torch函数" class="headerlink" title="torch函数"></a>torch函数</h3><h4 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat()"></a>torch.cat()</h4><p>函数目的： 在给定维度上对输入的张量序列seq 进行连接操作。</p>
<p><code>outputs = torch.cat(inputs, dim=?) → Tensor</code></p>
<p><strong>参数</strong></p>
<ul>
<li>inputs : 待连接的张量序列，可以是任意相同<code>Tensor</code>类型的python 序列</li>
<li>dim : 选择的扩维, 必须在<code>0</code>到<code>len(inputs[0])</code>之间，沿着此维连接张量序列。</li>
</ul>
<p><strong>重点</strong></p>
<ol>
<li>输入数据必须是序列，序列中数据是任意相同的<code>shape</code>的同类型<code>tensor</code></li>
<li>维度不可以超过输入数据的任一个张量的维度</li>
</ol>
<p><strong>例子</strong></p>
<ol>
<li>准备数据，每个的shape都是[2,3]</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x1 = torch.tensor([[<span class="number">11</span>,<span class="number">21</span>,<span class="number">31</span>],[<span class="number">21</span>,<span class="number">31</span>,<span class="number">41</span>]],dtype=torch.<span class="built_in">int</span>)</span><br><span class="line">x1.shape <span class="comment"># torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x2 = torch.tensor([[<span class="number">12</span>,<span class="number">22</span>,<span class="number">32</span>],[<span class="number">22</span>,<span class="number">32</span>,<span class="number">42</span>]],dtype=torch.<span class="built_in">int</span>)</span><br><span class="line">x2.shape  <span class="comment"># torch.Size([2, 3])</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>合成inputs</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27; inputs为２个形状为[2 , 3]的矩阵 &#x27;</span></span><br><span class="line">inputs = [x1, x2]</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br><span class="line"><span class="string">&#x27;打印查看&#x27;</span></span><br><span class="line">[tensor([[<span class="number">11</span>, <span class="number">21</span>, <span class="number">31</span>],</span><br><span class="line">         [<span class="number">21</span>, <span class="number">31</span>, <span class="number">41</span>]], dtype=torch.int32),</span><br><span class="line"> tensor([[<span class="number">12</span>, <span class="number">22</span>, <span class="number">32</span>],</span><br><span class="line">         [<span class="number">22</span>, <span class="number">32</span>, <span class="number">42</span>]], dtype=torch.int32)]</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>查看结果, 测试不同的dim拼接结果</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In    [<span class="number">1</span>]: torch.cat(inputs, dim=<span class="number">0</span>).shape</span><br><span class="line">Out   [<span class="number">1</span>]: torch.Size([<span class="number">4</span>,  <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">In    [<span class="number">2</span>]: torch.cat(inputs, dim=<span class="number">1</span>).shape</span><br><span class="line">Out   [<span class="number">2</span>]: torch.Size([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">In    [<span class="number">3</span>]: torch.cat(inputs, dim=<span class="number">2</span>).shape</span><br><span class="line">IndexError: Dimension out of <span class="built_in">range</span> (expected to be <span class="keyword">in</span> <span class="built_in">range</span> of [-<span class="number">2</span>, <span class="number">1</span>], but got <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h4 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack()"></a>torch.stack()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">T1 = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">T2 = torch.tensor([[<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>],</span><br><span class="line">                [<span class="number">40</span>, <span class="number">50</span>, <span class="number">60</span>],</span><br><span class="line">                [<span class="number">70</span>, <span class="number">80</span>, <span class="number">90</span>]])</span><br><span class="line"></span><br><span class="line">T3 = torch.stack((T1,T2),dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(T3.shape)</span><br><span class="line"><span class="built_in">print</span>(T3)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([2, 3, 3])</span></span><br><span class="line"><span class="string">tensor([[[ 1,  2,  3],</span></span><br><span class="line"><span class="string">         [ 4,  5,  6],</span></span><br><span class="line"><span class="string">         [ 7,  8,  9]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[10, 20, 30],</span></span><br><span class="line"><span class="string">         [40, 50, 60],</span></span><br><span class="line"><span class="string">         [70, 80, 90]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">T4 = torch.stack((T1,T2),dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(T4.shape)</span><br><span class="line"><span class="built_in">print</span>(T4)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([3, 2, 3])</span></span><br><span class="line"><span class="string">tensor([[[ 1,  2,  3],</span></span><br><span class="line"><span class="string">         [10, 20, 30]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 4,  5,  6],</span></span><br><span class="line"><span class="string">         [40, 50, 60]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 7,  8,  9],</span></span><br><span class="line"><span class="string">         [70, 80, 90]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">T5 = torch.stack((T1,T2),dim=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(T5.shape)</span><br><span class="line"><span class="built_in">print</span>(T5)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([3, 3, 2])</span></span><br><span class="line"><span class="string">tensor([[[ 1, 10],</span></span><br><span class="line"><span class="string">         [ 2, 20],</span></span><br><span class="line"><span class="string">         [ 3, 30]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 4, 40],</span></span><br><span class="line"><span class="string">         [ 5, 50],</span></span><br><span class="line"><span class="string">         [ 6, 60]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 7, 70],</span></span><br><span class="line"><span class="string">         [ 8, 80],</span></span><br><span class="line"><span class="string">         [ 9, 90]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h4 id="torch-ones-like"><a href="#torch-ones-like" class="headerlink" title="torch.ones_like()"></a>torch.ones_like()</h4><h4 id="torch-zeros-like"><a href="#torch-zeros-like" class="headerlink" title="torch.zeros_like()"></a>torch.zeros_like()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># 生成与input形状相同、元素全为1的张量</span></span><br><span class="line">a = torch.ones_like(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 生成与input形状相同、元素全为0的张量</span></span><br><span class="line">b = torch.zeros_like(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.1362, 0.6439, 0.3817],</span></span><br><span class="line"><span class="string">        [0.0971, 0.3498, 0.8780]])</span></span><br><span class="line"><span class="string">tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]])</span></span><br><span class="line"><span class="string">tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h4 id="torch-range"><a href="#torch-range" class="headerlink" title="torch.range()"></a>torch.range()</h4><h4 id="torch-arange"><a href="#torch-arange" class="headerlink" title="torch.arange()"></a>torch.arange()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y=torch.<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.dtype</span><br><span class="line">torch.float32</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z=torch.arange(<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.dtype</span><br><span class="line">torch.int64</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<p>torch.range必须有begin和end值</p>
<p>但是torch.arange可以只有单值或设置间隔值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; torch.arange(<span class="number">4</span>)</span><br><span class="line">&gt;&gt; tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">&gt;&gt; torch.arange(<span class="number">1</span>,<span class="number">0.6</span>,-<span class="number">0.1</span>)</span><br><span class="line">&gt;&gt; tensor([<span class="number">1.0000</span>, <span class="number">0.9000</span>, <span class="number">0.8000</span>, <span class="number">0.7000</span>])</span><br></pre></td></tr></table></figure>

<h4 id="torch-normal"><a href="#torch-normal" class="headerlink" title="torch.normal()"></a>torch.normal()</h4><p>原型：<code>normal(mean, std, *, generator=None, out=None)</code></p>
<p>该函数返回从单独的<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83&spm=1001.2101.3001.7020">正态分布</a>中提取的随机数的张量，该正态分布的均值是mean，标准差是std。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.normal(mean=torch.arange(<span class="number">4.</span>),std=torch.arange(<span class="number">1.</span>,<span class="number">0.6</span>,-<span class="number">0.1</span>)).reshape(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[-1.4455,  0.9446],</span></span><br><span class="line"><span class="string">        [ 3.2138,  3.3914]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h3><h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><p>基本上趋同于numpy.array，但是<strong>不支持str</strong>!!!   包括：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.float64(torch.double),</span><br><span class="line">torch.float32(torch.<span class="built_in">float</span>),</span><br><span class="line">torch.float16,</span><br><span class="line">torch.int64(torch.long),</span><br><span class="line">torch.int32(torch.<span class="built_in">int</span>),</span><br><span class="line">torch.int16,</span><br><span class="line">torch.int8,</span><br><span class="line">torch.uint8,</span><br><span class="line">torch.<span class="built_in">bool</span></span><br></pre></td></tr></table></figure>

<p>一般神经网络都是用<code>torch.float32</code>类型</p>
<p>几种构造方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动推断</span></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>)</span><br><span class="line"><span class="built_in">print</span>(a,a.dtype)</span><br><span class="line"><span class="comment"># tensor(2.) torch.float32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定数据类型</span></span><br><span class="line">b = torch.tensor(<span class="number">2.0</span>, dtype=torch.double)</span><br><span class="line"><span class="built_in">print</span>(b,b.dtype)</span><br><span class="line"><span class="comment"># tensor(2.,dtype=torch.float64) torch.float64</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用特定构造函数 &amp; numpt转tensor</span></span><br><span class="line">c = torch.IntTensor(<span class="number">1</span>)</span><br><span class="line">d = torch.Tensor(np.array(<span class="number">2.0</span>))  <span class="comment">#等价于torch.FloatTensor</span></span><br><span class="line">e = torch.BoolTensor(np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(c,c.dtype)</span><br><span class="line"><span class="built_in">print</span>(d,d.dtype)</span><br><span class="line"><span class="built_in">print</span>(e,e.dtype)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([5], dtype=torch.int32) torch.int32</span></span><br><span class="line"><span class="string">tensor(2.) torch.float32</span></span><br><span class="line"><span class="string">tensor([ True, False,  True, False]) torch.bool</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同类型转换</span></span><br><span class="line">i = torch.tensor(<span class="number">1</span>); <span class="built_in">print</span>(i,i.dtype)</span><br><span class="line">x = i.<span class="built_in">float</span>(); <span class="built_in">print</span>(x,x.dtype) <span class="comment">#调用 float方法转换成浮点类型</span></span><br><span class="line">y = i.<span class="built_in">type</span>(torch.<span class="built_in">float</span>); <span class="built_in">print</span>(y,y.dtype) <span class="comment">#使用type函数转换成浮点类型</span></span><br><span class="line">z = i.type_as(x);<span class="built_in">print</span>(z,z.dtype) <span class="comment">#使用type_as方法转换成某个Tensor相同类型</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(1) torch.int64</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h4 id="张量的维度"><a href="#张量的维度" class="headerlink" title="张量的维度"></a>张量的维度</h4><p>不同类型数据会是不同的维度，<strong>有几层中括号就是多少维张量</strong></p>
<hr>
<p>标量为0维张量，向量为1维张量，矩阵为2维张量，彩色图像有rgb三个通道为3维张量，视频还有时间维表示为4维张量。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scalar = torch.tensor(<span class="literal">True</span>) <span class="comment"># 0维</span></span><br><span class="line">vector = torch.tensor([<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>]) <span class="comment">#向量，1维张量</span></span><br><span class="line">matrix = torch.tensor([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]]) <span class="comment">#矩阵, 2维张量</span></span><br><span class="line">tensor3 = torch.tensor([[[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]],[[<span class="number">5.0</span>,<span class="number">6.0</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]]])  <span class="comment"># 3维张量</span></span><br><span class="line">tensor4 = torch.tensor([[[[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">2.0</span>,<span class="number">2.0</span>]],[[<span class="number">3.0</span>,<span class="number">3.0</span>],[<span class="number">4.0</span>,<span class="number">4.0</span>]]],</span><br><span class="line">                        [[[<span class="number">5.0</span>,<span class="number">5.0</span>],[<span class="number">6.0</span>,<span class="number">6.0</span>]],[[<span class="number">7.0</span>,<span class="number">7.0</span>],[<span class="number">8.0</span>,<span class="number">8.0</span>]]]])  <span class="comment"># 4维张量</span></span><br></pre></td></tr></table></figure>

<h4 id="张量的尺寸"><a href="#张量的尺寸" class="headerlink" title="张量的尺寸"></a>张量的尺寸</h4><ul>
<li>使用<code>shape</code>属性或者<code>size()</code>方法查看张量在每一维的长度</li>
<li>使用<code>view()</code>方法改变张量的尺寸,view()和numpy中的reshape很像，所以<strong>view()失败可以直接使用reshape</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有些操作会让张量存储结构扭曲，直接使用view会失败，可以用reshape方法</span></span><br><span class="line">matrix26 = torch.arange(<span class="number">0</span>,<span class="number">12</span>).view(<span class="number">2</span>,<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(matrix26)       <span class="comment"># tensor([[ 0,  1,  2,  3,  4,  5],</span></span><br><span class="line">        			  <span class="comment">#			[ 6,  7,  8,  9, 10, 11]])</span></span><br><span class="line"><span class="built_in">print</span>(matrix26.shape) <span class="comment"># torch.Size([2, 6])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转置操作让张量存储结构扭曲</span></span><br><span class="line">matrix62 = matrix26.t()</span><br><span class="line"><span class="built_in">print</span>(matrix62.is_contiguous()) <span class="comment"># False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接使用view方法会失败，可以使用reshape方法</span></span><br><span class="line"><span class="comment">#matrix34 = matrix62.view(3,4) #error!</span></span><br><span class="line">matrix34 = matrix62.reshape(<span class="number">3</span>,<span class="number">4</span>) <span class="comment">#等价于matrix34 = matrix62.contiguous().view(3,4)</span></span><br><span class="line"><span class="built_in">print</span>(matrix34) </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 0,  6,  1,  7],</span></span><br><span class="line"><span class="string">        [ 2,  8,  3,  9],</span></span><br><span class="line"><span class="string">        [ 4, 10,  5, 11]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p><strong>is_contiguous()：Tensor底层一维数组元素的存储顺序与Tensor按行优先一维展开的元素顺序是否一致</strong></p>
<p>行有限列有限博客：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/64551412">https://zhuanlan.zhihu.com/p/64551412</a></p>
<h4 id="张量和numpy数组"><a href="#张量和numpy数组" class="headerlink" title="张量和numpy数组"></a>张量和numpy数组</h4><p>numpy –&gt; tensor</p>
<ul>
<li>用numpy方法从Tensor得到numpy数组</li>
<li>用torch.from_numpy从numpy数组得到Tensor</li>
</ul>
<p>这两种方法关联的tensor和numpy数组是共享数据内存的，改变一个也会改变另一个。当然需要的话可以通过张量的<code>clone</code>方法拷贝张量中断这种关联。</p>
<p>此外可以使用<code>item</code>方法从标量张量得到对应的Python数值</p>
<p>​				使用<code>tolist</code>方法从张量得到对应的Python数值列表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ndarray-&gt;tensor</span></span><br><span class="line">arr = np.zeros(<span class="number">3</span>)</span><br><span class="line">tensor = torch.from_numpy(arr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor-&gt;ndarray</span></span><br><span class="line">tensor = torch.zeros(<span class="number">3</span>)</span><br><span class="line">arr = tensor.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以用clone() 方法拷贝张量，中断这种关联</span></span><br><span class="line">tensor = torch.zeros(<span class="number">3</span>)</span><br><span class="line">arr = tensor.clone().numpy() <span class="comment"># 也可以使用tensor.data.numpy()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># item方法和tolist方法可以将张量转换成Python数值和数值列表</span></span><br><span class="line">scalar = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">s = scalar.item()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(s),<span class="string">&#x27; &#x27;</span>,s)</span><br><span class="line"><span class="comment"># &lt;class &#x27;float&#x27;&gt; 1.0</span></span><br><span class="line"></span><br><span class="line">tensor = torch.rand(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">t = tensor.tolist()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(t),<span class="string">&#x27; &#x27;</span>,t)</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;list&#x27;</span>&gt; [[<span class="number">0.8211846351623535</span>, <span class="number">0.20020723342895508</span>], [<span class="number">0.011571824550628662</span>, <span class="number">0.2906131148338318</span>]]</span><br></pre></td></tr></table></figure>

<h3 id="autograd自动微分"><a href="#autograd自动微分" class="headerlink" title="autograd自动微分"></a>autograd自动微分</h3><p>神经网络通常依靠反向传播求梯度更新网络参数，pytorch通过反向传播backward方法实现梯度计算，可以调用torch.autograd.grad函数来实现梯度计算，这就是Pytorch的自动微分机制</p>
<h4 id="backward求导数"><a href="#backward求导数" class="headerlink" title="backward求导数"></a>backward求导数</h4><p>backward 方法通常在一个标量张量上调用，该方法求得的梯度将存在对应自变量张量的grad属性下。</p>
<p>如果调用的张量非标量，则要传入一个和它同形状 的gradient参数张量。</p>
<p>相当于用该gradient参数张量与调用张量作向量点乘，得到的标量结果再反向传播。</p>
<ol>
<li>标量的反向传播</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line">dy_dx = x.grad</span><br><span class="line"><span class="built_in">print</span>(dy_dx) <span class="comment"># tensor(-2.)</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>非标量的反向传播</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c</span></span><br><span class="line">x = torch.tensor([[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">1.0</span>,<span class="number">2.0</span>]],requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">gradient = torch.tensor([[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x:\n&quot;</span>,x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y:\n&quot;</span>,y)</span><br><span class="line">y.backward(gradient = gradient)</span><br><span class="line">x_grad = x.grad</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_grad:\n&quot;</span>,x_grad)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x:</span></span><br><span class="line"><span class="string"> tensor([[0., 0.],</span></span><br><span class="line"><span class="string">        [1., 2.]], requires_grad=True)</span></span><br><span class="line"><span class="string">y:</span></span><br><span class="line"><span class="string"> tensor([[1., 1.],</span></span><br><span class="line"><span class="string">        [0., 1.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">x_grad:</span></span><br><span class="line"><span class="string"> tensor([[-2., -2.],</span></span><br><span class="line"><span class="string">        [ 0.,  2.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>非标量的反向传播可以用标量的反向传播实现</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor([[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">1.0</span>,<span class="number">2.0</span>]],requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">gradient = torch.tensor([[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line">z = torch.<span class="built_in">sum</span>(y*gradient)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x:&quot;</span>,x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y:&quot;</span>,y)</span><br><span class="line">z.backward()</span><br><span class="line">x_grad = x.grad</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_grad:\n&quot;</span>,x_grad)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x: tensor([[0., 0.],</span></span><br><span class="line"><span class="string">        [1., 2.]], requires_grad=True)</span></span><br><span class="line"><span class="string">y: tensor([[1., 1.],</span></span><br><span class="line"><span class="string">        [0., 1.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">x_grad:</span></span><br><span class="line"><span class="string"> tensor([[-2., -2.],</span></span><br><span class="line"><span class="string">        [ 0.,  2.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<hr>
<ul>
<li>在numpy和torch.tensor中  *  都是指相同size的矩阵各个位置相乘产生新矩阵</li>
<li>numpy中的矩阵乘法为<code>np.dot(a,b)</code>，torch中为<code>torch.matmul(c,d)</code></li>
</ul>
<hr>
<h4 id="autograd-grad自动微分"><a href="#autograd-grad自动微分" class="headerlink" title="autograd.grad自动微分"></a>autograd.grad自动微分</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line"></span><br><span class="line"><span class="comment"># create_graph 设置为 True 将允许创建更高阶的导数 </span></span><br><span class="line">dy_dx = torch.autograd.grad(y,x,create_graph=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(dy_dx.data)      <span class="comment"># tensor(-2.)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求二阶导数</span></span><br><span class="line">dy2_dx2 = torch.autograd.grad(dy_dx,x)[<span class="number">0</span>] </span><br><span class="line"><span class="built_in">print</span>(dy2_dx2.data) <span class="comment"># tensor(2.)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">x1 = torch.tensor(<span class="number">1.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">x2 = torch.tensor(<span class="number">2.0</span>,requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y1 = x1*x2</span><br><span class="line">y2 = x1+x2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 允许同时对多个自变量求导数</span></span><br><span class="line">(dy1_dx1,dy1_dx2) = torch.autograd.grad(outputs=y1,inputs = [x1,x2],retain_graph = <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(dy1_dx1,dy1_dx2)</span><br><span class="line"><span class="comment"># tensor(2.) tensor(1.)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果有多个因变量，相当于把多个因变量的梯度结果求和</span></span><br><span class="line">(dy12_dx1,dy12_dx2) = torch.autograd.grad(outputs=[y1,y2],inputs = [x1,x2])</span><br><span class="line"><span class="built_in">print</span>(dy12_dx1,dy12_dx2)</span><br><span class="line"><span class="comment"># tensor(3.) tensor(2.)</span></span><br></pre></td></tr></table></figure>

<h4 id="利用autograd和optimizer求最小值"><a href="#利用autograd和optimizer求最小值" class="headerlink" title="利用autograd和optimizer求最小值"></a>利用autograd和optimizer求最小值</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>, requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(params=[x], lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    result = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    <span class="keyword">return</span> (result)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    y = f(x)</span><br><span class="line">    y.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;y= <span class="subst">&#123;f(x).data&#125;</span>; x= <span class="subst">&#123;x.data&#125;</span>&quot;</span>) <span class="comment"># f-string格式化</span></span><br><span class="line"><span class="comment"># y= tensor(0.) ; x= tensor(1.0000)</span></span><br></pre></td></tr></table></figure>

<h3 id="动态计算图"><a href="#动态计算图" class="headerlink" title="动态计算图"></a>动态计算图</h3><p>之前的一些深度学习框架使用静态计算图，而pytorch使用动态计算图，有它的优点</p>
<h4 id="动态图简介"><a href="#动态图简介" class="headerlink" title="动态图简介"></a>动态图简介</h4><p><img src="https://www.pytorchmaster.com/data/torch%E5%8A%A8%E6%80%81%E5%9B%BE.gif" alt="img"></p>
<p>pytorch的计算图由<strong>节点</strong>和<strong>边</strong>组成，节点表示<strong>张量</strong>或<strong>Function</strong>,边表示张量和Function之间的依赖关系</p>
<p>动态有两重含义</p>
<ul>
<li>计算图正向传播是立即执行的，无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到的计算结果。</li>
<li>计算图在反向传播后立即销毁。下次调用需要重新构建计算图。如果在程序中使用了backward方法执行了反向传播，或者利用torch.autograd.grad方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建</li>
</ul>
<p>1.计算图的正向传播是立即执行的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.randn(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">Y = torch.randn(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">Y_hat = X@w.t() + b  <span class="comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span></span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss.data)</span><br><span class="line"><span class="built_in">print</span>(Y_hat.data)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(17.8969)</span></span><br><span class="line"><span class="string">tensor([[3.2613],</span></span><br><span class="line"><span class="string">        [4.7322],</span></span><br><span class="line"><span class="string">        [4.5037],</span></span><br><span class="line"><span class="string">        [7.5899],</span></span><br><span class="line"><span class="string">        [7.0973],</span></span><br><span class="line"><span class="string">        [1.3287],</span></span><br><span class="line"><span class="string">        [6.1473],</span></span><br><span class="line"><span class="string">        [1.3492],</span></span><br><span class="line"><span class="string">        [1.3911],</span></span><br><span class="line"><span class="string">        [1.2150]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>2.计算图在反向传播后立即销毁</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.randn(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">Y = torch.randn(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">Y_hat = X@w.t() + b  <span class="comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span></span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算图在反向传播后立即销毁，如果需要保留计算图, 需要设置retain_graph = True</span></span><br><span class="line">loss.backward()  <span class="comment">#loss.backward(retain_graph = True) </span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss.backward() <span class="comment">#如果再次执行反向传播将报错</span></span><br></pre></td></tr></table></figure>

<p>RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph&#x3D;True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.</p>
<p>第二次尝试向后遍历图形（或在张量已被释放后直接访问已保存的张量）。当您调用 .backward() 或 autograd.grad() 时，图形的已保存中间值将被释放。如果您需要第二次向后遍历图形，或者如果您需要在向后调用后访问保存的张量，请指定 retain_graph&#x3D;True。</p>
<h4 id="计算图中的Function"><a href="#计算图中的Function" class="headerlink" title="计算图中的Function"></a>计算图中的Function</h4><p>计算图中除了张量的另外一种节点是<code>Function</code>, 实际上就是 Pytorch中各种对张量操作的函数</p>
<p>这些Function和我们python中的函数有一个较大的区别，那就是它<strong>同时包含正向计算逻辑和反向传播逻辑</strong></p>
<p>我们可以通过继承torch.autograd.Function来创建这种支持反向传播的Function</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyReLU</span>(torch.autograd.Function):</span><br><span class="line">    <span class="comment"># 正向传播逻辑，可以用ctx存储一些值，供反向传播使用</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span></span>):</span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span>.clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播逻辑</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        <span class="built_in">input</span>, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[<span class="built_in">input</span> &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.tensor([[-<span class="number">1.0</span>,-<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>,<span class="number">3.0</span>]])</span><br><span class="line"></span><br><span class="line">relu = MyReLU.apply <span class="comment"># relu现在也可以具有正向传播和反向传播功能</span></span><br><span class="line">Y_hat = relu(X@w.t() + b)</span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(w.grad)	<span class="comment">#tensor([[4.5000, 4.5000]])</span></span><br><span class="line"><span class="built_in">print</span>(b.grad)	<span class="comment">#tensor([[4.5000]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Y_hat的梯度函数即是我们自己所定义的 MyReLU.backward</span></span><br><span class="line"><span class="built_in">print</span>(Y_hat.grad_fn)	<span class="comment">#&lt;torch.autograd.function.MyReLUBackward object at 0x1205a46c8&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="计算图与反向传播"><a href="#计算图与反向传播" class="headerlink" title="计算图与反向传播"></a>计算图与反向传播</h4><p>简单理解反向传播的原理和过程（链式法则）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x + <span class="number">1</span></span><br><span class="line">y2 = <span class="number">2</span>*x</span><br><span class="line">loss = (y1-y2)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>

<p>loss.backward()语句调用后，依次发生以下计算过程。</p>
<ol>
<li><p>loss自己的grad梯度赋值为1，即对自身的梯度为1。</p>
</li>
<li><p>loss根据其自身梯度以及关联的backward方法，计算出其对应的自变量即y1和y2的梯度，将该值赋值到y1.grad和y2.grad。</p>
</li>
<li><p>y2和y1根据其自身梯度以及关联的backward方法, 分别计算出其对应的自变量x的梯度，x.grad将其收到的多个梯度值累加。</p>
</li>
</ol>
<p>（注意，1,2,3步骤的求梯度顺序和对多个梯度值的累加规则恰好是求导链式法则的程序表述）</p>
<p>正因为求导链式法则衍生的梯度累加规则，张量的grad梯度不会自动清零，在需要的时候需要手动置零。</p>
<h4 id="叶子节点和非叶子节点"><a href="#叶子节点和非叶子节点" class="headerlink" title="叶子节点和非叶子节点"></a>叶子节点和非叶子节点</h4><p>执行<a href="#jump">上述代码</a>，我们会发现loss.grad并不是我们期望的1，而是None。类似地 y1.grad 以及 y2.grad也是 None。</p>
<p>这是<strong>由于它们不是叶子节点张量</strong>。</p>
<p>在反向传播过程中，只有 <code>is_leaf=True</code> 的叶子节点，需要求导的张量的导数结果才会被最后保留下来。</p>
<p>那么什么是叶子节点张量呢？叶子节点张量需要满足两个条件。</p>
<p>1，叶子节点张量是由用户直接创建的张量，而非由某个Function通过计算得到的张量。</p>
<p>2，叶子节点张量的 requires_grad属性必须为True.</p>
<p>Pytorch设计这样的规则主要是为了节约内存或者显存空间，因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。</p>
<p>所有依赖于叶子节点张量的张量, 其requires_grad 属性必定是True的，但其梯度值只在计算过程中被用到，不会最终存储到grad属性中。</p>
<p>如果需要保留中间计算结果的梯度到grad属性中，可以使用 retain_grad方法。 如果仅仅是为了调试代码查看梯度值，可以利用register_hook打印日志。<span id="jump"></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x + <span class="number">1</span></span><br><span class="line">y2 = <span class="number">2</span>*x</span><br><span class="line">loss = (y1-y2)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;loss.grad:&quot;</span>, loss.grad) <span class="comment"># loss.grad: None</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y1.grad:&quot;</span>, y1.grad)	<span class="comment"># y1.grad: None</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y2.grad:&quot;</span>, y2.grad)	<span class="comment"># y2.grad: None</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)	<span class="comment"># tensor(4.)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.is_leaf) <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(y1.is_leaf) <span class="comment"># False</span></span><br><span class="line"><span class="built_in">print</span>(y2.is_leaf) <span class="comment"># False</span></span><br><span class="line"><span class="built_in">print</span>(loss.is_leaf)	<span class="comment"># False</span></span><br></pre></td></tr></table></figure>

<p>利用retain_grad可以保留非叶子节点的梯度值，利用register_hook可以查看非叶子节点的梯度值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment">#正向传播</span></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x + <span class="number">1</span></span><br><span class="line">y2 = <span class="number">2</span>*x</span><br><span class="line">loss = (y1-y2)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#非叶子节点梯度显示控制</span></span><br><span class="line">y1.register_hook(<span class="keyword">lambda</span> grad: <span class="built_in">print</span>(<span class="string">&#x27;y1 grad: &#x27;</span>, grad))	<span class="comment"># y2 grad:  tensor(4.)</span></span><br><span class="line">y2.register_hook(<span class="keyword">lambda</span> grad: <span class="built_in">print</span>(<span class="string">&#x27;y2 grad: &#x27;</span>, grad))	<span class="comment"># y1 grad:  tensor(-4.)</span></span><br><span class="line">loss.retain_grad()	</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;loss.grad:&quot;</span>, loss.grad)	<span class="comment"># loss.grad: tensor(1.)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x.grad:&quot;</span>, x.grad)	 <span class="comment"># loss.grad: tensor(1.)</span></span><br></pre></td></tr></table></figure>

<h4 id="计算图在TensorBoard中的可视化"><a href="#计算图在TensorBoard中的可视化" class="headerlink" title="计算图在TensorBoard中的可视化"></a>计算图在TensorBoard中的可视化</h4><p>可以利用 torch.utils.tensorboard 将计算图导出到 TensorBoard进行可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.w = nn.Parameter(torch.randn(<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">        self.b = nn.Parameter(torch.zeros(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = x@self.w + self.b</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./data/tensorboard&#x27;</span>)</span><br><span class="line">writer.add_graph(net,input_to_model = torch.rand(<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line">%load_ext tensorboard</span><br><span class="line"><span class="comment">#%tensorboard --logdir ./data/tensorboard</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorboard <span class="keyword">import</span> notebook</span><br><span class="line">notebook.<span class="built_in">list</span>() </span><br><span class="line"></span><br><span class="line"><span class="comment">#在tensorboard中查看模型</span></span><br><span class="line">notebook.start(<span class="string">&quot;--logdir ./data/tensorboard&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://www.pytorchmaster.com/data/2-3-%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt="img"></p>
<h3 id="pytorch的层次结构"><a href="#pytorch的层次结构" class="headerlink" title="pytorch的层次结构"></a>pytorch的层次结构</h3><p>5个不同的层次结构：即硬件层，内核层，低阶API，中阶API，高阶API [torchkeras]</p>
<p>Pytorch的层次结构从低到高可以分成如下五层。</p>
<p>最底层为硬件层，Pytorch支持CPU、GPU加入计算资源池。</p>
<p>第二层为C++实现的内核。</p>
<p>第三层为Python实现的操作符，提供了封装C++内核的低级API指令，主要包括各种张量操作算子、自动微分、变量管理. 如torch.tensor,torch.cat,torch.autograd.grad,nn.Module. 如果把模型比作一个房子，那么第三层API就是【模型之砖】。</p>
<p>第四层为Python实现的模型组件，对低级API进行了函数封装，主要包括各种模型层，损失函数，优化器，数据管道等等。 如torch.nn.Linear,torch.nn.BCE,torch.optim.Adam,torch.utils.data.DataLoader. 如果把模型比作一个房子，那么第四层API就是【模型之墙】。</p>
<p>第五层为Python实现的模型接口。Pytorch没有官方的高阶API。为了便于训练模型，<a target="_blank" rel="noopener" href="https://github.com/lyhue1991/eat_pytorch_in_20_days">博客作者</a>仿照keras中的模型接口，使用了不到300行代码，封装了Pytorch的高阶模型接口torchkeras.Model。如果把模型比作一个房子，那么第五层API就是模型本身，即【模型之屋】。</p>
<h4 id="低阶API示范"><a href="#低阶API示范" class="headerlink" title="低阶API示范"></a>低阶API示范</h4><p>下面的范例使用Pytorch的低阶API实现线性回归模型和DNN二分类模型</p>
<p>低阶API主要包括<strong>张量操作</strong>，<strong>计算图</strong>和<strong>自动微分</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">printbar</span>():</span><br><span class="line">    nowtime = datetime.datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;==========&quot;</span>*<span class="number">8</span> + <span class="string">&quot;%s&quot;</span>%nowtime)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mac系统上pytorch和matplotlib在jupyter中同时跑需要更改环境变量</span></span><br><span class="line"><span class="comment"># os.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;]=&quot;TRUE&quot; </span></span><br></pre></td></tr></table></figure>

<h5 id="一、线性回归模型"><a href="#一、线性回归模型" class="headerlink" title="一、线性回归模型"></a>一、线性回归模型</h5><ol>
<li>data prepare</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#样本数量</span></span><br><span class="line">n = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成测试用数据集</span></span><br><span class="line">X = <span class="number">10</span>*torch.rand([n,<span class="number">2</span>])-<span class="number">5.0</span>  <span class="comment">#torch.rand是均匀分布 </span></span><br><span class="line">w0 = torch.tensor([[<span class="number">2.0</span>],[-<span class="number">3.0</span>]])</span><br><span class="line">b0 = torch.tensor([[<span class="number">10.0</span>]])</span><br><span class="line">Y = X@w0 + b0 + torch.normal( <span class="number">0.0</span>,<span class="number">2.0</span>,size = [n,<span class="number">1</span>])  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;svg&#x27;</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))  <span class="comment"># 图形窗口的宽度为 12 英寸，高度为 5 英寸</span></span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line"><span class="comment">#画散点图 </span></span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>].numpy(),Y[:,<span class="number">0</span>].numpy(), c = <span class="string">&quot;b&quot;</span>,label = <span class="string">&quot;samples&quot;</span>)</span><br><span class="line"><span class="comment">#设置图例</span></span><br><span class="line">ax1.legend()  <span class="comment"># 没有此不会有samples图例</span></span><br><span class="line">plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>].numpy(),Y[:,<span class="number">0</span>].numpy(), c = <span class="string">&quot;g&quot;</span>,label = <span class="string">&quot;samples&quot;</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>,rotation = <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230119174857563.png" alt="image-20230119174857563"></p>
<p>注：**<code>plt.subplot(121)</code> 是 matplotlib 库中用于创建多个子图的函数之一。其中，<code>121</code> 是参数，表示将整个图分成 1 行 2 列，并在第 1 个位置创建子图。因此，这条语句将创建一个 1 行 2 列的子图网格，并在第一个子图中绘制图形。**</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建数据管道迭代器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">features, labels, batch_size=<span class="number">8</span></span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    np.random.shuffle(indices)  <span class="comment">#样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        indexs = torch.LongTensor(indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span>  features.index_select(<span class="number">0</span>, indexs), labels.index_select(<span class="number">0</span>, indexs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据管道效果   </span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">(features,labels) = <span class="built_in">next</span>(data_iter(X,Y,batch_size))</span><br><span class="line"><span class="built_in">print</span>(features)</span><br><span class="line"><span class="built_in">print</span>(labels)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 1.0449, -0.3581],</span></span><br><span class="line"><span class="string">        [-3.0645, -2.9230],</span></span><br><span class="line"><span class="string">        [ 3.7969, -4.5846],</span></span><br><span class="line"><span class="string">        [-0.2429,  0.5349],</span></span><br><span class="line"><span class="string">        [ 2.2708,  0.1713],</span></span><br><span class="line"><span class="string">        [ 4.6910,  4.3684],</span></span><br><span class="line"><span class="string">        [ 2.1360, -4.7411],</span></span><br><span class="line"><span class="string">        [ 3.3687,  0.3648]])</span></span><br><span class="line"><span class="string">tensor([[15.6397],</span></span><br><span class="line"><span class="string">        [14.1632],</span></span><br><span class="line"><span class="string">        [31.6240],</span></span><br><span class="line"><span class="string">        [ 7.4723],</span></span><br><span class="line"><span class="string">        [11.8881],</span></span><br><span class="line"><span class="string">        [ 6.8064],</span></span><br><span class="line"><span class="string">        [30.4618],</span></span><br><span class="line"><span class="string">        [15.2579]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>define the model</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegression</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.w = torch.randn_like(w0,requires_grad=<span class="literal">True</span>)</span><br><span class="line">        self.b = torch.zeros_like(b0,requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">#正向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>): </span><br><span class="line">        <span class="keyword">return</span> x@self.w + self.b</span><br><span class="line">    <span class="comment"># 损失函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss_func</span>(<span class="params">self,y_pred,y_true</span>):  </span><br><span class="line">        <span class="keyword">return</span> torch.mean((y_pred - y_true)**<span class="number">2</span>/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model = LinearRegression()</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>model training</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">model, features, labels</span>):</span><br><span class="line">    predictions = model.forward(features)</span><br><span class="line">    loss = model.loss_func(predictions,labels)</span><br><span class="line">    <span class="comment"># 反向传播求梯度</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用torch.no_grad()避免梯度记录，也可以通过操作 model.w.data 实现避免梯度记录 </span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 梯度下降法更新参数</span></span><br><span class="line">        model.w -= <span class="number">0.001</span>*model.w.grad</span><br><span class="line">        model.b -= <span class="number">0.001</span>*model.b.grad</span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        model.w.grad.zero_()</span><br><span class="line">        model.b.grad.zero_()</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试train_step效果</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">(features,labels) = <span class="built_in">next</span>(data_iter(X,Y,batch_size))</span><br><span class="line">train_step(model,features,labels)</span><br></pre></td></tr></table></figure>

<p>Out[]: tensor(141.2280, grad_fn&#x3D;<MeanBackward0>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">model,epochs</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> data_iter(X,Y,<span class="number">10</span>):</span><br><span class="line">            loss = train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">200</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;epoch =&quot;</span>,epoch,<span class="string">&quot;loss = &quot;</span>,loss.item())</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;model.w =&quot;</span>,model.w.data)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;model.b =&quot;</span>,model.b.data)</span><br><span class="line"></span><br><span class="line">train_model(model,epochs = <span class="number">1000</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">================================================================================2023-01-20 17:03:37</span><br><span class="line">epoch = 200 loss =  2.0936481952667236</span><br><span class="line">model.w = tensor([[ 2.0185],</span><br><span class="line">        [-2.9519]])</span><br><span class="line">model.b = tensor([[10.0203]])</span><br><span class="line"></span><br><span class="line">================================================================================2023-01-20 17:03:38</span><br><span class="line">epoch = 400 loss =  1.3962600231170654</span><br><span class="line">model.w = tensor([[ 2.0212],</span><br><span class="line">        [-2.9512]])</span><br><span class="line">model.b = tensor([[10.0241]])</span><br><span class="line"></span><br><span class="line">================================================================================2023-01-20 17:03:39</span><br><span class="line">epoch = 600 loss =  0.3997553586959839</span><br><span class="line">model.w = tensor([[ 2.0167],</span><br><span class="line">        [-2.9528]])</span><br><span class="line">model.b = tensor([[10.0237]])</span><br><span class="line"></span><br><span class="line">================================================================================2023-01-20 17:03:41</span><br><span class="line">epoch = 800 loss =  1.8717002868652344</span><br><span class="line">model.w = tensor([[ 2.0194],</span><br><span class="line">        [-2.9495]])</span><br><span class="line">model.b = tensor([[10.0240]])</span><br><span class="line"></span><br><span class="line">================================================================================2023-01-20 17:03:42</span><br><span class="line">epoch = 1000 loss =  1.8542640209197998</span><br><span class="line">model.w = tensor([[ 2.0176],</span><br><span class="line">        [-2.9507]])</span><br><span class="line">model.b = tensor([[10.0237]])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>].numpy(),Y[:,<span class="number">0</span>].numpy(), c = <span class="string">&quot;b&quot;</span>,label = <span class="string">&quot;samples&quot;</span>)</span><br><span class="line">ax1.plot(X[:,<span class="number">0</span>].numpy(),(model.w[<span class="number">0</span>].data*X[:,<span class="number">0</span>]+model.b[<span class="number">0</span>].data).numpy(),<span class="string">&quot;-r&quot;</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">&quot;model&quot;</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>].numpy(),Y[:,<span class="number">0</span>].numpy(), c = <span class="string">&quot;g&quot;</span>,label = <span class="string">&quot;samples&quot;</span>)</span><br><span class="line">ax2.plot(X[:,<span class="number">1</span>].numpy(),(model.w[<span class="number">1</span>].data*X[:,<span class="number">1</span>]+model.b[<span class="number">0</span>].data).numpy(),<span class="string">&quot;-r&quot;</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">&quot;model&quot;</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230120170539911.png" alt="image-20230120170539911"></p>
<h5 id="二、DNN二分类模型"><a href="#二、DNN二分类模型" class="headerlink" title="二、DNN二分类模型"></a>二、DNN二分类模型</h5><ol>
<li>data prepare</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;svg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正负样本数量</span></span><br><span class="line">n_positive,n_negative = <span class="number">2000</span>,<span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正样本, 小圆环分布</span></span><br><span class="line">r_p = <span class="number">5.0</span> + torch.normal(<span class="number">0.0</span>,<span class="number">1.0</span>,size = [n_positive,<span class="number">1</span>]) </span><br><span class="line">theta_p = <span class="number">2</span>*np.pi*torch.rand([n_positive,<span class="number">1</span>])</span><br><span class="line">Xp = torch.cat([r_p*torch.cos(theta_p),r_p*torch.sin(theta_p)],axis = <span class="number">1</span>)</span><br><span class="line">Yp = torch.ones_like(r_p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成负样本, 大圆环分布</span></span><br><span class="line">r_n = <span class="number">8.0</span> + torch.normal(<span class="number">0.0</span>,<span class="number">1.0</span>,size = [n_negative,<span class="number">1</span>]) </span><br><span class="line">theta_n = <span class="number">2</span>*np.pi*torch.rand([n_negative,<span class="number">1</span>])</span><br><span class="line">Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = <span class="number">1</span>)</span><br><span class="line">Yn = torch.zeros_like(r_n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#汇总样本</span></span><br><span class="line">X = torch.cat([Xp,Xn],axis = <span class="number">0</span>)</span><br><span class="line">Y = torch.cat([Yp,Yn],axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize = (<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">&quot;r&quot;</span>)</span><br><span class="line">plt.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">&quot;g&quot;</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;positive&quot;</span>,<span class="string">&quot;negative&quot;</span>])</span><br></pre></td></tr></table></figure>

<p><img src="https://www.pytorchmaster.com/data/3-1-%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt="img"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://HaomingX.github.io">HaomingX</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://haomingx.github.io/2023/01/05/pytorch-learning/">https://haomingx.github.io/2023/01/05/pytorch-learning/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://HaomingX.github.io" target="_blank">HaomingX的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209210056384.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/02/22/computer-composition1/"><img class="prev-cover" src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209210056384.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">computer_composition1</div></div></a></div><div class="next-post pull-right"><a href="/2023/01/04/python%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B9/"><img class="next-cover" src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209210056384.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">python小知识点</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209202359712.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HaomingX</div><div class="author-info__description">哈尔滨工业大学(Harbin not ShenZhen) | 计算机科学与技术 | 自然语言处理</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/HaomingX"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/HaomingX" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/978545377@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#torch%E5%87%BD%E6%95%B0"><span class="toc-number">1.</span> <span class="toc-text">torch函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-cat"><span class="toc-number">1.1.</span> <span class="toc-text">torch.cat()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-stack"><span class="toc-number">1.2.</span> <span class="toc-text">torch.stack()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-ones-like"><span class="toc-number">1.3.</span> <span class="toc-text">torch.ones_like()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-zeros-like"><span class="toc-number">1.4.</span> <span class="toc-text">torch.zeros_like()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-range"><span class="toc-number">1.5.</span> <span class="toc-text">torch.range()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-arange"><span class="toc-number">1.6.</span> <span class="toc-text">torch.arange()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-normal"><span class="toc-number">1.7.</span> <span class="toc-text">torch.normal()</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F"><span class="toc-number">2.</span> <span class="toc-text">张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">2.1.</span> <span class="toc-text">数据类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E7%BB%B4%E5%BA%A6"><span class="toc-number">2.2.</span> <span class="toc-text">张量的维度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B0%BA%E5%AF%B8"><span class="toc-number">2.3.</span> <span class="toc-text">张量的尺寸</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%92%8Cnumpy%E6%95%B0%E7%BB%84"><span class="toc-number">2.4.</span> <span class="toc-text">张量和numpy数组</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#autograd%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">3.</span> <span class="toc-text">autograd自动微分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#backward%E6%B1%82%E5%AF%BC%E6%95%B0"><span class="toc-number">3.1.</span> <span class="toc-text">backward求导数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#autograd-grad%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">3.2.</span> <span class="toc-text">autograd.grad自动微分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A9%E7%94%A8autograd%E5%92%8Coptimizer%E6%B1%82%E6%9C%80%E5%B0%8F%E5%80%BC"><span class="toc-number">3.3.</span> <span class="toc-text">利用autograd和optimizer求最小值</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">4.</span> <span class="toc-text">动态计算图</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E5%9B%BE%E7%AE%80%E4%BB%8B"><span class="toc-number">4.1.</span> <span class="toc-text">动态图简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%AD%E7%9A%84Function"><span class="toc-number">4.2.</span> <span class="toc-text">计算图中的Function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">4.3.</span> <span class="toc-text">计算图与反向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9%E5%92%8C%E9%9D%9E%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9"><span class="toc-number">4.4.</span> <span class="toc-text">叶子节点和非叶子节点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%9C%A8TensorBoard%E4%B8%AD%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">4.5.</span> <span class="toc-text">计算图在TensorBoard中的可视化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pytorch%E7%9A%84%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84"><span class="toc-number">5.</span> <span class="toc-text">pytorch的层次结构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%8E%E9%98%B6API%E7%A4%BA%E8%8C%83"><span class="toc-number">5.1.</span> <span class="toc-text">低阶API示范</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.1.1.</span> <span class="toc-text">一、线性回归模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BA%8C%E3%80%81DNN%E4%BA%8C%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.1.2.</span> <span class="toc-text">二、DNN二分类模型</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/03/08/machine-code/" title="machine_code"><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209210056384.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="machine_code"/></a><div class="content"><a class="title" href="/2023/03/08/machine-code/" title="machine_code">machine_code</a><time datetime="2023-03-08T11:47:29.000Z" title="发表于 2023-03-08 19:47:29">2023-03-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/28/csapp-data/" title="csapp_data"><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209210056384.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="csapp_data"/></a><div class="content"><a class="title" href="/2023/02/28/csapp-data/" title="csapp_data">csapp_data</a><time datetime="2023-02-28T00:07:38.000Z" title="发表于 2023-02-28 08:07:38">2023-02-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/22/computer-composition1/" title="computer_composition1"><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209210056384.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="computer_composition1"/></a><div class="content"><a class="title" href="/2023/02/22/computer-composition1/" title="computer_composition1">computer_composition1</a><time datetime="2023-02-22T01:10:27.000Z" title="发表于 2023-02-22 09:10:27">2023-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/05/pytorch-learning/" title="pytorch_learning"><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209210056384.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="pytorch_learning"/></a><div class="content"><a class="title" href="/2023/01/05/pytorch-learning/" title="pytorch_learning">pytorch_learning</a><time datetime="2023-01-05T02:51:16.000Z" title="发表于 2023-01-05 10:51:16">2023-01-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/04/python%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B9/" title="python小知识点"><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209210056384.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="python小知识点"/></a><div class="content"><a class="title" href="/2023/01/04/python%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B9/" title="python小知识点">python小知识点</a><time datetime="2023-01-04T03:26:21.000Z" title="发表于 2023-01-04 11:26:21">2023-01-04</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By HaomingX</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><script type="text/javascript" id="maid-script" src="https://unpkg.com/mermaid@8.8.4/dist/mermaid.min.js?v=undefined"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'POjfzg5t5qs0voXewA5Jq4Is-MdYXbMMI',
      appKey: '39tt9rBJ3NB2K6af48k8Bp0Q',
      avatar: 'wavatar',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"]):not([href="/shuoshuo/"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>