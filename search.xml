<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深度学习的不确定性和校准</title>
      <link href="/2023/09/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A0%A1%E5%87%86/"/>
      <url>/2023/09/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A0%A1%E5%87%86/</url>
      
        <content type="html"><![CDATA[<h1 id="深度学习的不确定性和校准"><a href="#深度学习的不确定性和校准" class="headerlink" title="深度学习的不确定性和校准"></a>深度学习的不确定性和校准</h1><h2 id="几个概念"><a href="#几个概念" class="headerlink" title="几个概念"></a>几个概念</h2><h3 id="不确定性是什么"><a href="#不确定性是什么" class="headerlink" title="不确定性是什么"></a>不确定性是什么</h3><ul><li><p>偶然事件不确定性</p><p>数据生成本身具有的随机性，无法通过收集更多数据来消除不确定（应该优化采集数据的策略）</p></li><li><p>认知不确定性</p><p>就是模型的不确定性， 模型对输入数据因为训练的各种问题导致的不准确，与某一单一的数据无关</p></li><li><p>超出分布的不确定性</p><p>就是常说的 OOD(out of distribution)问题, 感觉模型学习就是一个记忆的过程，遇到分布之外的数据模型就会胡说八道，这就涉及到了异常检测</p><blockquote><p>假设数据集 A(报纸上的文本数据)，B(维基百科上的文本数据)，利用 A 训练一个模型，然后利用 A，B 分别作为输入去做测试，此时 A 就是域内数据 <strong>in-domain</strong>，B 叫 <strong>out-domain</strong>，而 open-domain 就更加广泛随意了，例如将 AB 混合起来成为数据集 C，C 去用作模型的测试输入，这时 C 就是 <strong>open-domain</strong>。</p></blockquote></li></ul><h3 id="置信度是什么"><a href="#置信度是什么" class="headerlink" title="置信度是什么"></a>置信度是什么</h3><p>简单来说就是 <strong>对决策的信心程度</strong>，具体到模型里，logits 就能反应置信度</p><blockquote><p>复习: 概率论&#x2F;机器学习中的置信度和置信区间</p><p>置信度就是对事件的估计的信心，置信区间则是一个范围，在固定置信度下估计某个参数或事件落在的区间</p><p>参数估计：利用总体抽样得到的信息来估计总体的某些参数或参数的某些函数</p><ol><li><p>点估计</p><ul><li>矩估计  用样本矩估计总体矩    依据是大数定理</li><li>最大似然估计    找出概率的最大值</li></ul></li><li><p>区间估计</p><p><img src="https://pic2.zhimg.com/80/v2-3fc0b84ef0cf575cd7d89169a1e79ef5_720w.webp" alt="img"></p></li></ol><p>各种假设检验方法（如 t 检验）：可以推断若在测试集上观察到学习器A优于B,则A的<strong>泛化性能</strong>是否在统计意义上优于B,以及这个结论的把握有多大</p></blockquote><p>概率型模型评估指标：</p><ol><li>布里尔分数（Brier Score）：</li></ol><p>概率预测相对于测试样本的均方误差，衡量了概率距离真实标签结果的差异：</p><p>$Brier Score&#x3D;(p−o)^2$ </p><ol start="2"><li><p>对数似然函数 Log Loss (二元交叉熵损失)</p><p>对数似然函数的取值越小，则证明概率估计越准确，模型越理想</p></li></ol><p>$Log Loss&#x3D;−(y⋅log(p)+(1−y)⋅log(1−p))$</p><ol start="3"><li><strong>可靠性曲线 （校准曲线）</strong></li></ol><blockquote><p>是用于评估分类模型的概率预测准确性的一种图形化工具。它帮助我们了解模型的概率预测与实际观测之间的关系，以便判断模型是否具有良好的概率校准性。</p><p>在分类问题中，模型不仅会做出类别预测，还会给出属于每个类别的概率估计。这些概率估计可以被视为模型对于样本属于不同类别的自信程度。而良好的概率校准性意味着，模型预测的概率与实际发生的频率相匹配。</p><p>可靠性曲线通常是一个散点图，其中 x 轴表示模型预测的概率，y 轴表示实际观测的频率（即在给定预测概率范围内的实际样本比例）。理想情况下，如果模型的概率预测是完全校准的，那么这些点将会在对角线 y <em>&#x3D;</em> x 上。这表示当模型预测的概率为 p 时，实际观测的频率也应该接近 p。</p><p>然而，在实际应用中，模型可能会存在概率校准性不足的情况。例如，模型可能倾向于给出过于极端的概率预测，导致可靠性曲线上的点偏离了对角线。通过观察可靠性曲线，我们可以判断模型是否需要进行概率校准，以提高其概率预测的准确性。</p></blockquote><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230829205256146.png" alt="image-20230829205256146"></p><ul><li>逻辑回归是一种特殊情况，因为它在设计上经过了良好的校准，因为它的目标函数最小化了对数损失函数</li></ul><ol start="4"><li>预测概率的直方图</li></ol><blockquote><p>预测概率的直方图是一种图表，用于显示分类模型对于不同类别的概率预测的分布情况。它可以帮助我们理解模型对于不同类别的预测置信程度分布，从而了解模型的概率预测性能和可靠性。</p><p>在一个分类问题中，模型通常会输出每个样本属于不同类别的概率估计值。例如，在二分类问题中，模型会给出样本属于正类别的概率 p，以及属于负类别的概率 1 − p。预测概率的直方图将按照这些概率值的范围，将预测概率划分为不同的区间，并统计落在每个区间内的样本数量。</p><p>直方图的 x 轴通常表示概率的范围（例如从 0 到 1），y 轴表示落在对应概率区间内的样本数量。通过观察直方图，我们可以获得以下信息：</p><ol><li><strong>概率分布：</strong> 我们可以看到模型的概率预测在不同区间的分布情况。这有助于了解模型是如何对于不同类别的概率进行预测的。</li><li><strong>置信程度：</strong> 预测概率的直方图可以显示出模型的置信程度分布。高度集中的直方柱表示模型对于概率的估计比较自信，而分散的分布可能表示模型对于预测的不确定性较大。</li><li><strong>校准性：</strong> 如果模型的概率预测是校准的，那么直方图应该在每个区间内都大致匹配实际观测的频率分布。</li></ol><p>预测概率的直方图通常用于了解模型的概率预测性能和分布情况，但它并不直接提供关于模型分类准确性的信息。为了更全面地评估模型，可以结合其他工具如可靠性曲线、Brier 分数等进行分析。</p></blockquote><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230829205530721.png" alt="image-20230829205530721"></p><h3 id="不确定性估计-uncertainty-estimates"><a href="#不确定性估计-uncertainty-estimates" class="headerlink" title="不确定性估计 uncertainty estimates"></a>不确定性估计 uncertainty estimates</h3><p>就是计算 <strong>置信度</strong> 和 <strong>真实准确率</strong> 之间的误差程度，得到不确定性的概率值 uncertainty，实际上 uncertainty 同样也反映置信度</p><blockquote><p>关注模型对预测结果有多大的把握，从而为模型排除错误</p></blockquote><h2 id="calibration"><a href="#calibration" class="headerlink" title="calibration"></a>calibration</h2><h3 id="评价指标："><a href="#评价指标：" class="headerlink" title="评价指标："></a>评价指标：</h3><ol><li><p>ECE-预期校准误差</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230830154848108.png" alt="image-20230830154848108"></p><p>ECE 是基于可靠性图来的</p><p>soft ECE:</p><blockquote><p>Soft-Binned ECE &#x3D; $$\sum_{i&#x3D;1}^n \frac{w_i}{\sum_{j&#x3D;1}^n w_j} |a_i - c_i|$$</p><p>其中，<em>n</em> 是分区的数量，w <strong>i 是第 <em>i</em> 个分区的权重，a</strong> i 是第 i 个分区内的预测概率的平均值，c <strong>i 是第 i 个分区内的真实概率的平均值。Soft-Binned ECE 的关键点是使用一个连续的函数来计算每个分区的权重和平均值，而不是使用一个离散的函数。具体来说，对于每个样本(<em>x</em>, <em>y</em>)，它的预测概率为 <em>p</em> &#x3D; <em>f</em>(<em>x</em>)，它对应的分区索引为 <em>k</em> &#x3D; ⌊ n</strong> p <strong>⌋</strong>+1，那么它对第 <em>i</em> 个分区的权重贡献为：</p><p>$w_i(x,y)&#x3D;max(0,1−∣i−k∣)$</p><p>它对第 i* 个分区的平均预测概率贡献为：</p><p>$a_i(x,y)&#x3D;w_i(x,y)p$</p><p>它对第 i* 个分区的平均真实概率贡献为：</p><p>$c_i(x,y)&#x3D;w_i(x,y)I[y&#x3D;argmax(p)]$</p><p>其中 I 是指示函数。可以看出，这些函数都是连续可微的，因此可以直接用于训练过程中。更多细节可以参考 <a href="https://deepai.org/publication/soft-calibration-objectives-for-neural-networks">这篇论文</a>。</p></blockquote><p>Region ECE：把样本分 Region</p></li><li><p>MCE-最大校准误差</p></li></ol><p>（Expected Calibration Error）</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230718151732832.png" alt="image-20230718151732832"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230721204209858.png" alt="image-20230721204209858"></p><p>其中，b 代表第 b 个 bin，B 代表 bin 的总数，$n_b$ 代表第 b 个 bin 中样本的总数，acc(b)代表第 b 个 bin 中样本真实标签的平均值，conf(b)代表第 b 个 bin 中模型预测概率的平均值。两者之差的绝对值就能度量模型的置信度，差距越大代表模型置信度越小。</p><p><strong>衡量了置信度分数与基本事实的不确定性之间的一致性</strong></p><ol start="3"><li>Negative log likelihood 负对数似然</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230830145609133.png" alt="image-20230830145609133"></p><ol start="4"><li><p>Reliability Diagrams 可靠性曲线</p></li><li><p>AvUC</p><p>Soft AvUC 是基于 AvUC (Average Uncertainty Calibration) 的改进，AvUC 是一种校准误差度量，它将预测概率分成若干个区间（bin），然后计算每个区间内预测概率的方差和真实概率的方差之间的差异，再对所有区间求加权平均。Soft AvUC 的主要创新是将分区操作从离散化变为连续化，这样可以使得训练目标可微分(可用来直接训练)，并且可以避免分区数量和位置的选择对结果的影响。</p><p>Soft AvUC 的具体公式如下：</p><p>Soft AvUC &#x3D; $$\sum_{i&#x3D;1}^n \frac{w_i}{\sum_{j&#x3D;1}^n w_j} |v_i - u_i|$$</p></li><li><p>AUROC</p></li></ol><p>接收器操作特征曲线下的面积（AUROC, ROC）评估置信度分数在 <strong>区分正确和错误样本方面的能力</strong></p><ol start="7"><li>AUPRC</li></ol><p>精度-召回曲线下的面积（AUPRC）<strong>衡量识别正确样本</strong>（AUPRC-Positive, PR-P）和 <strong>错误样本</strong>（AUPRC-Negative, PR-N）的能力</p><h3 id="校准方法"><a href="#校准方法" class="headerlink" title="校准方法"></a>校准方法</h3><p><a href="https://ar5iv.labs.arxiv.org/html/2308.01222">Calibration in Deep Learning: A Survey of the State-of-the-Art</a></p><p>分类：</p><ul><li><p>先验校准(prior calibration)、后验校准(post calibration)</p></li><li><p>参数化方法、    非参数化方法（通过直方图分箱结果来校准）</p><p>参数化方法假设校准映射属于有限维参数化族族，可以通过最小化损失函数来学习参数。非参数化方法则假设校准映射由无限维参数描述，可以通过直方图、贝叶斯平均或保序回归等方法进行学习。</p></li></ul><p><a href="https://aitechtogether.com/article/28062.html">https://aitechtogether.com/article/28062.html</a></p><ol><li><p>先验校准：</p><ul><li><p>数据增强 </p><ul><li><p>mixup:<a href="https://blog.csdn.net/ouyangfushu/article/details/87866579">https://blog.csdn.net/ouyangfushu/article/details/87866579</a></p><p><img src="https://img-blog.csdnimg.cn/20190221185118987.png" alt="img"></p></li><li><p>EDA代码: <a href="https://github.com/zhanlaoban/EDA_NLP_for_Chinese/blob/master/code/eda.py">https://github.com/zhanlaoban/EDA_NLP_for_Chinese/blob/master/code/eda.py</a></p></li><li><p>标签平滑（Label Smooth）：<a href="https://blog.csdn.net/weixin_44441131/article/details/106436808">https://blog.csdn.net/weixin_44441131/article/details/106436808</a></p><p>​MLE（Max likehood estimation）在 in-domain 表现得更好，LS 在 out-of-domain 表现得更好</p></li></ul></li><li><p>few shot</p></li><li><p>COT</p></li><li><p>Deep Ensemble</p><p>模型集成</p><ul><li>在模型训练之前，Deep Ensemble需要选择合适的神经网络结构和参数，以及集成的数量和方式。这些选择会影响模型的复杂度和多样性，从而影响模型的泛化能力和稳定性。</li><li>在模型训练过程中，Deep Ensemble需要对不同的神经网络进行随机初始化和训练，以及对它们的输出进行平均或统计。这些操作会影响模型的学习过程和预测结果，从而影响模型的准确性和不确定性。</li></ul></li><li><p>魔改损失函数  （正则化操作）</p><p>不良校准与负对数似然 (NLL) 的过度拟合有关</p><ul><li><p>dice 损失代码：<a href="https://blog.csdn.net/hqllqh/article/details/112056385">https://blog.csdn.net/hqllqh/article/details/112056385</a></p></li><li><p>LogitNorm 交叉熵损失：<a href="https://arxiv.org/abs/2205.09310">https://arxiv.org/abs/2205.09310</a></p><p>对传统交叉熵的改进，主要解决神经网络过拟合和模型矫正问题。虽然实验都是在 OOD 检测任务上做的，但是这个方法应该是具有比较强的通用性的，适用于一些需要知识迁移的任务。本文通过理论推导结合实验分析的方式，逐步引出方法，这个行文思路值得借鉴</p></li><li><p>[On&#x2F;Off-manifold regularization](On&#x2F;Off-manifold regularization) 损失函数上加正则项</p></li><li><p>Distance-based logits &amp; One vs ALL: <a href="https://arxiv.org/abs/2007.05134">https://arxiv.org/abs/2007.05134</a></p></li></ul><p>DM:  logits zj 被定义为嵌入和最后c一层的权重之间的负欧氏距离，即 zj &#x3D; − kf θ(x) − wjk</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230725170352955.png" alt="image-20230725170352955"></p><p>One vs ALL: <img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230725170426142.png" alt="image-20230725170426142"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230725170446060.png" alt="image-20230725170446060"></p><p>其中使用平滑算法</p><blockquote><p>trick:</p><p><a href="https://www.cnblogs.com/liweikuan/p/14253867.html">平滑算法</a>在transformer中softmax改进的应用：</p><p>$W_{i,j} &#x3D; \frac{\exp(\text{score}(q_i, k_j))}{\sum_{j&#x3D;1}^{m} \exp(\text{score}(q_i, k_j)) + \exp(\text{extra_logit})}$</p></blockquote><p>插眼：半监督、伪标签、熵正则化</p></li></ul></li><li><p>后验校准</p><ul><li><p>platt 方法使用训练、校准和测试分割来校准分类器（adaboost）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># uncalibrated model</span></span><br><span class="line">clf = AdaBoostClassifier(n_estimators=<span class="number">50</span>)</span><br><span class="line">y_proba = clf.fit(X_train, y_train).predict_proba(X_test)</span><br><span class="line"><span class="comment"># calibrated model</span></span><br><span class="line">calibrator = LogisticRegression()</span><br><span class="line">calibrator.fit(clf.predict_proba(X_calib), y_calib)</span><br><span class="line"></span><br><span class="line">y_proba_calib = calibrator.predict_proba(y_proba)</span><br></pre></td></tr></table></figure><p>使用校准分类器</p><p><img src="https://miro.medium.com/v2/resize:fit:700/1*BzjuduA4cxiy3Aoek9r_2A.png" alt="img">  train cali test分开</p></li><li><p>Isotonic Regression 等保回归  –让输出保持单调性</p><p>将一组无序的数变为有序，一组数为 {1，3，2，2} ，遍历发现3&gt;2，将这两个数转为均值，即 {1,2.5,2.5,2}，然后又发现2.5&gt;2，于是将3,2,2转为三者的均值，即 {1,2.5,2.5,2.5}<br>为了保证不引入偏差，用作校准的数据集应该和训练模型的数据集不同。</p></li><li><p>温度缩放 T scaling</p><p>对logits进行缩放</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9WQmNEMDJqRmhnbndXWmMxZlF6RzJYam9GZTZZRXFpYWxRYUJWaGJpYVJaQTBMQ0dlazZ5VkZ6WDBhNnVQWFJ1MTVNQVo2SXE5ODhqcVBLTWljN0kzUGQyQS82NDA?x-oss-process=image/format,png" alt="img"></p><p>要注意的是，上述方法需要在 validation set 上进行优化，来学习参数 temperature，而不能在 training set 上进行学习，所以 Temperature scaling 是一个 post process，即后处理步骤，这种方法也暂时只能用于分类任务，不能用于回归。</p><p><strong>补充:</strong></p><blockquote><p> NRTP(Neural Rank-Preserving Transforms):</p><p> 将温度缩放改进：让每个样本有自己的温度参数T(x),而不是全局的T。公式为: fTθ(zb;x) &#x3D; zb&#x2F;Tθ(x)</p><p> 作者观察到温度缩放保持logits排序的关键性质是单调性。因此,作者提出使用更一般的单调校准器:</p><p> fθ(zb;x) &#x3D; [gθ(zb1;x),…,gθ(zbK;x)]</p><p> 其中gθ单调递增。这保持了排序关系所以也保持了精确度。</p><p> 使用单调的两层网络实现</p><p> 作者使用了一种两层网络结构来学习单调的gθ函数:</p><p> gθ(zi;x) &#x3D; Σj ajφ((zi - bθj(x))&#x2F;Tθj(x))</p><p> 其中aj≥0,Tθj(x)&gt;0,φ是单调的非线性激活函数。这可以保证单调性。</p><p> Region-dependent temperature scaling</p><p> 同样的改进temperature、分区域不同t的温度缩放</p></blockquote></li><li><p>MC(蒙特卡洛)-Dropout</p><p>在inference中也使用Dropout、对于一个样本的 inference，MC-Dropout 要求随机进行 K 次 dropout，进行 K 次前传，得到 K 个输出结果。而 K 个输出结果再进行 ensemble</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230914145040644.png" alt="image-20230914145040644"></p><h3 id="其他论文（大模型）"><a href="#其他论文（大模型）" class="headerlink" title="其他论文（大模型）"></a>其他论文（大模型）</h3><ol><li><p><a href="https://ar5iv.labs.arxiv.org/html/2205.14334">Teaching models to express their uncertainty in words</a>口头表达的置信度（用语言文本直接微调 confidence</p></li><li><p><a href="https://arxiv.org/abs/2212.10071">Large Language Models Are Reasoning Teachers</a></p><p>一种蒸馏的方式，大模型的输出来微调小模型</p></li><li><p><a href="https://ar5iv.labs.arxiv.org/html/2306.13063">Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs</a></p><p><strong>有两个指标可用于衡量模型的不确定性：模型生成的文本输出以及对同一问题的多次回答之间的一致性</strong></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230831235327273.png" alt="image-20230831235327273"></p><p>​可靠的不确定性估计对于人机协作至关重要，可以促进更加理性和明智的决策。具体而言，准确获得模型的置信度可以为其响应的可靠性提供有价值的洞察，促进风险评估和错误缓解，并减少自然语言生成任务中的错觉。</p><p>​以前的 model 获得置信度主要依赖 <strong>model logit</strong> 及其相关校准技术。校准是指将模型的预测结果调整为更接近真实概率的方法，使得模型的置信度能够更准确地反映实际概率。常用的校准技术包括 <strong>平滑方法、温度缩放、直方图校准</strong> 等。</p><p>​然而对于 LLMs, 使用 model logits 有很多限制，一、<strong>对数几率在许多情况意味着过度自信</strong>（见 paper1）；二、<strong>logits 仅捕捉模型对下一个 token 的不确定性</strong>，而不提供对特定主张可靠性的评估，而人类式回答中期望的是能够评估 <strong>主张本身的可靠性</strong>。例如，在生成报告时，模型明确指示其断言的可靠性非常重要，这不能直接通过标记的 logits 实现。三、<strong>封源 LLMs 其商业化 API 仅允许文本输入和输出，缺乏 model logits 和 embeddings</strong>。因此，这些局限性需要对 LLMs 进行 <em>non-logit-based</em> 方法的不确定性 eclicitation，即 <em>confidence elicitation</em>。</p><p>​本文目标：1）探索不需要模型微调或访问专有信息的置信度唤起方法；2）对它们的表现进行比较分析，以揭示可以提供更准确的不确定性估计（即置信度）的方法和方向。</p><p>​<strong>此文混合了 COT 和 Consistency实现了更好的校准</strong></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230921193858057.png" alt="image-20230921193858057"></p><p>$$Confidence&#x3D;\sigma(W_1\cdot LCE + W_2\cdot NLCE + b)$$</p><p>LCE如果本身很低的话，说明总有些词的预测概率本来就低，整体也不一定可信；然后再加上NLCE的判定结合（NLCE有点道理就是，大模型肯定倾向于自己的答案，所以口头输出的置信度一般都会大于50%，所以当能判别其答案错误时就调低置信度）</p></li><li><p><a href="https://aclanthology.org/2023.findings-acl.624/">Making Pre-trained Language Models both Task-solvers and Self-calibrators</a></p><p>无监督训练置信度</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230721140312656.png" alt="image-20230721140312656"></p><p>​在实践中，我们需要有效地利用原始任务和校准任务的训练样本。提出了三个挑战： - 有限的训练样本：如何有效地利用训练样本来提高校准任务性能，同时保持原始任务性能？ - 数据不平衡：由于 PLMs 具有较高的性能，正样本（被正确分类的样本）在校准训练集中占据显著地位，导致数据不平衡问题。 - 分布偏移：在部署时，PLMs 也需要展示对于分布外（OOD）样本的鲁棒性，给出合理的置信度分数。</p><p>​实验的三种基准方法：（1）Vanilla：将原始预测概率作为置信度估计；（2）温度缩放（TS）：应用温度缩放方法来校准 PLMs 的置信度得分；（3）标签平滑（LS）：应用标签平滑来防止 PLMs 对其预测过于自信</p><p>补充：半监督学习：</p><p><a href="https://zhuanlan.zhihu.com/p/387907614">https://zhuanlan.zhihu.com/p/387907614</a></p><p>PI Model(一个半监督模型的框架)</p><p><img src="https://pic4.zhimg.com/80/v2-1d16af140e9cc3432792ac339db9a85f_720w.webp" alt="img"></p></li><li><p><a href="https://arxiv.org/pdf/2307.10442v1.pdf">Thrust: Adaptively Propels Large Language Models with External Knowledge</a></p><p>工作流程：</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230721131558174.png" alt="image-20230721131558174"></p><blockquote><p>有跟我自己做的 arxiv 检索那个有相关，查到的信息一个是不一定准确，一个是不一定有用，可能有很大噪声</p></blockquote><p>提出了 the Instance-level Adaptive Propulsion of External Knowledge（IAPEK）模块 （which adaptively retrieves external knowledge when it is necessary.）</p><p>给出了 Trust 的置信度估计模块</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230721135338307.png" alt="image-20230721135338307"></p><p>s(q)就是 trust 打分函数</p><blockquote><p>设计原则：注意，等式 1 中ℓ 2 范数内的表达式可以视为从查询向量 f(q)指向质心向量{mkl}的规范化（单位）向量{dkl(q)&#x2F;∥ dkl(q)∥}的加权平均值。权重与聚类大小成正比，与查询和质心之间的平方距离成反比。这样的设计选择基于早期关于知识的表示假设得出的以下原则。首先，当一个任务的样本被良好聚类，并且如果 q 接近其中一个聚类而远离其他聚类，意味着查询实例 q 可以通过 PTLM 的内在知识得到很好地解决，Thrust 评分应该较高。假设 mkl 是 q 靠近的聚类质心，那么我们观察到等式 1 中分母中对应的∥ dkl ∥ 2 项会使得对应的项在计算中占主导地位，并且值较大。其次，如果 q 远离所有的聚类质心，即查询超出了 PTLM 的知识范围，那么二次项∥ dkl ∥ 2 将迅速抑制等式 1 中的所有项，使得 Thrust 评分消失。第三，当 PTLM 不能在其隐藏状态中足够地对任务样本进行聚类时，这意味着 PTLM 没有足够的知识来解决整个任务。在这种情况下，单位向量 dkl(q)&#x2F;∥ dkl(q)∥将随机指向不同的方向，使得等式 1 中ℓ 2 范数内的平均向量减小。最后，我们首先将每个类别中的样本聚合成 K 个簇，然后再计算 Thrust 评分，主要原因是即使属于同一类别，它们仍然可能分布在多个簇中。等式 1 中的|Ckl|项用于加重指向更大簇的矢量 dkl(q)&#x2F;∥ dkl(q)∥。然而，我们发现 K 可以相对较小。</p></blockquote></li><li><p><a href="https://arxiv.org/abs/1906.05664">Calibration, Entropy Rates, and Memory in Language Models</a></p><p>发现了在各种语料库上训练的最先进语言模型中熵放大的普遍现象。基于这一发现，本文的重点有两个方面：一是基于模型长期属性（例如熵率）的任何测量不匹配来改进生成，并提供可证明的保证；二是量化模型预测与远期过去的依赖关系。这两个方面的核心是基于校准的方法，这种方法在统计学和机器学习的其他领域中被使用</p><p>补充：<strong>KL 散度</strong>（Kullback–Leibler divergence，缩写 KLD）是一种统计学度量，表示的是一个概率分布相对于另一个概率分布的差异程度，在信息论中又称为 <strong>相对熵</strong>（Relative entropy）。</p><p>​设离散概率空间 X 上有两个概率分布 P 和 Q, 那么 P 相对于 Q 的 KL 散度定义如下：</p></li></ol><p>​$D_{K L}(P | Q)&#x3D;\sum_{x \in X} P(x) \ln \left(\frac{P(x)}{Q(x)}\right)&#x3D;\sum_{x \in X} P(x)(\ln (P(x))-\ln (Q(x))) \text {. }$<br>   ​对于连续型随机变量，设概率空间 X 上有两个概率分布 P 和 Q, 其概率密度分别为 p 和 q, 那么，P 相对于 Q 的 KL 散度定义如下：</p><p>   ​$D_{K L}(P | Q)&#x3D;\int_{-\infty}^{+\infty} p(x) \ln \left(\frac{p(x)}{q(x)}\right) d x$</p><p>   ​显然，当 P &#x3D; Q 时，DKL &#x3D; 0。<br>​根据 KL 散度的定义，可以知道此度量是没有对称性的。也就是说，P 相对于 Q 和 KL 散度一般并不等于 Q 相对<br>   于 P 的。由于 K 工散度能够衡量两个概率分布之间的差异，现在广泛用于机器学习中，用于评估生成模型所产生的数据<br>分布与实际数据分布之间的差异程度。</p></li></ul></li></ol><h2 id="多分类数据不平衡"><a href="#多分类数据不平衡" class="headerlink" title="多分类数据不平衡"></a>多分类数据不平衡</h2><p>  也会导致overconfidence</p><p>  解决：</p><ul><li><p>数据级方法</p><p>此方案处于与分类器无关的预处理阶段                 </p><ol><li>SMOTE(利用knn线性内插生成少样本, 同时减少多样本) 和 DeepSMOTE（使用生成对抗网络（GANs）来生成更逼真的合成样本）</li></ol></li><li><p>算法级方法</p><p>修改网络和模型的权重、cost-sensitive等</p></li><li><p>Ensemble</p><p>模型集成</p><p> Loss Function:</p></li></ul><p>  在自然语言处理（NLP）中，二进制交叉熵（BCE）损失常用于多标签文本分类（Bengio et al., 2013）。给定一个包含N个训练实例的数据集，每个实例具有一个多标签的真实标签（其中C是类别的数量），以及分类器的输出概率P(y_i)，BCE损失可以定义如下（为简单起见，平均减少步骤未显示出来）：</p><p>  $BCE &#x3D; -1&#x2F;N * Σ [Σ (y_i * log(P(y_i)) + (1 - y_i) * log(1 - P(y_i)))]$</p><pre><code> 通常情况下，纯粹的BCE损失函数容易受到头部类别或负实例的支配而产生标签不平衡的问题（Durand et al., 2019）。以下，我们描述了三种解决多标签文本分类中长尾数据集类别不平衡问题的替代方法。这些平衡方法的主要思想是重新加权BCE，以便罕见的实例-标签组合能够直观地获得合理的“关注”。    令![image-20230906164649369](C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230906164649369.png) 交叉熵损失可写为 $LOSS_&#123;ce&#125; = -log(p_t)$ 1. Focal loss (FL) （Lin et al., 2017） ![image-20230906163115279](C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230906163115279.png) 可以写为$ L_&#123;FL&#125; = -(p_i^k)^γ* log(p_i^k)$  注： inverse focal loss是与Focal loss反面出发 焦点损失通过引入焦点因子来调整损失函数，以便更关注难以分类的样本，从而减轻了类别不平衡问题。焦点因子通常介于0和1之间，用于降低容易分类的样本的权重，同时增加难以分类的样本的权重。这使得模型更加关注那些容易混淆或分类错误的样本，从而提高了模型在少数类别上的性能。 逆焦点损失是焦点损失的一种变种，其主要思想是反转焦点因子的作用。逆焦点损失通过将焦点因子的值设置为大于1的数来增加容易分类的样本的权重，同时将焦点因子的值设置为小于1的数来减少难以分类的样本的权重。这与标准的焦点损失相反，后者更加关注难以分类的样本。 2. Class-balanced focal loss (CB) 3. Distribution-balanced loss (DB) https://blog.csdn.net/weixin_42437114/article/details/127774342</code></pre><ul><li><h3 id="一点小思考"><a href="#一点小思考" class="headerlink" title="一点小思考"></a>一点小思考</h3><ol><li><p>难样本问题</p><p>难分类样本与易分类样本其实是一个动态概念，也就是说 $p_t$ 会随着训练过程而变化。原先易分类样本即$p_t$ 大的样本，可能随着训练过程变化为难训练样本即$p_t$ 小的样本。</p></li></ol><p>上面讲到，由于Loss梯度中，难训练样本起主导作用，即参数的变化主要是朝着优化难训练样本的方向改变。当参数变化后，可能会使原先易训练的样本$p_t$发生变化，即可能变为难训练样本。当这种情况发生时，可能会造成模型收敛速度慢，正如苏剑林在他的文章中提到的那样。</p><p>为了防止难易样本的频繁变化，应当选取小的学习率。防止学习率过大，造成 $w$ 变化较大从而引起 $p_t$的巨大变化，造成难易样本的改变。</p><ol start="2"><li><p>训练大模型时利用负样例和loss函数平滑的思想类似吗？</p></li><li><p>如何表示生成式大模型的置信度</p><ul><li><p>求和或平均：一种简单的方法是将所有token的logits相加或取平均。这可以给出一个粗略的整体置信度指标，但可能无法捕捉到序列中不同部分的重要性。</p></li><li><p>加权平均：我们可以根据每个token的位置或其他因素给予不同的权重。例如，序列开始部分的token可能比后面的token更重要。</p></li><li><p>序列模型：我们可以使用额外的序列模型（如RNN或Transformer）来处理整个logits序列，并输出一个整体置信度。</p></li><li><p>注意力机制：我们可以使用注意力机制来确定模型应该关注序列中的哪些部分。这可以帮助模型更好地理解整个序列，并提高整体置信度的准确性。</p></li></ul><p>如何提高大模型的置信度</p><p>如何让大模型认识到自己的知识边界</p></li></ol></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习</title>
      <link href="/2023/03/08/nlp/"/>
      <url>/2023/03/08/nlp/</url>
      
        <content type="html"><![CDATA[<p>摘抄得刘知远老师的仓库：<a href="https://github.com/zibuyu/research_tao">https://github.com/zibuyu/research_tao</a></p><h2 id="What-is-NLP"><a href="#What-is-NLP" class="headerlink" title="What is NLP?"></a>What is NLP?</h2><p>理解并处理自然语言</p><h3 id="研究内容："><a href="#研究内容：" class="headerlink" title="研究内容："></a>研究内容：</h3><p>语法分析、语义分析、篇章理解等</p><h3 id="难题："><a href="#难题：" class="headerlink" title="难题："></a>难题：</h3><p>歧义解决</p><h3 id="nlp-amp-cv"><a href="#nlp-amp-cv" class="headerlink" title="nlp &amp; cv"></a>nlp &amp; cv</h3><p><img src="https://github.com/zibuyu/research_tao/raw/master/figures/00_nlp_vs_cv.jpg" alt="image"></p><p>进入高层任务后，两个领域都将面临共同的关键挑战，都可以归结为复杂语境下的多对象（图像中是不同对象，文本中是不同概念）的语义组合问题</p><h3 id="中英文nlp差异"><a href="#中英文nlp差异" class="headerlink" title="中英文nlp差异"></a>中英文nlp差异</h3><p>在词性分析、隐性表示上具有很大</p><h3 id="学术期刊-amp-会议"><a href="#学术期刊-amp-会议" class="headerlink" title="学术期刊&amp;会议"></a>学术期刊&amp;会议</h3><p>NLP&#x2F;CL的高水平学术成果主要分布在ACL、NAACL、EMNLP和COLING等几个学术会议上</p><p><a href="https://github.com/zibuyu/research_tao/blob/master/01_community.md">https://github.com/zibuyu/research_tao/blob/master/01_community.md</a></p><p>根据Google Scholar Metrics 2018年发布的NLP&#x2F;CL学术期刊和会议论文引用排名，ACL、EMNLP、NAACL、SemEval、TACL、LREC位于前6位，基本反映了本领域学者的关注程度。其中ACL、EMNLP、NAACL的H5-Index和H5-Median明显高于其他会议和期刊，也是该领域每年参会人数最多的会议，可谓NLP&#x2F;CL的三大顶级国际会议。</p><p>ACL wiki:<a href="https://aclweb.org/aclwiki/Main_Page">https://aclweb.org/aclwiki/Main_Page</a></p><h4 id="人工智能领域"><a href="#人工智能领域" class="headerlink" title="人工智能领域"></a>人工智能领域</h4><p>人工智能领域相关学术会议包括IJCAI和AAAI。AAAI全称美国人工智能年会，IJCAI全称人工智能国际联合大会。这两个会议方向非常广泛，涵盖机器人、知识、规划、自然语言处理、机器学习、计算机视觉等几乎所有AI子领域，是AI领域“奥运会”式的学术会议。近年来，由于AI领域备受社会各界关注，这两个会议的录用论文数也成倍增长。以AAAI 2019为例，投稿数猛增至7000多篇，最终录用1150篇，录用率降低至16.2%。有些老师在社交媒体上如此评价，AAAI&#x2F;IJCAI更像花样齐全的“奥运会”，而ACL&#x2F;EMNLP&#x2F;NAACL更像专业领域的“锦标赛”，所以一般对专业领域任务的精细研究，更多发表在锦标赛式的专业会议上。由于知识表示等方向没有更权威的专门学术会议，所以更多发表在AAAI&#x2F;IJCAI上。人工智能领域相关学术期刊包括Artificial Intelligence、Journal of AI Research。</p><h4 id="机器学习领域"><a href="#机器学习领域" class="headerlink" title="机器学习领域"></a>机器学习领域</h4><p>机器学习领域相关学术会议包括ICML，NIPS，ICLR、AISTATS等。其中NIPS全称是Conference on Neural Information Processing Systems，由于最近这波AI浪潮就源自以神经网络技术为基础的深度学习，所以近年来备受关注，参会人数倍增，近几年会议注册页面刚开放就会被抢注一空。树大招风，2018年由于NIPS缩写有性别歧视的意味，所以从2019年开始更名为了NeurIPS。ICLR是深度学习兴起后在2013年创立的年轻会议，采用的开放审稿模式，整个审稿过程的审稿意见、作者回复全部实时公开，也允许其他围观用户评论，面貌一新，关注者众，颇领一时风气之先。机器学习领域相关学术期刊主要包括Journal of Machine Learning Research（JMLR）和Machine Learning（ML）等。</p><h4 id="信息检索和数据挖掘领域"><a href="#信息检索和数据挖掘领域" class="headerlink" title="信息检索和数据挖掘领域"></a>信息检索和数据挖掘领域</h4><p>信息检索和数据挖掘领域相关学术会议主要由美国计算机学会（ACM）主办，包括SIGIR、KDD、WWW（从2018年开始更名为The Web Conference）、WSDM。信息检索和数据挖掘领域相关学术期刊包括ACM TOIS、IEEE TKDE、ACM TKDD、ACM TIST等。其中ACM TOIS和IEEE TKDE历史比较悠久，地位卓然；ACM TKDD则创立于2007年，ACM TIST创立于2010年，均为新兴的著名期刊，特别是ACM TIST创刊时就邀请了LibSVM等有影响力的成果发表，现在SCI影响因子比较高。</p><h4 id="国内"><a href="#国内" class="headerlink" title="国内"></a>国内</h4><blockquote><p>中国计算机学会（CCF）制定了“中国计算机学会推荐国际学术会议和期刊目录”，基本公允地列出了每个领域的高水平期刊与会议。大家可以通过这个列表，迅速了解每个领域的主要期刊与学术会议。</p></blockquote><p>与国际学术组织和会议相似，国内也有一家与NLP&#x2F;CL相关的专业学术组织，中国中文信息学会（CIPS，<a href="http://www.cipsc.org.cn/">http://www.cipsc.org.cn/</a> ），是国内最大的自然语言处理学术组织，最早由著名科学家钱伟长先生发起成立。通过学会的理事名单（<a href="http://www.cipsc.org.cn/lingdao.php">http://www.cipsc.org.cn/lingdao.php</a> ）基本可以了解国内从事NLP&#x2F;CL的主要单位和学者。中文信息学会每年组织很多学术会议，例如全国计算语言学学术会议（CCL）、中国自然语言处理青年学者研讨会（YSSNLP）、全国信息检索学术会议（CCIR）、全国机器翻译研讨会（CWMT）等，是国内NLP&#x2F;CL学者进行学术交流的重要平台。尤其值得一提的是，YSSNLP是专门面向国内NLP&#x2F;CL青年学者的研讨交流会，采用邀请制参加，大家自愿报名在研讨会上报告学术前沿动态，是国内NLP&#x2F;CL青年学者进行学术交流、建立学术合作的绝佳平台。2010年的COLING和2015年的ACL在北京召开，均由中文信息学会负责组织工作，这在一定程度上反映了学会在国内NLP&#x2F;CL领域的重要地位。此外，计算机学会中文信息技术专委会组织的自然语言处理与中文计算会议（NLP&amp;CC）是最近崛起的国内重要NLP&#x2F;CL学术会议。中文信息学会主编了一份历史悠久的《中文信息学报》，是国内该领域的重要学术期刊，发表过很多篇重量级论文。此外，国内著名的《计算机学报》、《软件学报》等期刊上也经常有NLP&#x2F;CL论文发表，值得关注。</p><h5 id="全国计算语言学大会（CCL）"><a href="#全国计算语言学大会（CCL）" class="headerlink" title="全国计算语言学大会（CCL）"></a>全国计算语言学大会（CCL）</h5><h5 id="全国知识图谱与语义计算大会（CCKS）"><a href="#全国知识图谱与语义计算大会（CCKS）" class="headerlink" title="全国知识图谱与语义计算大会（CCKS）"></a>全国知识图谱与语义计算大会（CCKS）</h5><h5 id="全国社会媒体处理大会（SMP）"><a href="#全国社会媒体处理大会（SMP）" class="headerlink" title="全国社会媒体处理大会（SMP）"></a>全国社会媒体处理大会（SMP）</h5><h5 id="全国信息检索学术会议（CCIR）"><a href="#全国信息检索学术会议（CCIR）" class="headerlink" title="全国信息检索学术会议（CCIR）"></a>全国信息检索学术会议（CCIR）</h5><h5 id="全国机器翻译研讨会（CWMT）"><a href="#全国机器翻译研讨会（CWMT）" class="headerlink" title="全国机器翻译研讨会（CWMT）"></a>全国机器翻译研讨会（CWMT）</h5><h5 id="自然语言处理青年学者研讨会（YSSNLP）"><a href="#自然语言处理青年学者研讨会（YSSNLP）" class="headerlink" title="自然语言处理青年学者研讨会（YSSNLP）"></a>自然语言处理青年学者研讨会（YSSNLP）</h5><h5 id="CIPS暑期学校（CIPS-Summer-School）"><a href="#CIPS暑期学校（CIPS-Summer-School）" class="headerlink" title="CIPS暑期学校（CIPS Summer School）"></a>CIPS暑期学校（CIPS Summer School）</h5><h5 id="CCF国际自然语言处理与中文计算会议（NLPCC）"><a href="#CCF国际自然语言处理与中文计算会议（NLPCC）" class="headerlink" title="CCF国际自然语言处理与中文计算会议（NLPCC）"></a>CCF国际自然语言处理与中文计算会议（NLPCC）</h5><h3 id="Reading-paper"><a href="#Reading-paper" class="headerlink" title="Reading paper"></a>Reading paper</h3><p><a href="https://github.com/zibuyu/research_tao/blob/master/02_reading_paper.md">https://github.com/zibuyu/research_tao/blob/master/02_reading_paper.md</a></p><p>阅读论文也不必需要每篇都从头到尾看完。一篇学术论文通常包括以下结构，我们用序号来标记建议的阅读顺序：</p><ul><li>题目（1）</li><li>摘要（2）</li><li>正文：导论（3）、相关工作（6）、本文工作（5）、实验结果（4）、结论（7）</li><li>参考文献（6）</li><li>附录</li></ul><p>按照这个顺序，基本在读完题目和摘要后，大致可以判断这篇论文与自己研究课题的相关性，然后就可以决定是否要精读导论和实验结果判断学术价值，是否阅读本文工作了解方法细节。此外，如果希望了解相关工作和未来工作，则可以有针对性地阅读“相关工作”和“结论”等部分。</p><h3 id="Where’s-Idea"><a href="#Where’s-Idea" class="headerlink" title="Where’s Idea?"></a>Where’s Idea?</h3><p><strong>实践法</strong>。即在研究任务上实现已有最好的算法，通过分析实验结果，例如发现这些算法计算复杂度特别高、训练收敛特别慢，或者发现该算法的错误样例呈现明显的规律，都可以启发你改进已有算法的思路。现在很多自然语言处理任务的Leaderboard上的最新算法，就是通过分析错误样例来有针对性改进算法的 [1]。</p><p><strong>类比法</strong>。即将研究问题与其他任务建立类比联系，调研其他相似任务上最新的有效思想、算法或工具，通过合理的转换迁移，运用到当前的研究问题上来。例如，当初注意力机制在神经网络机器翻译中大获成功，当时主要是在词级别建立注意力，后来我们课题组的林衍凯和沈世奇提出建立句子级别的注意力解决关系抽取的远程监督训练数据的标注噪音问题 [2]，这就是一种类比的做法。</p><p><strong>组合法</strong>。即将新的研究问题分解为若干已被较好解决的子问题，通过有机地组合这些子问题上的最好做法，建立对新的研究问题的解决方案。例如，我们提出的融合知识图谱的预训练语言模型，就是将BERT和TransE等已有算法融合起来建立的新模型 [3]。</p><p>正如武侠中的最高境界是无招胜有招，好的研究想法并不拘泥于以上的路径，很多时候是在研究者对研究问题深刻认知的基础上，综合丰富的研究阅历和聪明才智产生”顿悟“的结果。这对初学者而言恐怕还很难一窥门径，需要从基本功做起，经过大量科研实践训练后，才能有登堂入室之感。</p><p>在科研实践过程中，除了通过大量文献阅读了解历史，通过深入思考总结产生洞察力外，还有一项必不可少的工作，那就是主动开放的学术交流和合作意识。不同研究领域思想和成果交流碰撞，既为创新思想提供了新的来源，也为”类比“和”顿悟“提供了机会。了解一下历史就可以知晓，人工智能的提出，就是数学、计算机科学、控制论、信息论、脑科学等学科交叉融合的产物。而当红的深度学习的起源，1980年代的Parallel Distributed Processing （PDP），也是计算机科学、脑认知科学、心理学、生物学等领域研究者通力合作的产物。</p><h3 id="如何写一篇论文"><a href="#如何写一篇论文" class="headerlink" title="如何写一篇论文"></a>如何写一篇论文</h3><p><a href="https://github.com/zibuyu/research_tao/blob/master/04_writing_paper.md">https://github.com/zibuyu/research_tao/blob/master/04_writing_paper.md</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>位置编码</title>
      <link href="/2023/03/08/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"/>
      <url>/2023/03/08/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h1 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h1><h2 id="起源-Transformer"><a href="#起源-Transformer" class="headerlink" title="起源-Transformer"></a>起源-Transformer</h2><p>transformer提出了经典的Sinusoidal位置编码，然后各种位置编码被陆续提出，也产生了一些变体Transformer如（Reformer）</p><h4 id="思考一：位置编码是干嘛，什么是好的位置编码方案"><a href="#思考一：位置编码是干嘛，什么是好的位置编码方案" class="headerlink" title="思考一：位置编码是干嘛，什么是好的位置编码方案"></a><strong>思考一</strong>：位置编码是干嘛，什么是好的位置编码方案</h4><p>位置编码是表示字句时序位置的编码，因为Transformer这种纯靠attention的模型，无法天然的获取位置信息（即改变attention的内部顺序得到的结果没有变）</p><p>好的位置编码：</p><ol><li>对于给定的位置，它的位置编码是唯一的 （绝对和相对按道理都应该这样）</li><li>不同长度的句子之间，任何两个时间步之间的距离应该尽量一致</li><li>模型是很容易泛化到更长句子的  （最近的Longtext研究给了一些泛化方案）</li></ol><p>但是具体来说还需要考虑计算复杂度，具体下游任务的实际实验效果</p><h4 id="疑惑一：-为什么position-encoding就直接加到embedding向量上了"><a href="#疑惑一：-为什么position-encoding就直接加到embedding向量上了" class="headerlink" title="疑惑一： 为什么position encoding就直接加到embedding向量上了"></a>疑惑一： 为什么position encoding就直接加到embedding向量上了</h4><p>根据网上的理解，embedding本质就是onehot进行全连接，所以coding之后相加其实等价于coding之前torch.cat之后再进行一个大的全连接。<strong>所以相加相当于一个特征的融合</strong>，相加也符合向量空间关系的一种折中，Bert coding的时候相加也可以相同理解</p><p>那为什么不能是向量相乘呢（后续也有相关工作）</p><h2 id="绝对位置编码"><a href="#绝对位置编码" class="headerlink" title="绝对位置编码"></a>绝对位置编码</h2><h3 id="三角式"><a href="#三角式" class="headerlink" title="三角式"></a>三角式</h3><p><a href="https://kexue.fm/archives/8231">Sinusoidal位置编码</a></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230713211624459.png" alt="image-20230713211624459"></p><p>其中 pk,2i, pk,2i+1 分别是位置 k 的编码向量的第 2i,2i+1 个分量，d 是位置向量的维度。</p><p>是绝对位置编码，但含有相对位置信息，推导可以从泰勒展开理解，但仅限二维，所以可解释性差</p><h3 id="递归式"><a href="#递归式" class="headerlink" title="递归式"></a>递归式</h3><p>论文：<a href="https://arxiv.org/abs/2003.09229">Learning to Encode Position for Transformer with Continuous Dynamical Model</a></p><p>思想就是通过 RNN 结构来学习一种编码方案，外推性较好，但牺牲了并行性，可能会带来速度瓶颈</p><h3 id="相乘式"><a href="#相乘式" class="headerlink" title="相乘式"></a>相乘式</h3><p>博客：<a href="https://zhuanlan.zhihu.com/p/183234823">https://zhuanlan.zhihu.com/p/183234823</a></p><h3 id="RoPE旋转位置编码"><a href="#RoPE旋转位置编码" class="headerlink" title="RoPE旋转位置编码"></a>RoPE旋转位置编码</h3><p>也是绝对位置编码。</p><p>二维形式：<img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230713212328868.png" alt="image-20230713212328868"></p><p>根据矩阵乘法的几何意义可以知道，此时实际上代表着<strong>query向量的旋转</strong>，所以取名旋转位置编码。</p><p>动机：原来的Sinusoidal编码不够好</p><p>作者苏剑林是从向量的内积表示两个向量的位置关系出发，通过复数表示（两个复向量的内积为复向量*复向量的共轭），推导出了这样一个旋转位置编码，更有可解释性，从预训练模型 RoFormer 的结果来看，RoPE 具有良好的外推性，应用到 Transformer 中体现出较好的处理长文本的能力。且能作用于<strong>线性attention</strong>（Transformer的attention为二阶复杂度），因为编码矩阵是正交矩阵且直接作用于query和key，不改变向量模长。</p><p>偶数多维：</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230713213830123.png" alt="image-20230713213830123"></p><p>由于$R_{n-m}$是正交矩阵，不改变向量模长，所以应该不会改变模型的稳定性</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230713214036206.png" alt="image-20230713214036206"></p><p>苏剑林还想到将这样一个稀疏矩阵乘积化成</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230713214257770.png" alt="image-20230713214257770"></p><p>既降低计算的复杂度，使RoPE可以视为<strong>乘性位置编码的变体</strong></p><p>最近几天国外网友推出的NTK-Aware Scaled RoPE，使苏剑林提出了：从 <code>β</code> 进制编码的角度理解 RoPE，放在后面讲</p><p>一些使用RoPE的模型（<a href="https://github.com/ZhuiyiTechnology/roformer">ReFormer</a>（苏剑林自己开源的）、<a href="https://github.com/bojone/GlobalPointer">GlobalPoint</a>）</p><h2 id="相对位置编码"><a href="#相对位置编码" class="headerlink" title="相对位置编码"></a>相对位置编码</h2><h3 id="经典式"><a href="#经典式" class="headerlink" title="经典式"></a>经典式</h3><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230710111543547.png" alt="image-20230710111543547"></p><p><strong>只需要有限个位置编码，就可以表达出任意长度的相对位置（因为进行了截断）</strong></p><h3 id="XLNET式"><a href="#XLNET式" class="headerlink" title="XLNET式"></a>XLNET式</h3><p><a href="https://arxiv.org/abs/1901.02860">《Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context》</a></p><p>位置编码：</p><p>$\boldsymbol{q}<em>{i} \boldsymbol{k}</em>{j}^{\top}&#x3D;\boldsymbol{x}<em>{i} \boldsymbol{W}</em>{Q} \boldsymbol{W}<em>{K}^{\top} \boldsymbol{x}</em>{j}^{\top}+\boldsymbol{x}<em>{i} \boldsymbol{W}</em>{Q} \boldsymbol{W}<em>{K}^{\top} \boldsymbol{p}</em>{j}^{\top}+\boldsymbol{p}<em>{i} \boldsymbol{W}</em>{Q} \boldsymbol{W}<em>{K}^{\top} \boldsymbol{x}</em>{j}^{\top}+\boldsymbol{p}<em>{i} \boldsymbol{W}</em>{Q} \boldsymbol{W}<em>{K}^{\top} \boldsymbol{p}</em>{j}^{\top} —(*)$</p><p>最终：</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230710111624848.png" alt="image-20230710111624848"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230710111655900.png" alt="image-20230710111655900"></p><p>相对位置矩阵只加到 attention 矩阵上，不加到 $v_j$ 上去了，后续的工作也都如此</p><h3 id="T5-式"><a href="#T5-式" class="headerlink" title="T5 式"></a>T5 式</h3><p><a href="https://arxiv.org/abs/1910.10683">《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》</a></p><p>*式可以理解为“输入-输入”、“输入-位置”、“位置-输入”、“位置-位置”四项注意力的组合，如果我们认为输入信息与位置信息应该是独立（解耦）的，那么它们就不应该有过多的交互，所以“输入-位置”、“位置-输入”两项 Attention 可以删掉。</p><p>而 <img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230710111848513.png" alt="image-20230710111848513"> 实际上只是一个只依赖于(i, j)的标量，我们可以直接将它作为参数训练出来，即简化为 <img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230710111907614.png" alt="image-20230710111907614"></p><p>说白了，它仅仅是在 Attention 矩阵的基础上加一个可训练的偏置项而已，而跟 XLNET 式一样，在 $v_j$ 上的位置偏置则直接被去掉了</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230710125334026.png" alt="image-20230710125334026"></p><p>包含同样的思想的还有微软在ICLR 2021的论文<a href="https://arxiv.org/abs/2006.15595">《Rethinking Positional Encoding in Language Pre-training》</a>中提出的TUPE位置编码</p><p>TUPE位置编码中还通过重置与[CLS]相关的位置相关性来解除[CLS]</p><h3 id="DoBERTa-式"><a href="#DoBERTa-式" class="headerlink" title="DoBERTa 式"></a>DoBERTa 式</h3><p>DeBERTa 和 T5 刚刚相反，它扔掉了第 4 项，保留第 2、3 项并且替换为相对位置编码</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230710125841040.png" alt="image-20230710125841040"></p><p>语录：&#x3D;&#x3D;科研就是枚举所有排列组合看哪个更优&#x3D;&#x3D;</p><h3 id="LongText最新进展："><a href="#LongText最新进展：" class="headerlink" title="LongText最新进展："></a>LongText最新进展：</h3><ol><li>baseline 直接外推</li><li>SuperHOT LoRA 线性内插+微调  同时还有Meta的<a href="https://arxiv.org/abs/2306.15595">《Extending Context Window of Large Language Models via Positional Interpolation》</a></li></ol><p>首先是进制思想</p><p>其次线性内插其实简单来说就是将2000以内压缩到1000以内</p><p><img src="https://kexue.fm/usr/uploads/2023/07/4113541717.png" alt="img"></p><p>原本模型已经学会了875&gt;874, 现在泛化一个874.5&gt;874应该不会太难</p><ol start="3"><li>NBCE (Naive Bayes-based Context Extension)   <a href="https://kexue.fm/archives/9617">https://kexue.fm/archives/9617</a>  (之前苏剑林根据朴素贝叶斯提出的一个东西，他测试不微调就可以扩展Context长度)</li></ol><p><a href="https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E6%95%B0%E5%AD%A6%E8%AF%BE/10%20%20%E4%BF%A1%E6%81%AF%E7%86%B5%EF%BC%9A%E4%BA%8B%E4%BB%B6%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%EF%BC%9F.md">条件熵</a></p><ol start="4"><li>NTK-Aware Scaled RoPE (不微调就很好，微调了可能更好)    <a href="https://kexue.fm/archives/9675">Transformer 升级之路：10、RoPE 是一种β进制编码</a></li></ol><p>一方面可以从进制方面理解，另一方面可以从高频外推，低频内插理解</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230714000941973.png" alt="image-20230714000941973"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230714001057302.png" alt="image-20230714001057302"></p><p>这个扩增方案就能解释直接外推方案就是啥也不改，内插方案就是将n换成n&#x2F;k。</p><p>进制转换，就是要扩大k倍表示范围，那么原本的β进制至少要扩大成$β(k^{2&#x2F;d})$进制或者等价地原来的底数10000换成10000k</p><p>这其实就是NTK-Aware Scaled RoPE （苏剑林的推导）</p><p>提出者的推导：高频外推、低频内插</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230714001923448.png" alt="image-20230714001923448"></p><p>苏剑林的实验中还发现NTK-RoPE在“重复”外推上比“不重复”外推效果明显好，表明这样修改之后是保留了全局依赖，而不是单纯将注意力局部化</p><ol start="5"><li><a href="https://arxiv.org/pdf/2307.03172.pdf">Myth of Context Length</a> ：  Stanford  &amp;  UC Berkeley  &amp;   Samaya AI</li></ol><ul><li>扩展上下文的模型不一定更擅长利用其输入上下文</li></ul><p>eg: longchat在140个键值设置中，longchat是一个显著的异常值；当相关信息在输入上下文的开头时， 它倾向于生成用于检索键的代码，而不是输出值本身。</p><ul><li>与其基准模型（即在指令微调之前）MPT30B相比，MPT-30B-Instruct在多文档问答中的性能表现进行了对比。这两个模型都具有一个呈U型的性能曲线，当相关信息出现在输入上下文的开头 或结尾时，性能显著提高，这表明指令调优过程本 身不一定是造成这些性能趋势的原因</li><li>查询query和数据data的顺序对于decoder-only模型？（decoder-only模型在每个时间步只能关注先前标记的方式来处理）</li></ul><p>Query-Aware Contextualization显著提高key-value retrieval task， 对多文档问题提升不大（放在开头稍好）</p><p>结论： 有监督 的指令微调数据中，任务规范和&#x2F;或指令通常 放置在输入上下文的开头，这可能导致经过指 令微调的语言模型更重视输入上下文的开头部 分</p><ul><li>根据下游任务权衡上游模型。提供更多信息给经过训练的 指令型语言模型，可能有助于提高下游任务的 性能，但也会增加模型需要处理的内容量。</li></ul><p>&#x3D;&#x3D;只做了实验探究，没有给出合理的解释，只给出了一个人类心理学现象作为类比&#x3D;&#x3D;</p><p>​6. softmax_1:<a href="https://www.evanmiller.org/attention-is-off-by-one.html?continueFlag=5d0e431f4edf1d8cccea47871e82fbc4">https://www.evanmiller.org/attention-is-off-by-one.html?continueFlag=5d0e431f4edf1d8cccea47871e82fbc4</a></p><h4 id="思考二："><a href="#思考二：" class="headerlink" title="思考二："></a>思考二：</h4><ol><li>线性内插当处理范围更大时，内插方案的维度（先是个位，后十位）会压缩得更拥挤，每个维度的极限密度（达到性能瓶颈）是多少</li></ol><p>​        这应该取决于具体的计算资源、内存限制和线性内插算法的效率</p><ol start="2"><li><p>在具体的下游任务上评估线性内插压缩的程度的影响，不同的下游任务可能是不是选不同的k</p></li><li><p>为什么在transformer这类模型中，长文本时同样更容易注意两端文本</p></li><li><p>transformer的改进（一直都在进行的工作）</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>语言模型</title>
      <link href="/2023/03/08/%E5%A4%8F%E5%AD%A3%E5%88%9B%E6%96%B0%E8%AF%BE%E6%8A%A5%E5%91%8A/"/>
      <url>/2023/03/08/%E5%A4%8F%E5%AD%A3%E5%88%9B%E6%96%B0%E8%AF%BE%E6%8A%A5%E5%91%8A/</url>
      
        <content type="html"><![CDATA[<h1 id="夏季创新课报告-语言模型"><a href="#夏季创新课报告-语言模型" class="headerlink" title="夏季创新课报告-语言模型"></a>夏季创新课报告-语言模型</h1><h2 id="学号：2021112905姓名：徐浩铭"><a href="#学号：2021112905姓名：徐浩铭" class="headerlink" title="学号：2021112905姓名：徐浩铭"></a>学号：2021112905姓名：徐浩铭</h2><ol><li><p>从智能体出发</p><ol><li>介绍了chatGPT的背景和作用。chatGPT是OpenAI公司开发的对话机器人,标志着自然语言处理技术取得突破,从弱AI向强AI转变。</li><li>从语言模型的角度分析chatGPT。chatGPT的核心是大型语言模型,可以处理自然语言的远距离依赖关系,进行语义计算,具备多任务处理能力。</li><li>从知识的角度分析chatGPT。chatGPT实现了知识的统一,是大知识和富知识,改变了人类的知识获取方式。</li><li>分析了chatGPT对互联网内容生成、社交媒体、各行各业的影响。</li><li>探讨了人工智能产业的发展趋势。chatGPT带来新的chatGPT+X模式,形成新的产业集群和创新网络。</li></ol><p>&#x3D;&#x3D;智能体才是未来&#x3D;&#x3D;</p></li><li><p>n-gram</p><ol><li><p>介绍了语言现象、语料库、如何从语料库中学习语言知识的基本思路。</p></li><li><p>介绍了语言模型n-gram的相关内容: </p><p>(1) 定义了语言模型的两个作用: (a) 计算一个词序列s出现的概率p(s),判断s是否是一个合法的语言现象。 (b) 在给定上下文context的条件下,预测下一个词w的概率p(w|context)。</p><p> (2) 介绍了利用语料库训练语言模型n-gram的过程:</p><p>​ (a) 对语料库中的文本进行分词,获得词序列。</p><p>​ (b) 统计词频,得到每个词w的出现次数c(w)。</p><p>​ (c) 统计bigram频率,得到每个词对(w1,w2)的出现次数c(w1,w2)。<br>​(d) 统计trigram频率,得到每个词组(w1,w2,w3)的出现次数c(w1,w2,w3)。</p><p>​ (e) 将这些统计结果保存为unigram、bigram、trigram模型。</p><p> (3) 介绍了如何利用n-gram模型计算句子概率,以及平滑处理方法。 (4) 介绍了n-gram模型的评价方法和下一词预测方法。 (5) 介绍了困惑度可以评价n-gram模型的效果，分析了n-gram模型的优缺点。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230724102136724.png" alt="image-20230724102136724"></p><p>(6)总结了n-gram模型的优缺点: (a) 优点:无监督训练,利用近义词相关性。 (b) 缺点:n限制在3以下,只用了短距离相关性。</p></li><li><p>介绍了词典tokenize的两种方法:中文分词和BPE英文子词切分法。</p><p>BPE英文字词分法：<a href="https://zhuanlan.zhihu.com/p/383650769">https://zhuanlan.zhihu.com/p/383650769</a></p></li><li><p>结尾部分,讨论了语言模型的why、what和how三个问题。 why:阐述了学习语言模型的意义在于服务于智能体。 what:概述了语言模型的发展历程。 how:分析了语言模型学习资料的局限性,需要多方查阅。</p></li></ol></li><li><p>词向量</p><ol><li><p>导言部分提出语言模型的三大任务:学习词语义、文本理解和文本生成。</p></li><li><p>介绍了表示词语义的两种方法: </p><p>(1) 建立知识库的方法获取词语义, <strong>类似字典</strong>，面临语料覆盖有限、无法判断上下文词义的问题。</p><p>(2) 统计语言模型n-gram，对词进行了符号表示。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230724103037912.png" alt="image-20230724103037912"></p><p>(3) one-hot编码：给定一个词典V，将每个词表示为一个二进制向量。**向量的维度为|V|**，它采用词表中每个词的索引作为二进制编码的位置，只有该位置上取值为1，其他位置的取值都为0。这种词的编码，称为one-hot编码。</p><ul><li><p>one-hot向量是词的一种符号表示，没有表示出词的语义。</p></li><li><p>one-hot编码是个高维、稀疏的向量。向量维度是词典的大小。</p></li><li><p>one-hot向量是正交的，不能计算两个词之间的相似度。</p></li></ul><p> (4) 利用上下文学习词向量的<strong>分布式表示方法</strong>(<strong>上下文相似的词，有着相似的语义</strong>)。</p></li><li><p>介绍了词编码的两种表示: </p><p>(1) One-hot 高维稀疏的离散表示,无法计算词义相关性。<br>(2) &#x3D;&#x3D;词向量&#x3D;&#x3D;低维稠密的连续实数表示,可以表示词义。</p></li><li><p>详细介绍了Bengio等人提出的神经网络语言模型NNLM: </p><p>(1) 模型结构:输入层、词向量层、隐层、输出层。详述了每个层的计算公式。</p><p> (2) 模型参数:词向量矩阵E,隐层矩阵W、偏置b,输出层矩阵W、偏置b。</p><p>(3) 目标函数设计采用最大对数似然估计。</p><p> (4) 通过学习词向量矩阵E获得词向量。</p></li><li><p>介绍了Mikolov的CBOW模型: </p><p>(1) 模型结构:输入层、词向量层、隐层、输出层。详述了每个层的计算公式。</p><p> (2) 模型参数:上下文词向量矩阵E,输出词向量矩阵E’。 </p><p>(3) 目标函数采用对数似然估计。 </p><p>(4) 通过输入词上下文,学习输出词的词向量。</p><ul><li><p>每个词的词向量是一个低维稠密的实数向量，学习了词的语义表示。</p></li><li><p>每个词被映射到一个固定维度的向量空间。语义相近的词在空间中的距离较近。通过计算两个词向量之间的相似度，也学习到两个词之间的语义相关性。</p></li></ul></li></ol></li><li><p>神经网络</p><ol><li>神经网络的思想从神经元得来</li><li>激活函数、前馈网络、文本分类、情感分析</li></ol></li><li><p>RNN</p><ol><li><p>介绍了循环神经网络语言模型RNN LM:</p><p>(1) RNN模型结构包括输入层、隐层和输出层。隐层保存历史信息。</p><p>(2) RNN语言模型的输入为完整的上下文序列,输出为下一个词。克服了固定窗口长度的限制。</p><p>(3) 详细介绍了RNN语言模型的输入层、隐层和输出层的计算公式。</p><p>(4) RNN语言模型可以用来文本生成。</p></li><li><p>分析了RNN的不同任务:</p><p>(1) one-one:多层感知机MLP。</p><p>(2) one-many:图像标题生成。</p><p>(3) many-one:文本生成、情感分类。</p><p>(4) many-many:序列标注、seq2seq编解码</p><p>(5) Encoder-Decoder:机器翻译。</p></li><li><p>介绍了RNN的改进模型LSTM:</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/img/image-20230724105758026.png" alt="image-20230724105758026"></p><p>(1) LSTM在RNN基础上增加了记忆单元。</p><p>(2) LSTM单元详细介绍了输入、输出和各个门的计算。</p><p>(3) LSTM引入了门控机制解决了RNN的梯度问题。</p></li><li><p>RNN的本质分析:</p><p>(1) RNN是一个序列模型,可以处理变长序列,编码历史序列信息。</p><p>(2) RNN通过隐层保存历史,实现序列间的依赖学习。</p><p>(3) RNN是一个通用的序列学习框架,可用于多种不同的序列任务。</p><p>(4) RNN的改进模型LSTM增强了 RNN 在较长序列上的记忆能力。</p></li></ol></li><li><p>attention+Transformer</p><ol><li><p>编码器-解码器结构的机器翻译模型（seq2seq）存在的问题:</p><p>（1）Encoder（RNN）生成的上下文向量，缺乏远距离的语言依赖关系。（2）对Decoder的当前状态所对应的语言依赖关系，也缺乏指导。</p></li><li><p>注意力机制的原理:计算查询和键值之间的相关性,对值进行加权求和作为输出。可以建模远距离依赖关系。</p><p>eg: TextRNN+attention进行文本分类任务</p></li><li><p>自注意力机制的本质:利用上下文中的词为目标词编码,计算目标词与上下文词的相关性作为权重,上下文词的词向量作为特征值,加权求和作为目标词的表示。</p></li><li><p>掩码自注意力机制:防止目标词看到后面的词,只利用前面的上下文进行编码。</p></li><li><p>多头自注意力机制:使用多个不同的键、值、查询投影,获得多个子空间的表示,提高模型表达能力。</p></li><li><p>Transformer的组成:输入模块、Encoder模块、Decoder模块、输出模块。核心是stacked的Encoder和Decoder中的多头自注意力机制和前馈全连接网络。</p></li><li><p>Transformer的预训练:最大化语料概率;微调:最大化人工标注语料的概率。</p></li><li><p>典型的预训练语言模型GPT和BERT的区别:前者自回归预测下一个词,后者掩码预测被遮蔽的词。</p></li><li><p>Transformer利用注意力机制有效建模全局依赖关系,是当前自然语言处理中主流的模型结构。</p></li></ol></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>使用 2048 源代码构建 2048 镜像</title>
      <link href="/2023/03/08/%E4%BD%BF%E7%94%A8%202048%20%E6%BA%90%E4%BB%A3%E7%A0%81%E6%9E%84%E5%BB%BA%202048%20%E9%95%9C%E5%83%8F/"/>
      <url>/2023/03/08/%E4%BD%BF%E7%94%A8%202048%20%E6%BA%90%E4%BB%A3%E7%A0%81%E6%9E%84%E5%BB%BA%202048%20%E9%95%9C%E5%83%8F/</url>
      
        <content type="html"><![CDATA[<ol><li><p>使用 2048 源代码构建 2048 镜像 </p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230628154549433.png" alt="image-20230628154549433"></p></li><li><p>通过 Docker 运行单机版 2048</p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230628154614031.png" alt="image-20230628154614031"></p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230628154648383.png" alt="image-20230628154648383"></p></li><li><p>在 2048 镜像中，使用 RUN 命令安装 iputils-ping （apt update &amp;&amp; apt install -y iputilsping) </p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230628192236742.png" alt="image-20230628192236742"></p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230628192258458.png" alt="image-20230628192258458"></p></li><li><p>使用 docker exec 命令，进入到容器中，执行 ping 命令</p></li></ol><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230628192428643.png" alt="image-20230628192428643"></p><p>原图片找不到了</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>assembly_code</title>
      <link href="/2023/03/08/assembly-code/"/>
      <url>/2023/03/08/assembly-code/</url>
      
        <content type="html"><![CDATA[<p>本文参考了<a href="https://www.ruanyifeng.com/blog/2018/01/assembly-language-primer.html">阮一峰汇编语言入门教程</a></p><p>引：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc -Og -s hello.c</span><br></pre></td></tr></table></figure><p>实际上-Og只是为了让机器在产生汇编和机器代码时不要过于优化导致代码严重变形，便于理解</p><p>工程中追求性能时常用 -O1 或 -O2</p><h2 id="一、汇编是什么"><a href="#一、汇编是什么" class="headerlink" title="一、汇编是什么"></a>一、汇编是什么</h2><ol><li><p>cpu执行的指令是二进制的,称作<strong>操作码(opcode)</strong></p></li><li><p>编译器的作用就是将高级语言程序翻译成一条条操作码</p></li><li><p>二进制对人类是不可读的,所以产生<strong>汇编语言</strong></p><blockquote><p>汇编语言是二进制指令的文本形式,和其是<strong>一一对应</strong>的关系.比如，加法指令<code>00000011</code>写成汇编语言就是 ADD。只要还原成二进制，汇编语言就可以被 CPU 直接执行，所以它是最底层的低级语言。</p></blockquote></li></ol><h2 id="二、汇编的来历"><a href="#二、汇编的来历" class="headerlink" title="二、汇编的来历"></a>二、汇编的来历</h2><p>为解决二进制指令的可读性问题,产生了文本指令,执行时需要把文本指令翻译为二进制,这被称为<strong>assembling</strong>。所以就产生了<strong>assembly code(汇编语言)</strong></p><p>目前主流的是<strong>x86汇编语言</strong>,即Intel公司的cpu所使用。</p><h2 id="三、寄存器"><a href="#三、寄存器" class="headerlink" title="三、寄存器"></a>三、寄存器</h2><p>CPU 本身只负责运算，不负责储存数据。数据一般都储存在内存之中，CPU 要用的时候就去内存读写数据。但是，CPU 的运算速度远高于内存的读写速度，为了避免被拖慢，CPU 都自带一级缓存和二级缓存。基本上，CPU 缓存可以看作是读写速度较快的内存。</p><p>但是，CPU 缓存还是不够快，另外数据在缓存里面的地址是不固定的，CPU 每次读写都要寻址也会拖慢速度。因此，除了缓存之外，CPU 还自带了寄存器（register），用来储存最常用的数据。也就是说，那些最频繁读写的数据（比如循环变量），都会放在寄存器里面，CPU 优先读写寄存器，再由寄存器跟内存交换数据。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/bg2018012206.png" alt="img"></p><p>寄存器不依靠地址区分数据，而依靠名称。每一个寄存器都有自己的名称，我们告诉 CPU 去具体的哪一个寄存器拿数据，这样的速度是最快的。有人比喻寄存器是 CPU 的零级缓存。</p><h2 id="四、寄存器的种类"><a href="#四、寄存器的种类" class="headerlink" title="四、寄存器的种类"></a>四、寄存器的种类</h2><p>早期的 x86 CPU 只有8个寄存器，而且每个都有不同的用途。现在的寄存器已经有100多个了，都变成通用寄存器，不特别指定用途了，但是早期寄存器的名字都被保存了下来。</p><blockquote><ul><li>EAX</li><li>EBX</li><li>ECX</li><li>EDX</li><li>EDI</li><li>ESI</li><li>EBP</li><li>ESP</li></ul></blockquote><p>且现在的机器都是64位的了，上述E–&gt;R</p><p>上面这8个寄存器之中，前面七个都是通用的。ESP 寄存器有特定用途，保存当前 Stack 的地址（详见下一节）。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/bg2018012207.png" alt="img"></p><p>我们常常看到 32位 CPU、64位 CPU 这样的名称，其实指的就是寄存器的大小。32 位 CPU 的寄存器大小就是4个字节。</p><h2 id="五、内存模型：堆（Heap）"><a href="#五、内存模型：堆（Heap）" class="headerlink" title="五、内存模型：堆（Heap）"></a>五、内存模型：堆（Heap）</h2><p>寄存器只能存放很少量的数据，大多数时候，CPU 要指挥寄存器，直接跟内存交换数据。所以，除了寄存器，还必须了解内存怎么储存数据。</p><p>程序运行的时候，操作系统会给它分配一段内存，用来储存程序和运行产生的数据。这段内存有起始地址和结束地址，比如从<code>0x1000</code>到<code>0x8000</code>，起始地址是较小的那个地址，结束地址是较大的那个地址。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/bg2018012208.png" alt="img"></p><p>程序运行过程中，对于动态的内存占用请求（比如新建对象，或者使用<code>malloc</code>命令），系统就会从预先分配好的那段内存之中，划出一部分给用户，具体规则是从起始地址开始划分（实际上，起始地址会有一段静态数据，这里忽略）。举例来说，用户要求得到10个字节内存，那么从起始地址<code>0x1000</code>开始给他分配，一直分配到地址<code>0x100A</code>，如果再要求得到22个字节，那么就分配到<code>0x1020</code>。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/bg2018012209.png" alt="img"></p><p>这种因为用户主动请求而划分出来的内存区域，叫做 Heap（堆）。它由起始地址开始，从低位（地址）向高位（地址）增长。Heap 的一个重要特点就是不会自动消失，必须手动释放，或者由垃圾回收机制来回收。</p><h2 id="六、内存模型：栈（Stack）"><a href="#六、内存模型：栈（Stack）" class="headerlink" title="六、内存模型：栈（Stack）"></a>六、内存模型：栈（Stack）</h2><p>简单说，Stack 是由于函数运行而临时占用的内存区域。结束后会自动回收··</p><p>Stack 是由内存区域的结束地址开始，从高位（地址）向低位（地址）分配。比如，内存区域的结束地址是<code>0x8000</code>，第一帧假定是16字节，那么下一次分配的地址就会从<code>0x7FF0</code>开始；第二帧假定需要64字节，那么地址就会移动到<code>0x7FB0</code>。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/bg2018012215.png" alt="img"></p><h2 id="七、cpu指令"><a href="#七、cpu指令" class="headerlink" title="七、cpu指令"></a>七、cpu指令</h2><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// file:example.c</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">add_a_and_b</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> add_a_and_b(<span class="number">2</span>,<span class="number">3</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>gcc将这个程序转成汇编语言</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc -S example.c</span><br></pre></td></tr></table></figure><p>上面的命令执行以后，会生成一个文本文件<code>example.s</code>，里面就是汇编语言，包含了几十行指令。这么说吧，一个高级语言的简单操作，底层可能由几个，甚至几十个 CPU 指令构成。CPU 依次执行这些指令，完成这一步操作。</p><p><code>example.s</code>经过简化以后，大概是下面的样子。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">_add_a_and_b:</span><br><span class="line">   push   %ebx</span><br><span class="line">   mov    %eax, [%esp+8] </span><br><span class="line">   mov    %ebx, [%esp+12]</span><br><span class="line">   add    %eax, %ebx </span><br><span class="line">   pop    %ebx </span><br><span class="line">   ret  </span><br><span class="line"></span><br><span class="line">_main:</span><br><span class="line">   push   3</span><br><span class="line">   push   2</span><br><span class="line">   call   _add_a_and_b </span><br><span class="line">   add    %esp, 8</span><br><span class="line">   ret</span><br></pre></td></tr></table></figure><p>​    </p><p>可以看到，原程序的两个函数<code>add_a_and_b</code>和<code>main</code>，对应两个标签<code>_add_a_and_b</code>和<code>_main</code>。每个标签里面是该函数所转成的 CPU 运行流程。</p><p>每一行就是 CPU 执行的一次操作。它又分成两部分，就以其中一行为例。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">push   %ebx</span><br></pre></td></tr></table></figure><p>这一行里面，<code>push</code>是 CPU 指令，<code>%ebx</code>是该指令要用到的运算子。一个 CPU 指令可以有零个到多个运算子。</p><h3 id="push指令"><a href="#push指令" class="headerlink" title="push指令"></a>push指令</h3><p>根据约定，程序从<code>_main</code>标签开始执行，这时会在 Stack 上为<code>main</code>建立一个帧，并将 Stack 所指向的地址，写入 ESP 寄存器。后面如果有数据要写入<code>main</code>这个帧，就会写在 ESP 寄存器所保存的地址。</p><p>然后，开始执行第一行代码。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">push   3</span><br></pre></td></tr></table></figure><p><code>push</code>指令用于将运算子放入 Stack，这里就是将<code>3</code>写入<code>main</code>这个帧。</p><p>虽然看上去很简单，<code>push</code>指令其实有一个前置操作。它会先取出 ESP 寄存器里面的地址，将其减去4个字节，然后将新地址写入 ESP 寄存器。使用减法是因为 Stack 从高位向低位发展，4个字节则是因为<code>3</code>的类型是<code>int</code>，占用4个字节。得到新地址以后， 3 就会写入这个地址开始的四个字节。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">push   2</span><br></pre></td></tr></table></figure><p>第二行也是一样，<code>push</code>指令将<code>2</code>写入<code>main</code>这个帧，位置紧贴着前面写入的<code>3</code>。这时，ESP 寄存器会再减去 4个字节（累计减去8）。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/bg2018012216.png" alt="img"></p><h3 id="call指令"><a href="#call指令" class="headerlink" title="call指令"></a>call指令</h3><p>第三行的 call 的指令用来调用函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">call   _add_a_and_b</span><br></pre></td></tr></table></figure><p>上面的代码表示调用<code>add_a_and_b</code>函数。这时，程序就会去找<code>_add_a_and_b</code>标签，并为该函数建立一个新的帧。</p><p>下面就开始执行<code>_add_a_and_b</code>的代码。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">push   %ebx</span><br></pre></td></tr></table></figure><p>这一行表示将 EBX 寄存器里面的值，写入<code>_add_a_and_b</code>这个帧。这是因为后面要用到这个寄存器，就先把里面的值取出来，用完后再写回去。</p><p>这时，<code>push</code>指令会再将 ESP 寄存器里面的地址减去4个字节（累计减去12）。</p><h3 id="mov-指令"><a href="#mov-指令" class="headerlink" title="mov 指令"></a>mov 指令</h3><p><code>mov</code>指令用于将一个值写入某个寄存器。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mov  %eax, [%esp+8]</span><br></pre></td></tr></table></figure><p>这一行代码表示，先将 ESP 寄存器里面的地址加上8个字节，得到一个新的地址，然后按照这个地址在 Stack 取出数据。根据前面的步骤，可以推算出这里取出的是<code>2</code>，再将<code>2</code>写入 EAX 寄存器。</p><p>下一行代码也是干同样的事情。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mov    %ebx, [%esp+12] </span><br></pre></td></tr></table></figure><p>上面的代码将 ESP 寄存器的值加12个字节，再按照这个地址在 Stack 取出数据，这次取出的是<code>3</code>，将其写入 EBX 寄存器。</p><h3 id="add指令"><a href="#add指令" class="headerlink" title="add指令"></a>add指令</h3><p><code>add</code>指令用于将两个运算子相加，并将结果写入第一个运算子。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add    %eax, %ebx</span><br></pre></td></tr></table></figure><p>上面的代码将 EAX 寄存器的值（即2）加上 EBX 寄存器的值（即3），得到结果5，再将这个结果写入第一个运算子 EAX 寄存器。</p><h3 id="pop指令"><a href="#pop指令" class="headerlink" title="pop指令"></a>pop指令</h3><p><code>pop</code>指令用于取出 Stack 最近一个写入的值（即最低位地址的值），并将这个值写入运算子指定的位置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pop %ebx</span><br></pre></td></tr></table></figure><p>上面的代码表示，取出 Stack 最近写入的值（即 EBX 寄存器的原始值），再将这个值写回 EBX 寄存器（因为加法已经做完了，EBX 寄存器用不到了）。</p><p>注意，<code>pop</code>指令还会将 ESP 寄存器里面的地址加4，即回收4个字节。</p><h3 id="ret指令"><a href="#ret指令" class="headerlink" title="ret指令"></a>ret指令</h3><p>ret指令用终止当前函数的执行, 将运行权交还给上层函数。也就是，当前函数的帧将被回收。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ret</span><br></pre></td></tr></table></figure><p>可以看到,该指令没有运算子</p><p>随着add_a_and_b函数终止执行,系统就回到刚才main函数中断的地方,继续往下执行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add %esp, 8</span><br></pre></td></tr></table></figure><p>上面的代码表示，将ESP寄存器里面的地址，手动加上8个字节，再写回ESP寄存器。这是因为ESP寄存器的是Stack的写入开始地址，前面的<code>pop</code>操作已经回收了4个字节，这里再回收8个字节，等于全部回收。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ret</span><br></pre></td></tr></table></figure><p>最后，<code>main</code>函数运行结束，<code>ret</code>指令退出程序执行。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>csapp_data</title>
      <link href="/2023/02/28/csapp-data/"/>
      <url>/2023/02/28/csapp-data/</url>
      
        <content type="html"><![CDATA[<p>有符号数一般都是<strong>算数右移</strong>，无符号数一般都是<strong>逻辑右移</strong>。算数右移时如果符号位为1，左端应该补1。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>computer_composition1</title>
      <link href="/2023/02/22/computer-composition1/"/>
      <url>/2023/02/22/computer-composition1/</url>
      
        <content type="html"><![CDATA[<h3 id="1-计算机发展历程"><a href="#1-计算机发展历程" class="headerlink" title="1. 计算机发展历程"></a>1. 计算机发展历程</h3><h4 id="1-1-硬件的发展"><a href="#1-1-硬件的发展" class="headerlink" title="1.1 硬件的发展"></a>1.1 硬件的发展</h4><ul><li>第一代计算机：(使用电子管)，</li><li>第二代计算机：(使用晶体管)，</li><li>第三代计算机：(使用较小规模的集成)，</li><li>第四代计算机：(使用较大规模的集成)，</li></ul><h4 id="1-2-软件的发展"><a href="#1-2-软件的发展" class="headerlink" title="1.2 软件的发展"></a>1.2 软件的发展</h4><p>各种软件及操作系统的蓬勃发展</p><h4 id="1-3-计算机的分类和发展方向"><a href="#1-3-计算机的分类和发展方向" class="headerlink" title="1.3 计算机的分类和发展方向"></a>1.3 计算机的分类和发展方向</h4><p>分为：</p><ul><li>电子模拟计算机和电子数字计算机。</li></ul><p>数字计算机又可以按照用途分为：</p><ul><li>专用计算机和通用计算机</li></ul><p>通用计算机又分为：</p><ul><li>巨型机、大型机、中型机、小型机、微型机和单片机6类。</li></ul><p>按照指令和数据流可以分为：</p><ul><li><p>单指令流和单数据流系统（SISD），即传统的冯·诺依曼体系结构。</p></li><li><p>单指令流和多数据流系统（SIMD），包括阵列处理器和向量处理器系统。</p></li><li><p>多指令流和单数据流系统（MISD），这种计算机实际上不存在。</p></li><li><p>多指令流和多数据流系统（MIMD），包括多处理器和计算机系统。</p></li></ul><h3 id="2-计算机系统层次结构"><a href="#2-计算机系统层次结构" class="headerlink" title="2. 计算机系统层次结构"></a>2. 计算机系统层次结构</h3><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222092519018.png" alt="image-20230222092519018"></p><h4 id="2-1-计算机系统的组成"><a href="#2-1-计算机系统的组成" class="headerlink" title="2.1 计算机系统的组成"></a>2.1 计算机系统的组成</h4><p>由硬件系统和软件系统共同构建</p><h4 id="2-2-计算机硬件的基本组成"><a href="#2-2-计算机硬件的基本组成" class="headerlink" title="2.2 计算机硬件的基本组成"></a>2.2 计算机硬件的基本组成</h4><ol><li>早期的冯诺依曼机</li></ol><hr><p>程序存储原理：指令以代码的形式事先输入到计算机的主存储器中，然后按其在存储器中的首地址执行程序的第一条指令，以后就按该程序的规定顺序执行其他指令，直至程序执行结束。即按地址访问并顺序执行指令</p><p>计算机按照此原理应具有5大功能：数据传送功能、数据存储功能、数据处理功能、操作控制功能、操作判断功能</p><hr><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222092112536.png" alt="image-20230222092112536"></p><ul><li>冯诺曼体系结构特点:</li></ul><ol><li>计算机硬件系统由五大部件组成(存储器、运算器、控制器、输出设备、输入设备)</li><li>指令和数据以同等地位存于存储器，可按地址寻访</li><li>指令和数据用二进制表示</li><li>指令由操作码和地址码组成</li><li>存储程序</li><li>以<strong>运算器</strong>为中心</li></ol><p>早期的冯·诺依曼机以运算器为中心，且是单处理机，<strong>最根本的特征</strong>是采用“<strong>存储程序</strong>”原理，基本工作方式是控制流驱动方式！</p><ol start="2"><li>现代计算机的组织结构</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222092120955.png" alt="image-20230222092120955"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222092642060.png" alt="image-20230222092642060"></p><p><strong>注意各箭头表示的含义</strong></p><h4 id="2-3-计算机的功能部件"><a href="#2-3-计算机的功能部件" class="headerlink" title="2.3 计算机的功能部件"></a>2.3 计算机的功能部件</h4><p>主机：主存、运算器、控制器</p><p><img src="https://img-blog.csdnimg.cn/2021011714554763.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhb2ppZV9kdWFu,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol><li>存储器的基本组成</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222103737433.png" alt="image-20230222103737433"></p><p>设 <strong>MAR &#x3D; 4位</strong>   <strong>MDR &#x3D; 8位</strong></p><p>则 存储单元16个    每个存储单元存储字长8位</p><ol start="2"><li>运算器的基本组成及操作过程</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222104447192.png" alt="image-20230222104447192"></p><ol start="3"><li>控制器的基本组成</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222105027251.png" alt="image-20230222105027251"></p><p>PC指向的是下一次执行的指令，IR指向的是当前要执行的指令</p><hr><p>一般将<strong>运算器和控制器集成</strong>到同一个芯片上，称为中央处理器(<strong>CPU</strong>)。<strong>CPU和主存储器</strong>共同构成<strong>主机</strong>，而除主机外的其他硬件装置(外存、I&#x2F;O设备等)统称为外部设备，简称<strong>外设</strong></p><ol start="4"><li>主机构造和执行指令</li></ol><p><img src="https://img-blog.csdnimg.cn/20210121090349881.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhb2ppZV9kdWFu,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>CPU和主存之间通过一组总线相连，总线中有地址、控制和数据3组信号线。MAR中的地址信息会直接送到地址线上，用于指向读&#x2F;写操作的主存存储单元；控制线中有读&#x2F;写信号线，指出数据是从CPU写入主存还是从主存读出到CPU,根据是读操作还是写操作来控制将MDR中的数据是直接送到数据线上还是将数据线上的数据接收到MDR中。</p><p><strong>主机完成一次指令的过程</strong>:</p><p>取数操作为例</p><p><img src="https://img-blog.csdnimg.cn/20210117172058667.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhb2ppZV9kdWFu,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222110244683.png" alt="image-20230222110244683"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222111002515.png" alt="image-20230222111002515"></p><h4 id="2-4-计算机硬件的性能指标"><a href="#2-4-计算机硬件的性能指标" class="headerlink" title="2.4 计算机硬件的性能指标"></a>2.4 计算机硬件的性能指标</h4><p><img src="https://img-blog.csdnimg.cn/20210117153043725.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhb2ppZV9kdWFu,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol><li>机器字长： cpu 一次能处理数据的位数，与cpu中的 <strong>寄存器位数</strong>有关，机器字长<strong>一般等于内部寄存器的大小</strong>，它决定了<strong>计算机的运算精度</strong>。</li></ol><hr><p>机器字长：计算机能直接处理的二进制数据的位数，机器字长一般等于内部寄存器的大小，它决定了计算机的运算精度。<br>指令字长：一个指令字中包含的二进制代码的位数。<br>存储字长：一个存储单元存储的二进制代码的长度。等于MDR的位数， 它们都必须是字节的整数倍。<br>数据字长：数据总线一次能传送信息的位数，它可以不等于MDR的位数。</p><p>指令字长一般取存储字长的整数倍，若指令字长等于存储字长的2倍，则需要2次访存来取出一条指令，因此取指周期为机器周期的2倍；若指令字长等于存储字长，则取指周期等于机器周期。</p><p>早期的计算机存储字长一般和机器的指令字长与数据字长相等，因此访问一次主存便可取出一条指令或一个数据。随着计算机的发展，指令字长可变，数据字长也可变，但它们必须都是字节的整数倍。</p><p>请注意64位操作系统是指特别为64位架构的计算机而设计的操作系统，它能够利用64位处理器的优势。但64位机器既可以使用64位操作系统，又可以使用32位操作系统。而32位处理器是无法使用64位操作系统的。</p><ol start="2"><li>运算速度：</li></ol><ul><li><p>主频</p></li><li><p>吉普森法<br>$$<br>T_M &#x3D; \sum_{i&#x3D;1}^{n}f_it_i<br>$$</p></li><li><p>MIPS   每秒执行百万条指令</p></li><li><p>FLOPS   每秒浮点运算次数</p></li><li><p>CPI   执行一条指令所需时钟周期数</p></li></ul><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222112210072.png" alt="image-20230222112210072"></p><h4 id="2-5-计算机软件的分类"><a href="#2-5-计算机软件的分类" class="headerlink" title="2.5 计算机软件的分类"></a>2.5 计算机软件的分类</h4><ol><li>系统软件和应用软件</li><li>三个级别的语言</li></ol><h3 id="3-数据的表示与运算"><a href="#3-数据的表示与运算" class="headerlink" title="3.数据的表示与运算"></a>3.数据的表示与运算</h3><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/20210118085022452.png" alt="在这里插入图片描述"></p><h4 id="3-1数制与编码"><a href="#3-1数制与编码" class="headerlink" title="3.1数制与编码"></a>3.1数制与编码</h4><h5 id="1-进位计数值及其相互转化"><a href="#1-进位计数值及其相互转化" class="headerlink" title="1.进位计数值及其相互转化"></a>1.进位计数值及其相互转化</h5><p><strong>进制之间的相互转换，乘x除x法</strong>  </p><p><strong>1位16进制数等价于4位二进制数,    一位8进制数等价于3位二进制数</strong></p><h5 id="2-BCD码-用二进制数表示十进制"><a href="#2-BCD码-用二进制数表示十进制" class="headerlink" title="2.BCD码   用二进制数表示十进制"></a>2.BCD码   用二进制数表示十进制</h5><p><strong>这是为了表示0和9，所以1010~1111都是违法的</strong></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230307230630880.png" alt="image-20230307230630880"></p><h5 id="3-字符与字符串"><a href="#3-字符与字符串" class="headerlink" title="3.字符与字符串"></a>3.字符与字符串</h5><p>英文字符的表示  ASCII码</p><p>中文字符的表示  GDB等其他编码</p><p><strong>字符串的大小端模式</strong></p><hr><ul><li>大端模式：将数据的最高有效字节存放在低地址单元中</li><li>小端模式：将数据的最高有效字节存放在高地址单元中</li></ul><h5 id="4-校验码"><a href="#4-校验码" class="headerlink" title="4.校验码"></a>4.校验码</h5><p>待补充…</p><h4 id="3-2-定点数的表示和运算"><a href="#3-2-定点数的表示和运算" class="headerlink" title="3.2 定点数的表示和运算"></a>3.2 定点数的表示和运算</h4><p><strong>定点数和浮点数是一对相对的概念</strong></p><p><strong>整数的小数点表示在最后一位数字的后面，而小数的小数点标识在真值的符号位后面</strong></p><h5 id="1-定点数的移位规则"><a href="#1-定点数的移位规则" class="headerlink" title="1.定点数的移位规则"></a>1.定点数的移位规则</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/f770pmhzm4.png" alt="img"></p><ul><li>将+26的原码、补码和反码分别左移一位 结果是：[26]原 &#x3D; [26]反 &#x3D; [26]补 &#x3D; 0,0011010，根据规则，原码、反码、补码左移一位的结果是：0,010100 </li><li>将-26的原码、反码、补码分别左移一位 [-26]原 &#x3D; 1,0011010，左移一位：1,0110100 [-26]反 &#x3D; 1,1100101,左移一位：1,1001011 [-26]补 &#x3D; 1,1100110，左移一位：1,100110</li></ul><h5 id="2-定点数的加减法"><a href="#2-定点数的加减法" class="headerlink" title="2.定点数的加减法"></a>2.定点数的加减法</h5><p><strong>加法直接加，减法先变为加法后再计算</strong></p><p>[A+B]补 &#x3D; [A+B]补，[A-B]补 &#x3D; [A]补 + [-B]补</p><ul><li>A &#x3D; -1001，B &#x3D; -0101，求[A+B]补 [A+B]补 &#x3D; [A]补 + [B]补，[A]补 &#x3D; 1,0111，[B]补 &#x3D; 1,1011，所以最终的结果是：11,0010，但是这并非我们的最终结果，最终结果应该丢掉第一个1，即1,0010.为什么呢？这涉及到一个模2运算的问题，如果不想深究只需要记住，<strong>一个数只能有一个符号位不是吗？</strong> </li><li>A &#x3D; -1001，B&#x3D;-0101.求[A-B]补 [A-B]补 &#x3D; [A]补 + [-B]补，[A]补&#x3D;1,0111，[-B]补&#x3D;0,0101（求法：**[-B]补等于[B]补符号位和各位取反，末位加一**），这样得到最终的结果，丢弃掉多余的位即可。</li></ul><p><strong>溢出的判断</strong>：如果计算机的机器字长为4，那么能够表示的真值范围在-8~+7之间，如果两个数相加减，跳出了这个范围，则为溢出</p><p><strong>判断原则：</strong></p><ul><li><strong>不论加法还是减法，只要实际参与运算的两个数的符号相同，但是与最终的结果的符号相反，则为溢出。</strong>比如我们的第一个例子，两个参与运算的数的符号相同，且和最终结果的符号也相同，则这种情况就不是溢出。</li><li><strong>最终结果的两位符号位如果相同，则无溢出，如果不同则溢出</strong>，还是第一个例子，计算后的结果是11,0010，两位符号位相同，没有溢出。</li></ul><p>补码加法硬件配置</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308102214393.png" alt="image-20230308102214393"></p><h5 id="3-定点数的乘法"><a href="#3-定点数的乘法" class="headerlink" title="3.定点数的乘法"></a>3.定点数的乘法</h5><ol><li>原码乘法</li></ol><p>一位</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308104605231.png" alt="image-20230308104605231"></p><ul><li>绝对值运算</li><li>用移位的次数判断乘法是否结束</li><li>逻辑移位</li></ul><p>两位</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/7q991tvqow.png" alt="img"></p><ol start="2"><li>补码乘法</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308105741103.png" alt="image-20230308105741103"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308110246959.png" alt="image-20230308110246959"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308110321370.png" alt="image-20230308110321370"></p><h5 id="4-定点数的除法"><a href="#4-定点数的除法" class="headerlink" title="4.定点数的除法"></a>4.定点数的除法</h5><ol><li>恢复余数法</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308112711712.png" alt="image-20230308112711712"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308112728241.png" alt="image-20230308112728241"></p><ol start="2"><li>不恢复余数法</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308113227128.png" alt="image-20230308113227128"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308113242477.png" alt="image-20230308113242477"></p><ul><li>上商n+1次</li><li>第一次上商判溢出</li><li>移n次，加n+1次</li><li>用移位的次数判断除法是否结束</li></ul><h4 id="3-3-浮点数的表示与运算"><a href="#3-3-浮点数的表示与运算" class="headerlink" title="3.3 浮点数的表示与运算"></a>3.3 浮点数的表示与运算</h4><p>用定点数表示数字时，会约定小数点的位置固定不变，整数部分和小数部分分别转换为二进制，就是定点数的结果。</p><p>但用定点数表示小数时，存在数值范围、精度范围有限的缺点，所以在计算机中，我们一般使用「浮点数」来表示小数。</p><p>其中「定点」指的是约定小数点位置固定不变。那浮点数的「浮点」就是指，其<strong>小数点的位置</strong>是可以是<strong>漂浮不定</strong>的。</p><p>例如十进制小数 8.345，用科学计数法表示，可以有多种方式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">8.345 = 8.345 * 10^0</span><br><span class="line">8.345 = 83.45 * 10^-1</span><br><span class="line">8.345 = 834.5 * 10^-2</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>用这种科学计数法的方式表示小数时，小数点的位置就变得「漂浮不定」了，这就是相对于定点数，浮点数名字的由来。</p><p>使用同样的规则，对于二进制数，我们也可以用科学计数法表示，也就是说把基数 10 换成 2 即可。</p><h5 id="浮点数如何表示数字："><a href="#浮点数如何表示数字：" class="headerlink" title="浮点数如何表示数字："></a>浮点数如何表示数字：</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">V = (-1)^S * M * R^E</span><br></pre></td></tr></table></figure><p>其中各个变量的含义如下：</p><ul><li>S：符号位，取值 0 或 1，决定一个数字的符号，0 表示正，1 表示负</li><li>M：尾数，用小数表示，例如前面所看到的 8.345 * 10^0，8.345 就是尾数</li><li>R：基数，表示十进制数 R 就是 10，表示二进制数 R 就是 2</li><li>E：指数，用整数表示，例如前面看到的 10^-1，-1 即是指数</li></ul><p>假设现在我们用 32 bit 表示一个浮点数，把以上变量按照一定规则，填充到这些 bit 上就可以了：</p><p><a href="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090678516816.jpg"><img src="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090678516816.jpg" alt="img"></a></p><p>假设我们定义如下规则来填充这些 bit：</p><ul><li>符号位 S 占 1 bit</li><li>指数 E 占 10 bit</li><li>尾数 M 占 21 bit</li></ul><p>按照这个规则，将十进制数 25.125 转换为浮点数，转换过程就是这样的（D代表十进制，B代表二进制）：</p><ol><li>整数部分：25(D) &#x3D; 11001(B)</li><li>小数部分：0.125(D) &#x3D; 0.001(B)</li><li>用二进制科学计数法表示：25.125(D) &#x3D; 11001.001(B) &#x3D; 1.1001001 * 2^4(B)</li></ol><p>所以符号位 S &#x3D; 0，尾数 M &#x3D; 1.001001(B)，指数 E &#x3D; 4(D) &#x3D; 100(B)。</p><p>按照上面定义的规则，填充到 32 bit 上，就是这样：</p><p><a href="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090693834968.jpg"><img src="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090693834968.jpg" alt="img"></a></p><p>上述规则只是随意设置的，若按新规则来，那浮点数表示出来也可以这样：</p><p><a href="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090693834973.jpg"><img src="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090693834973.jpg" alt="img"></a></p><p>可以看到：</p><ol><li>指数位越多，尾数位则越少，其表示的范围越大，但精度就会变差，反之，指数位越少，尾数位则越多，表示的范围越小，但精度就会变好</li><li>一个数字的浮点数格式，会因为定义的规则不同，得到的结果也不同，表示的范围和精度也有差异</li></ol><p>早期人们提出浮点数定义时，就是这样的情况，当时有很多计算机厂商，例如IBM、微软等，每个计算机厂商会定义自己的浮点数规则，</p><h5 id="浮点数标准"><a href="#浮点数标准" class="headerlink" title="浮点数标准"></a>浮点数标准</h5><p>直到1985年，IEEE 组织推出了浮点数标准，就是我们经常听到的 <strong>IEEE754 浮点数标准</strong>，这个标准统一了浮点数的表示形式，并提供了 2 种浮点格式：</p><ul><li>单精度浮点数 float：32 位，符号位 S 占 1 bit，指数 E 占 8 bit，尾数 M 占 23 bit</li><li>双精度浮点数 double：64 位，符号位 S 占 1 bit，指数 E 占 11 bit，尾数 M 占 52 bit</li></ul><p>为了使其表示的数字范围、精度最大化，浮点数标准还对指数和尾数进行了规定：</p><ol><li>尾数 M 的第一位总是 1（因为 1 &lt;&#x3D; M &lt; 2），因此这个 1 可以省略不写，它是个<strong>隐藏位</strong>，这样单精度 23 位尾数可以表示了 24 位有效数字，双精度 52 位尾数可以表示 53 位有效数字</li><li>指数 E 是个无符号整数，表示 float 时，一共占 8 bit，所以它的取值范围为 0 ~ 255。但因为指数可以是负的，所以规定在存入 E 时在它原本的值加上一个<strong>中间数</strong> 127，这样 E 的取值范围为 -127 ~ 128。表示 double 时，一共占 11 bit，存入 E 时加上中间数 1023，这样取值范围为 -1023 ~ 1024。</li></ol><p>除了规定尾数和指数位，还做了以下规定：</p><ul><li>指数 E 非全 0 且非全 1：规格化数字，按上面的规则正常计算</li><li>指数 E 全 0，尾数非 0：非规格化数，尾数隐藏位不再是 1，而是 0(M &#x3D; 0.xxxxx)，这样可以表示 0 和很小的数</li><li>指数 E 全 1，尾数全 0：正无穷大&#x2F;负无穷大（正负取决于 S 符号位）</li><li>指数 E 全 1，尾数非 0：NaN(Not a Number)</li></ul><p><a href="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090749848677.jpg"><img src="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090749848677.jpg" alt="3"></a></p><p><a href="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090749848677.jpg">3</a></p><h5 id="标准浮点数的表示"><a href="#标准浮点数的表示" class="headerlink" title="标准浮点数的表示"></a>标准浮点数的表示</h5><p>有了这个统一的浮点数标准，我们再把 25.125 转换为标准的 float 浮点数：</p><ol><li>整数部分：25(D) &#x3D; 11001(B)</li><li>小数部分：0.125(D) &#x3D; 0.001(B)</li><li>用二进制科学计数法表示：25.125(D) &#x3D; 11001.001(B) &#x3D; 1.1001001 * 2^4(B)</li></ol><p>所以 S &#x3D; 0，尾数 M &#x3D; 1.001001 &#x3D; 001001(去掉1，隐藏位)，指数 E &#x3D; 4 + 127(中间数) &#x3D; 135(D) &#x3D; 10000111(B)。填充到 32 bit 中，如下：</p><p><a href="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090678516830.jpg"><img src="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090678516830.jpg" alt="img"></a></p><p>这就是标准 32 位浮点数的结果。</p><p>如果用 double 表示，和这个规则类似，指数位 E 用 11 bit 填充，尾数位 M 用 52 bit 填充即可。</p><p><strong>注：</strong></p><blockquote><p>float 1位符号位，8位阶码位（移码表示，偏移127，取值范围为1~254，0和255表示特殊值），23位尾数（隐含1），所以能表示的最大正整数为 （1+1-2^-23）* 2^127</p><p>double 1位符号位，11位阶码位（偏移 1023），52位尾数位（隐含1），所以能表示的最大正整数为（1+1-2^52）* 2^1023</p></blockquote><h5 id="浮点数的范围和精度"><a href="#浮点数的范围和精度" class="headerlink" title="浮点数的范围和精度"></a>浮点数的范围和精度</h5><p>以单精度浮点数 float 为例，它能表示的最大二进制数为 +1.1.11111…1 * 2^127（小数点后23个1），而二进制 1.11111…1 ≈ 2，所以 float 能表示的最大数为 2^128 &#x3D; 3.4 * 10^38，即 float 的表示范围为：-3.4 * 10^38 ~ 3.4 * 10 ^38。</p><p><strong>精度：</strong></p><p>float 能表示的最小正二进制数为 0.0000….1（小数点后22个0，1个1），用十进制数表示就是 1&#x2F;2^23。</p><p>用同样的方法可以算出，double 能表示的最大二进制数为 +1.111…111（小数点后52个1） * 2^1023 ≈ 2^1024 &#x3D; 1.79 * 10^308，所以 double 能表示范围为：-1.79 * 10^308 ~ +1.79 * 10^308。</p><p>double 的最小精度为：0.0000…1(51个0，1个1)，用十进制表示就是 1&#x2F;2^52。</p><p>虽然浮点数的范围和精度也有限，但其范围和精度<strong>都已非常之大</strong></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315112408077.png" alt="image-20230315112408077"></p><p>总结</p><hr><ol><li>浮点数一般用科学计数法表示</li><li>把科学计数法中的变量，填充到固定 bit 中，即是浮点数的结果</li><li>在浮点数提出的早期，各个计算机厂商各自制定自己的浮点数规则，导致不同厂商对于同一个数字的浮点数表示各不相同，在计算时还需要先进行转换才能进行计算</li><li>后来 IEEE 组织提出了浮点数的标准，统一了浮点数的格式，并规定了单精度浮点数 float 和双精度浮点数 double，从此以后各个计算机厂商统一了浮点数的格式，一直延续至今</li><li>浮点数在表示小数时，由于十进制小数在转换为二进制时，存在无法精确转换的情况，而在固定 bit 的计算机中存储时会被截断，所以浮点数表示小数可能存在精度损失</li><li>浮点数在表示一个数字时，其范围和精度非常大，所以我们平时使用的小数，在计算机中通常用浮点数来存储</li></ol><blockquote><p>补充: <a href="https://blog.csdn.net/qq_43855740/article/details/104721619">负数补码表示范围以及规格化数</a></p></blockquote><h4 id="3-4-算数逻辑单元（ALU）"><a href="#3-4-算数逻辑单元（ALU）" class="headerlink" title="3.4 算数逻辑单元（ALU）"></a>3.4 算数逻辑单元（ALU）</h4><h5 id="1-ALU电路"><a href="#1-ALU电路" class="headerlink" title="1.ALU电路"></a>1.ALU电路</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315133712464.png" alt="image-20230315133712464"></p><p>ALU电路是一个<strong>组合逻辑电路</strong>，不含记忆功能，所以要接入寄存器</p><h5 id="2-运算器的组成"><a href="#2-运算器的组成" class="headerlink" title="2. 运算器的组成"></a>2. 运算器的组成</h5><p><img src="https://img-blog.csdnimg.cn/20210424213852316.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1F1YW50dW1Zb3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210424214104744.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1F1YW50dW1Zb3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h5 id="3-快速进位链"><a href="#3-快速进位链" class="headerlink" title="3.快速进位链"></a>3.快速进位链</h5><h6 id="a-并行加法器"><a href="#a-并行加法器" class="headerlink" title="a. 并行加法器"></a>a. 并行加法器</h6><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315134834037.png" alt="image-20230315134834037"></p><h6 id="b-串行进位链"><a href="#b-串行进位链" class="headerlink" title="b.串行进位链"></a>b.串行进位链</h6><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315135151661.png" alt="image-20230315135151661"></p><h6 id="c-并行进位链"><a href="#c-并行进位链" class="headerlink" title="c.并行进位链"></a>c.并行进位链</h6><p>先行进位链：电路复杂</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315135518513.png" alt="image-20230315135518513"></p><p>1）单重分组跳跃进位链</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315140854991.png" alt="image-20230315140854991"></p><ol start="2"><li>双重分组跳跃进位链</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315140918035.png" alt="image-20230315140918035"></p><p><strong>进位分析</strong></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315140933992.png" alt="image-20230315140933992"></p><p><strong>大组进位线路</strong></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315141040662.png" alt="image-20230315141040662"></p><p><strong>小组进位线路</strong></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315141122347.png" alt="image-20230315141122347"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315141142891.png" alt="image-20230315141142891"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315141154305.png" alt="image-20230315141154305"></p><h3 id="4-系统总线"><a href="#4-系统总线" class="headerlink" title="4. 系统总线"></a>4. 系统总线</h3><p>总线概念</p><p>如果两两单独连接，则连接的网络十分庞大，而且难以扩展</p><h4 id="1-总线是各个部件共享的传输介质"><a href="#1-总线是各个部件共享的传输介质" class="headerlink" title="1.总线是各个部件共享的传输介质"></a>1.总线是各个部件共享的传输介质</h4><p>总线上可以进行<strong>串行</strong>和<strong>并行</strong>两种传输方式</p><h4 id="2-总线结构"><a href="#2-总线结构" class="headerlink" title="2.总线结构"></a>2.总线结构</h4><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227100514706.png" alt="image-20230227100514706"></p><p>这种结构可扩展性强，但是任意时刻只能进行两个部件之间的信息传递，严重影响效率</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227100531192.png" alt="image-20230227100531192"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227100543225.png" alt="image-20230227100543225"></p><p><strong>现代技术中 通常情况下这两条总线也很难同时工作</strong></p><h4 id="3-总线分类"><a href="#3-总线分类" class="headerlink" title="3.总线分类"></a>3.总线分类</h4><ol><li>片内总线： 芯片内部的总线</li><li>系统总线： <strong>注意机器字长和存储字长的概念</strong></li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227101014280.png" alt="image-20230227101014280"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227101422395.png" alt="image-20230227101422395"></p><h4 id="4-总线特性及性能指标"><a href="#4-总线特性及性能指标" class="headerlink" title="4.总线特性及性能指标"></a>4.总线特性及性能指标</h4><ol><li>总线的物理实现</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227182408824.png" alt="image-20230227182408824"></p><p><strong>总线是印刷在主板上的</strong>，图中cpu插板等都是在主线上的接口</p><ol start="2"><li>总线的特性</li></ol><p>a. 机械特性：<strong>尺寸</strong>、形状、<strong>管脚数</strong>及<strong>排列顺序</strong></p><p>b. 电气特性： <strong>传输方向</strong> 和有效的 <strong>电平</strong>范围</p><p>c. 功能特性： 每根传输线的 <strong>功能</strong>（地址、数据、控制）</p><p>d. 时间特性： 信号的<strong>时序</strong>关系</p><ol start="3"><li>总线的性能指标</li></ol><p>总线宽度：数据线的根数</p><p>标准传输率： 每秒传输的最大字节数(MBps)</p><p>时钟同步&#x2F;异步： 同步、不同步</p><p>总线复用：地址线与数据线复用</p><p>信号线数：地址线、数据线和控制线的总和</p><p>总线控制方式：突发、自动、仲裁、逻辑、计数</p><p>其他指标：负载能力</p><ol start="4"><li>总线标准</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227195630263.png" alt="image-20230227195630263"></p><h4 id="5-多总线结构"><a href="#5-多总线结构" class="headerlink" title="5.多总线结构"></a>5.多总线结构</h4><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227201405612.png" alt="image-20230227201405612"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227205904087.png" alt="image-20230227205904087"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227210134573.png" alt="image-20230227210134573"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227210156759.png" alt="image-20230227210156759"></p><h4 id="6-总线控制"><a href="#6-总线控制" class="headerlink" title="6.总线控制"></a>6.总线控制</h4><h5 id="一、总线判优控制"><a href="#一、总线判优控制" class="headerlink" title="一、总线判优控制"></a>一、总线判优控制</h5><ol><li><p>主设备（模块） 对总线有 <strong>控制权</strong></p><p>从设备                <strong>响应</strong> 从主设备发来的总线命令</p></li></ol><pre class="mermaid">graph LRA[总线判优控制] --> B1[集中式]A --> B2[分布式]B1 --> c1[链式查询]B1 --> c2[计数器定时查询]B1 --> c3[独立请求方式]</pre><p>2.查询方式</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227214415925.png" alt="image-20230227214415925"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227214440898.png" alt="image-20230227214440898"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227221835173.png" alt="image-20230227221835173"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227221931763.png" alt="image-20230227221931763"></p><h5 id="二、总线通信控制"><a href="#二、总线通信控制" class="headerlink" title="二、总线通信控制"></a>二、总线通信控制</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227222023283.png" alt="image-20230227222023283"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227222036808.png" alt="image-20230227222036808"></p><ol><li>同步式数据输入</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227222118063.png" alt="image-20230227222118063"></p><ol start="2"><li>同步式数据输出</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227223345390.png" alt="image-20230227223345390"></p><ol start="3"><li>异步通信</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227223642412.png" alt="image-20230227223642412"></p><ol start="4"><li>半同步通信</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227224130590.png" alt="image-20230227224130590"></p><p>在同步通信之间插入<strong>WAIT信号</strong></p><ol start="5"><li>总结上述三种通信的共同点<br>一个总线传输周期（以输入数据为例）</li></ol><ul><li>主模块发地址、命令       占用总线</li><li>从模块准备数据               不占用总线总线空闲</li><li>从模块向主模块发数据   占用总线</li></ul><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227224751442.png" alt="image-20230227224751442"> </p><p>分离式通信特点</p><ul><li>各模块有权申请占用总线</li><li>采用同步方式通信，不等对方回答</li><li>各模块准备数据时，不占用总线</li><li>总线被占用时，无空闲</li><li><u>充分提高了总线的有效占用</u></li></ul><h3 id="5-存储器"><a href="#5-存储器" class="headerlink" title="5.存储器"></a>5.存储器</h3><h4 id="5-1存储器的分类"><a href="#5-1存储器的分类" class="headerlink" title="5.1存储器的分类"></a>5.1存储器的分类</h4><p>按在计算机中的作用对存储器分类：</p><ul><li><p>主存储器,简称<strong>主存</strong>。CPU可以直接随机地对其进行访问，也可以和高速缓存器及辅助存储器交换数据。</p></li><li><p>辅助存储器,简称<strong>辅存</strong>，不能与CPU直接相连，用来存放当前暂时不用的程序和数据</p></li><li><p><strong>高速缓冲存储器</strong>, 位于<strong>主存和CPU之间</strong>，用来存放正在执行的程序段和数据，作为cpu和主存之间的缓冲</p></li><li><p><strong>Flash Memory</strong>：可作为主存和辅存之间的临时存储器，也可以单独作为高性能存储器，为半导体元件，如U盘</p></li></ul><p>按存储介质分类：</p><p>磁表面存储器（磁盘，磁带），磁心存储器半导体存储器（MOS型存储器，双极存储器）和光存储器（光盘）。</p><p>按存取方式分类：</p><ul><li><p>随机存储器（<strong>RAM</strong>）。存储器的任何一个存储单元的内容都可以随机存取，而且存取时间与存取单元的物理位置无关，<strong>主要用作主存或高速缓冲存储器</strong>。</p></li><li><p>只读存储器（ROM）。存储器的内容只能随机读出而不能写入。即使断电，内容也不会丢失。</p></li><li><p>串行访问存储器。对存储单元进行读&#x2F;写操作时，需按其物理位置的先后顺序寻址，包括<strong>顺序存取存储器（如磁带）</strong>与<strong>直接存取存储器（如磁盘）</strong>。</p></li></ul><p>按信息的可保存性分类：</p><p>断电后，存储信息即消失的存储器，称为易失性存储器，如RAM。断电后信息仍然保持的存储器，称为非易失性存储器，如ROM，磁表面存储器和光存储器。若某个存储单元所存储的信息被读出时，原存储信息被破坏，则称为破坏性读出；若读出时，被读单元原存储信息不被破坏，则称为非破坏性读出。具有破坏性读出性能的存储器，每次读出操作后，必须紧接一个再生的操作，以便恢复被破坏的信息。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315141745772.png" alt="image-20230315141745772"></p><h4 id="5-2存储器的性能指标"><a href="#5-2存储器的性能指标" class="headerlink" title="5.2存储器的性能指标"></a>5.2存储器的性能指标</h4><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/20210118125344491.png" alt="在这里插入图片描述"></p><h4 id="5-3存储器的层次结构"><a href="#5-3存储器的层次结构" class="headerlink" title="5.3存储器的层次结构"></a>5.3存储器的层次结构</h4><p>1.三个主要特征的关系</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315142102827.png" alt="image-20230315142102827"></p><p>2.缓存一主存层次和主存一辅存层次</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315142136765.png" alt="image-20230315142136765"></p><p>cpu提升速度远快于主存，所以在二者之间加了一个缓存</p><p>cpu可以通过直接访问缓存从而访问主存的信息</p><blockquote><p>主存和缓存之间通过硬件直接设计连接到一起，不需要机器语言程序员考虑</p><p>而主存和辅存之间则软硬件结合</p></blockquote><h4 id="5-4主存"><a href="#5-4主存" class="headerlink" title="5.4主存"></a>5.4主存</h4><h5 id="1-主存的基本组成"><a href="#1-主存的基本组成" class="headerlink" title="1.主存的基本组成"></a>1.主存的基本组成</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230320210244821.png" alt="image-20230320210244821"></p><h5 id="2-主存和cpu的关系"><a href="#2-主存和cpu的关系" class="headerlink" title="2.主存和cpu的关系"></a>2.主存和cpu的关系</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230320214104344.png" alt="image-20230320214104344"></p><h5 id="3-主存中存储单元地址的分配"><a href="#3-主存中存储单元地址的分配" class="headerlink" title="3.主存中存储单元地址的分配"></a>3.主存中存储单元地址的分配</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230320214340051.png" alt="image-20230320214340051"></p><h5 id="4-主存的技术指标"><a href="#4-主存的技术指标" class="headerlink" title="4.主存的技术指标"></a>4.主存的技术指标</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230320214403647.png" alt="image-20230320214403647"></p><h4 id="5-5半导体存储芯片"><a href="#5-5半导体存储芯片" class="headerlink" title="5.5半导体存储芯片"></a>5.5半导体存储芯片</h4><h5 id="1-半导体存储芯片的基本结构"><a href="#1-半导体存储芯片的基本结构" class="headerlink" title="1.半导体存储芯片的基本结构"></a>1.半导体存储芯片的基本结构</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230320221448301.png" alt="image-20230320221448301"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230320222022340.png" alt="image-20230320222022340"></p><h5 id="2-半导体芯片译码驱动法"><a href="#2-半导体芯片译码驱动法" class="headerlink" title="2.半导体芯片译码驱动法"></a>2.半导体芯片译码驱动法</h5><h6 id="1-线选法"><a href="#1-线选法" class="headerlink" title="1.线选法"></a>1.线选法</h6><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230321000200109.png" alt="image-20230321000200109"></p><h6 id="2-重合法"><a href="#2-重合法" class="headerlink" title="2.重合法"></a>2.重合法</h6><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230321000229610.png" alt="image-20230321000229610"></p><h5 id="3-随机存取存储器（RAM）"><a href="#3-随机存取存储器（RAM）" class="headerlink" title="3.随机存取存储器（RAM）"></a>3.随机存取存储器（RAM）</h5><h6 id="1-静态RAM-SRAM"><a href="#1-静态RAM-SRAM" class="headerlink" title="1.静态RAM(SRAM)"></a>1.静态RAM(SRAM)</h6><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230321162919992.png" alt="image-20230321162919992"></p><p><strong>读写操作</strong></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327155902669.png" alt="image-20230327155902669"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327155919892.png" alt="image-20230327155919892"></p><h6 id="2-动态RAM-DRAM"><a href="#2-动态RAM-DRAM" class="headerlink" title="2.动态RAM(DRAM)"></a>2.动态RAM(DRAM)</h6><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327161252954.png" alt="image-20230327161252954"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327162525925.png" alt="image-20230327162525925"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327162606881.png" alt="image-20230327162606881"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327162626230.png" alt="image-20230327162626230"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327162659123.png" alt="image-20230327162659123"></p><h6 id="3-DRAM和SRAM比较"><a href="#3-DRAM和SRAM比较" class="headerlink" title="3.DRAM和SRAM比较"></a>3.DRAM和SRAM比较</h6><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327162751135.png" alt="image-20230327162751135"></p><h5 id="4-只读存储器（ROM）"><a href="#4-只读存储器（ROM）" class="headerlink" title="4.只读存储器（ROM）"></a>4.只读存储器（ROM）</h5><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230327163640770.png" alt="image-20230327163640770"></p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230327163653828.png" alt="image-20230327163653828"></p><h5 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h5><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230327164250734.png" alt="image-20230327164250734"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 计算机组成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch_learning</title>
      <link href="/2023/01/05/pytorch-learning/"/>
      <url>/2023/01/05/pytorch-learning/</url>
      
        <content type="html"><![CDATA[<p>本文参考：<a href="https://www.pytorchmaster.com/">https://www.pytorchmaster.com/</a></p><h3 id="torch函数"><a href="#torch函数" class="headerlink" title="torch函数"></a>torch函数</h3><h4 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat()"></a>torch.cat()</h4><p>函数目的： 在给定维度上对输入的张量序列seq 进行连接操作。</p><p><code>outputs = torch.cat(inputs, dim=?) → Tensor</code></p><p><strong>参数</strong></p><ul><li>inputs : 待连接的张量序列，可以是任意相同<code>Tensor</code>类型的python 序列</li><li>dim : 选择的扩维, 必须在<code>0</code>到<code>len(inputs[0])</code>之间，沿着此维连接张量序列。</li></ul><p><strong>重点</strong></p><ol><li>输入数据必须是序列，序列中数据是任意相同的<code>shape</code>的同类型<code>tensor</code></li><li>维度不可以超过输入数据的任一个张量的维度</li></ol><p><strong>例子</strong></p><ol><li>准备数据，每个的shape都是[2,3]</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x1 = torch.tensor([[<span class="number">11</span>,<span class="number">21</span>,<span class="number">31</span>],[<span class="number">21</span>,<span class="number">31</span>,<span class="number">41</span>]],dtype=torch.<span class="built_in">int</span>)</span><br><span class="line">x1.shape <span class="comment"># torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x2 = torch.tensor([[<span class="number">12</span>,<span class="number">22</span>,<span class="number">32</span>],[<span class="number">22</span>,<span class="number">32</span>,<span class="number">42</span>]],dtype=torch.<span class="built_in">int</span>)</span><br><span class="line">x2.shape  <span class="comment"># torch.Size([2, 3])</span></span><br></pre></td></tr></table></figure><ol start="2"><li>合成inputs</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27; inputs为２个形状为[2 , 3]的矩阵 &#x27;</span></span><br><span class="line">inputs = [x1, x2]</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br><span class="line"><span class="string">&#x27;打印查看&#x27;</span></span><br><span class="line">[tensor([[<span class="number">11</span>, <span class="number">21</span>, <span class="number">31</span>],</span><br><span class="line">         [<span class="number">21</span>, <span class="number">31</span>, <span class="number">41</span>]], dtype=torch.int32),</span><br><span class="line"> tensor([[<span class="number">12</span>, <span class="number">22</span>, <span class="number">32</span>],</span><br><span class="line">         [<span class="number">22</span>, <span class="number">32</span>, <span class="number">42</span>]], dtype=torch.int32)]</span><br></pre></td></tr></table></figure><ol start="3"><li>查看结果, 测试不同的dim拼接结果</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In    [<span class="number">1</span>]: torch.cat(inputs, dim=<span class="number">0</span>).shape</span><br><span class="line">Out   [<span class="number">1</span>]: torch.Size([<span class="number">4</span>,  <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">In    [<span class="number">2</span>]: torch.cat(inputs, dim=<span class="number">1</span>).shape</span><br><span class="line">Out   [<span class="number">2</span>]: torch.Size([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">In    [<span class="number">3</span>]: torch.cat(inputs, dim=<span class="number">2</span>).shape</span><br><span class="line">IndexError: Dimension out of <span class="built_in">range</span> (expected to be <span class="keyword">in</span> <span class="built_in">range</span> of [-<span class="number">2</span>, <span class="number">1</span>], but got <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h4 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack()"></a>torch.stack()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">T1 = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">T2 = torch.tensor([[<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>],</span><br><span class="line">                [<span class="number">40</span>, <span class="number">50</span>, <span class="number">60</span>],</span><br><span class="line">                [<span class="number">70</span>, <span class="number">80</span>, <span class="number">90</span>]])</span><br><span class="line"></span><br><span class="line">T3 = torch.stack((T1,T2),dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(T3.shape)</span><br><span class="line"><span class="built_in">print</span>(T3)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([2, 3, 3])</span></span><br><span class="line"><span class="string">tensor([[[ 1,  2,  3],</span></span><br><span class="line"><span class="string">         [ 4,  5,  6],</span></span><br><span class="line"><span class="string">         [ 7,  8,  9]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[10, 20, 30],</span></span><br><span class="line"><span class="string">         [40, 50, 60],</span></span><br><span class="line"><span class="string">         [70, 80, 90]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">T4 = torch.stack((T1,T2),dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(T4.shape)</span><br><span class="line"><span class="built_in">print</span>(T4)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([3, 2, 3])</span></span><br><span class="line"><span class="string">tensor([[[ 1,  2,  3],</span></span><br><span class="line"><span class="string">         [10, 20, 30]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 4,  5,  6],</span></span><br><span class="line"><span class="string">         [40, 50, 60]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 7,  8,  9],</span></span><br><span class="line"><span class="string">         [70, 80, 90]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">T5 = torch.stack((T1,T2),dim=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(T5.shape)</span><br><span class="line"><span class="built_in">print</span>(T5)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([3, 3, 2])</span></span><br><span class="line"><span class="string">tensor([[[ 1, 10],</span></span><br><span class="line"><span class="string">         [ 2, 20],</span></span><br><span class="line"><span class="string">         [ 3, 30]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 4, 40],</span></span><br><span class="line"><span class="string">         [ 5, 50],</span></span><br><span class="line"><span class="string">         [ 6, 60]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 7, 70],</span></span><br><span class="line"><span class="string">         [ 8, 80],</span></span><br><span class="line"><span class="string">         [ 9, 90]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="torch-ones-like"><a href="#torch-ones-like" class="headerlink" title="torch.ones_like()"></a>torch.ones_like()</h4><h4 id="torch-zeros-like"><a href="#torch-zeros-like" class="headerlink" title="torch.zeros_like()"></a>torch.zeros_like()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># 生成与input形状相同、元素全为1的张量</span></span><br><span class="line">a = torch.ones_like(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 生成与input形状相同、元素全为0的张量</span></span><br><span class="line">b = torch.zeros_like(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.1362, 0.6439, 0.3817],</span></span><br><span class="line"><span class="string">        [0.0971, 0.3498, 0.8780]])</span></span><br><span class="line"><span class="string">tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]])</span></span><br><span class="line"><span class="string">tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="torch-range"><a href="#torch-range" class="headerlink" title="torch.range()"></a>torch.range()</h4><h4 id="torch-arange"><a href="#torch-arange" class="headerlink" title="torch.arange()"></a>torch.arange()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y=torch.<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.dtype</span><br><span class="line">torch.float32</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z=torch.arange(<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.dtype</span><br><span class="line">torch.int64</span><br></pre></td></tr></table></figure><p>注意：</p><p>torch.range必须有begin和end值</p><p>但是torch.arange可以只有单值或设置间隔值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; torch.arange(<span class="number">4</span>)</span><br><span class="line">&gt;&gt; tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">&gt;&gt; torch.arange(<span class="number">1</span>,<span class="number">0.6</span>,-<span class="number">0.1</span>)</span><br><span class="line">&gt;&gt; tensor([<span class="number">1.0000</span>, <span class="number">0.9000</span>, <span class="number">0.8000</span>, <span class="number">0.7000</span>])</span><br></pre></td></tr></table></figure><h4 id="torch-normal"><a href="#torch-normal" class="headerlink" title="torch.normal()"></a>torch.normal()</h4><p>原型：<code>normal(mean, std, *, generator=None, out=None)</code></p><p>该函数返回从单独的<a href="https://so.csdn.net/so/search?q=%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83&spm=1001.2101.3001.7020">正态分布</a>中提取的随机数的张量，该正态分布的均值是mean，标准差是std。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.normal(mean=torch.arange(<span class="number">4.</span>),std=torch.arange(<span class="number">1.</span>,<span class="number">0.6</span>,-<span class="number">0.1</span>)).reshape(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[-1.4455,  0.9446],</span></span><br><span class="line"><span class="string">        [ 3.2138,  3.3914]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h3><h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><p>基本上趋同于numpy.array，但是<strong>不支持str</strong>!!!   包括：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.float64(torch.double),</span><br><span class="line">torch.float32(torch.<span class="built_in">float</span>),</span><br><span class="line">torch.float16,</span><br><span class="line">torch.int64(torch.long),</span><br><span class="line">torch.int32(torch.<span class="built_in">int</span>),</span><br><span class="line">torch.int16,</span><br><span class="line">torch.int8,</span><br><span class="line">torch.uint8,</span><br><span class="line">torch.<span class="built_in">bool</span></span><br></pre></td></tr></table></figure><p>一般神经网络都是用<code>torch.float32</code>类型</p><p>几种构造方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动推断</span></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>)</span><br><span class="line"><span class="built_in">print</span>(a,a.dtype)</span><br><span class="line"><span class="comment"># tensor(2.) torch.float32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定数据类型</span></span><br><span class="line">b = torch.tensor(<span class="number">2.0</span>, dtype=torch.double)</span><br><span class="line"><span class="built_in">print</span>(b,b.dtype)</span><br><span class="line"><span class="comment"># tensor(2.,dtype=torch.float64) torch.float64</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用特定构造函数 &amp; numpt转tensor</span></span><br><span class="line">c = torch.IntTensor(<span class="number">1</span>)</span><br><span class="line">d = torch.Tensor(np.array(<span class="number">2.0</span>))  <span class="comment">#等价于torch.FloatTensor</span></span><br><span class="line">e = torch.BoolTensor(np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(c,c.dtype)</span><br><span class="line"><span class="built_in">print</span>(d,d.dtype)</span><br><span class="line"><span class="built_in">print</span>(e,e.dtype)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([5], dtype=torch.int32) torch.int32</span></span><br><span class="line"><span class="string">tensor(2.) torch.float32</span></span><br><span class="line"><span class="string">tensor([ True, False,  True, False]) torch.bool</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同类型转换</span></span><br><span class="line">i = torch.tensor(<span class="number">1</span>); <span class="built_in">print</span>(i,i.dtype)</span><br><span class="line">x = i.<span class="built_in">float</span>(); <span class="built_in">print</span>(x,x.dtype) <span class="comment">#调用 float方法转换成浮点类型</span></span><br><span class="line">y = i.<span class="built_in">type</span>(torch.<span class="built_in">float</span>); <span class="built_in">print</span>(y,y.dtype) <span class="comment">#使用type函数转换成浮点类型</span></span><br><span class="line">z = i.type_as(x);<span class="built_in">print</span>(z,z.dtype) <span class="comment">#使用type_as方法转换成某个Tensor相同类型</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(1) torch.int64</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="张量的维度"><a href="#张量的维度" class="headerlink" title="张量的维度"></a>张量的维度</h4><p>不同类型数据会是不同的维度，<strong>有几层中括号就是多少维张量</strong></p><hr><p>标量为0维张量，向量为1维张量，矩阵为2维张量，彩色图像有rgb三个通道为3维张量，视频还有时间维表示为4维张量。</p><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scalar = torch.tensor(<span class="literal">True</span>) <span class="comment"># 0维</span></span><br><span class="line">vector = torch.tensor([<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>]) <span class="comment">#向量，1维张量</span></span><br><span class="line">matrix = torch.tensor([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]]) <span class="comment">#矩阵, 2维张量</span></span><br><span class="line">tensor3 = torch.tensor([[[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]],[[<span class="number">5.0</span>,<span class="number">6.0</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]]])  <span class="comment"># 3维张量</span></span><br><span class="line">tensor4 = torch.tensor([[[[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">2.0</span>,<span class="number">2.0</span>]],[[<span class="number">3.0</span>,<span class="number">3.0</span>],[<span class="number">4.0</span>,<span class="number">4.0</span>]]],</span><br><span class="line">                        [[[<span class="number">5.0</span>,<span class="number">5.0</span>],[<span class="number">6.0</span>,<span class="number">6.0</span>]],[[<span class="number">7.0</span>,<span class="number">7.0</span>],[<span class="number">8.0</span>,<span class="number">8.0</span>]]]])  <span class="comment"># 4维张量</span></span><br></pre></td></tr></table></figure><h4 id="张量的尺寸"><a href="#张量的尺寸" class="headerlink" title="张量的尺寸"></a>张量的尺寸</h4><ul><li>使用<code>shape</code>属性或者<code>size()</code>方法查看张量在每一维的长度</li><li>使用<code>view()</code>方法改变张量的尺寸,view()和numpy中的reshape很像，所以<strong>view()失败可以直接使用reshape</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有些操作会让张量存储结构扭曲，直接使用view会失败，可以用reshape方法</span></span><br><span class="line">matrix26 = torch.arange(<span class="number">0</span>,<span class="number">12</span>).view(<span class="number">2</span>,<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(matrix26)       <span class="comment"># tensor([[ 0,  1,  2,  3,  4,  5],</span></span><br><span class="line">          <span class="comment">#[ 6,  7,  8,  9, 10, 11]])</span></span><br><span class="line"><span class="built_in">print</span>(matrix26.shape) <span class="comment"># torch.Size([2, 6])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转置操作让张量存储结构扭曲</span></span><br><span class="line">matrix62 = matrix26.t()</span><br><span class="line"><span class="built_in">print</span>(matrix62.is_contiguous()) <span class="comment"># False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接使用view方法会失败，可以使用reshape方法</span></span><br><span class="line"><span class="comment">#matrix34 = matrix62.view(3,4) #error!</span></span><br><span class="line">matrix34 = matrix62.reshape(<span class="number">3</span>,<span class="number">4</span>) <span class="comment">#等价于matrix34 = matrix62.contiguous().view(3,4)</span></span><br><span class="line"><span class="built_in">print</span>(matrix34) </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 0,  6,  1,  7],</span></span><br><span class="line"><span class="string">        [ 2,  8,  3,  9],</span></span><br><span class="line"><span class="string">        [ 4, 10,  5, 11]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><strong>is_contiguous()：Tensor底层一维数组元素的存储顺序与Tensor按行优先一维展开的元素顺序是否一致</strong></p><p>行有限列有限博客：<a href="https://zhuanlan.zhihu.com/p/64551412">https://zhuanlan.zhihu.com/p/64551412</a></p><h4 id="张量和numpy数组"><a href="#张量和numpy数组" class="headerlink" title="张量和numpy数组"></a>张量和numpy数组</h4><p>numpy –&gt; tensor</p><ul><li>用numpy方法从Tensor得到numpy数组</li><li>用torch.from_numpy从numpy数组得到Tensor</li></ul><p>这两种方法关联的tensor和numpy数组是共享数据内存的，改变一个也会改变另一个。当然需要的话可以通过张量的<code>clone</code>方法拷贝张量中断这种关联。</p><p>此外可以使用<code>item</code>方法从标量张量得到对应的Python数值</p><p>​使用<code>tolist</code>方法从张量得到对应的Python数值列表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ndarray-&gt;tensor</span></span><br><span class="line">arr = np.zeros(<span class="number">3</span>)</span><br><span class="line">tensor = torch.from_numpy(arr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor-&gt;ndarray</span></span><br><span class="line">tensor = torch.zeros(<span class="number">3</span>)</span><br><span class="line">arr = tensor.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以用clone() 方法拷贝张量，中断这种关联</span></span><br><span class="line">tensor = torch.zeros(<span class="number">3</span>)</span><br><span class="line">arr = tensor.clone().numpy() <span class="comment"># 也可以使用tensor.data.numpy()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># item方法和tolist方法可以将张量转换成Python数值和数值列表</span></span><br><span class="line">scalar = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">s = scalar.item()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(s),<span class="string">&#x27; &#x27;</span>,s)</span><br><span class="line"><span class="comment"># &lt;class &#x27;float&#x27;&gt; 1.0</span></span><br><span class="line"></span><br><span class="line">tensor = torch.rand(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">t = tensor.tolist()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(t),<span class="string">&#x27; &#x27;</span>,t)</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;list&#x27;</span>&gt; [[<span class="number">0.8211846351623535</span>, <span class="number">0.20020723342895508</span>], [<span class="number">0.011571824550628662</span>, <span class="number">0.2906131148338318</span>]]</span><br></pre></td></tr></table></figure><h3 id="autograd自动微分"><a href="#autograd自动微分" class="headerlink" title="autograd自动微分"></a>autograd自动微分</h3><p>神经网络通常依靠反向传播求梯度更新网络参数，pytorch通过反向传播backward方法实现梯度计算，可以调用torch.autograd.grad函数来实现梯度计算，这就是Pytorch的自动微分机制</p><h4 id="backward求导数"><a href="#backward求导数" class="headerlink" title="backward求导数"></a>backward求导数</h4><p>backward 方法通常在一个标量张量上调用，该方法求得的梯度将存在对应自变量张量的grad属性下。</p><p>如果调用的张量非标量，则要传入一个和它同形状 的gradient参数张量。</p><p>相当于用该gradient参数张量与调用张量作向量点乘，得到的标量结果再反向传播。</p><ol><li>标量的反向传播</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line">dy_dx = x.grad</span><br><span class="line"><span class="built_in">print</span>(dy_dx) <span class="comment"># tensor(-2.)</span></span><br></pre></td></tr></table></figure><ol start="2"><li>非标量的反向传播</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c</span></span><br><span class="line">x = torch.tensor([[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">1.0</span>,<span class="number">2.0</span>]],requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">gradient = torch.tensor([[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x:\n&quot;</span>,x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y:\n&quot;</span>,y)</span><br><span class="line">y.backward(gradient = gradient)</span><br><span class="line">x_grad = x.grad</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_grad:\n&quot;</span>,x_grad)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x:</span></span><br><span class="line"><span class="string"> tensor([[0., 0.],</span></span><br><span class="line"><span class="string">        [1., 2.]], requires_grad=True)</span></span><br><span class="line"><span class="string">y:</span></span><br><span class="line"><span class="string"> tensor([[1., 1.],</span></span><br><span class="line"><span class="string">        [0., 1.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">x_grad:</span></span><br><span class="line"><span class="string"> tensor([[-2., -2.],</span></span><br><span class="line"><span class="string">        [ 0.,  2.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ol start="3"><li>非标量的反向传播可以用标量的反向传播实现</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor([[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">1.0</span>,<span class="number">2.0</span>]],requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">gradient = torch.tensor([[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line">z = torch.<span class="built_in">sum</span>(y*gradient)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x:&quot;</span>,x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y:&quot;</span>,y)</span><br><span class="line">z.backward()</span><br><span class="line">x_grad = x.grad</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_grad:\n&quot;</span>,x_grad)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x: tensor([[0., 0.],</span></span><br><span class="line"><span class="string">        [1., 2.]], requires_grad=True)</span></span><br><span class="line"><span class="string">y: tensor([[1., 1.],</span></span><br><span class="line"><span class="string">        [0., 1.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">x_grad:</span></span><br><span class="line"><span class="string"> tensor([[-2., -2.],</span></span><br><span class="line"><span class="string">        [ 0.,  2.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><hr><ul><li>在numpy和torch.tensor中  *  都是指相同size的矩阵各个位置相乘产生新矩阵</li><li>numpy中的矩阵乘法为<code>np.dot(a,b)</code>，torch中为<code>torch.matmul(c,d)</code></li></ul><hr><h4 id="autograd-grad自动微分"><a href="#autograd-grad自动微分" class="headerlink" title="autograd.grad自动微分"></a>autograd.grad自动微分</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line"></span><br><span class="line"><span class="comment"># create_graph 设置为 True 将允许创建更高阶的导数 </span></span><br><span class="line">dy_dx = torch.autograd.grad(y,x,create_graph=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(dy_dx.data)      <span class="comment"># tensor(-2.)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求二阶导数</span></span><br><span class="line">dy2_dx2 = torch.autograd.grad(dy_dx,x)[<span class="number">0</span>] </span><br><span class="line"><span class="built_in">print</span>(dy2_dx2.data) <span class="comment"># tensor(2.)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">x1 = torch.tensor(<span class="number">1.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">x2 = torch.tensor(<span class="number">2.0</span>,requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y1 = x1*x2</span><br><span class="line">y2 = x1+x2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 允许同时对多个自变量求导数</span></span><br><span class="line">(dy1_dx1,dy1_dx2) = torch.autograd.grad(outputs=y1,inputs = [x1,x2],retain_graph = <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(dy1_dx1,dy1_dx2)</span><br><span class="line"><span class="comment"># tensor(2.) tensor(1.)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果有多个因变量，相当于把多个因变量的梯度结果求和</span></span><br><span class="line">(dy12_dx1,dy12_dx2) = torch.autograd.grad(outputs=[y1,y2],inputs = [x1,x2])</span><br><span class="line"><span class="built_in">print</span>(dy12_dx1,dy12_dx2)</span><br><span class="line"><span class="comment"># tensor(3.) tensor(2.)</span></span><br></pre></td></tr></table></figure><h4 id="利用autograd和optimizer求最小值"><a href="#利用autograd和optimizer求最小值" class="headerlink" title="利用autograd和optimizer求最小值"></a>利用autograd和optimizer求最小值</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>, requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(params=[x], lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    result = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    <span class="keyword">return</span> (result)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    y = f(x)</span><br><span class="line">    y.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;y= <span class="subst">&#123;f(x).data&#125;</span>; x= <span class="subst">&#123;x.data&#125;</span>&quot;</span>) <span class="comment"># f-string格式化</span></span><br><span class="line"><span class="comment"># y= tensor(0.) ; x= tensor(1.0000)</span></span><br></pre></td></tr></table></figure><h3 id="动态计算图"><a href="#动态计算图" class="headerlink" title="动态计算图"></a>动态计算图</h3><p>之前的一些深度学习框架使用静态计算图，而pytorch使用动态计算图，有它的优点</p><h4 id="动态图简介"><a href="#动态图简介" class="headerlink" title="动态图简介"></a>动态图简介</h4><p><img src="https://www.pytorchmaster.com/data/torch%E5%8A%A8%E6%80%81%E5%9B%BE.gif" alt="img"></p><p>pytorch的计算图由<strong>节点</strong>和<strong>边</strong>组成，节点表示<strong>张量</strong>或<strong>Function</strong>,边表示张量和Function之间的依赖关系</p><p>动态有两重含义</p><ul><li>计算图正向传播是立即执行的，无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到的计算结果。</li><li>计算图在反向传播后立即销毁。下次调用需要重新构建计算图。如果在程序中使用了backward方法执行了反向传播，或者利用torch.autograd.grad方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建</li></ul><p>1.计算图的正向传播是立即执行的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.randn(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">Y = torch.randn(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">Y_hat = X@w.t() + b  <span class="comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span></span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss.data)</span><br><span class="line"><span class="built_in">print</span>(Y_hat.data)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(17.8969)</span></span><br><span class="line"><span class="string">tensor([[3.2613],</span></span><br><span class="line"><span class="string">        [4.7322],</span></span><br><span class="line"><span class="string">        [4.5037],</span></span><br><span class="line"><span class="string">        [7.5899],</span></span><br><span class="line"><span class="string">        [7.0973],</span></span><br><span class="line"><span class="string">        [1.3287],</span></span><br><span class="line"><span class="string">        [6.1473],</span></span><br><span class="line"><span class="string">        [1.3492],</span></span><br><span class="line"><span class="string">        [1.3911],</span></span><br><span class="line"><span class="string">        [1.2150]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>2.计算图在反向传播后立即销毁</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.randn(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">Y = torch.randn(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">Y_hat = X@w.t() + b  <span class="comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span></span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算图在反向传播后立即销毁，如果需要保留计算图, 需要设置retain_graph = True</span></span><br><span class="line">loss.backward()  <span class="comment">#loss.backward(retain_graph = True) </span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss.backward() <span class="comment">#如果再次执行反向传播将报错</span></span><br></pre></td></tr></table></figure><p>RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph&#x3D;True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.</p><p>第二次尝试向后遍历图形（或在张量已被释放后直接访问已保存的张量）。当您调用 .backward() 或 autograd.grad() 时，图形的已保存中间值将被释放。如果您需要第二次向后遍历图形，或者如果您需要在向后调用后访问保存的张量，请指定 retain_graph&#x3D;True。</p><h4 id="计算图中的Function"><a href="#计算图中的Function" class="headerlink" title="计算图中的Function"></a>计算图中的Function</h4><p>计算图中除了张量的另外一种节点是<code>Function</code>, 实际上就是 Pytorch中各种对张量操作的函数</p><p>这些Function和我们python中的函数有一个较大的区别，那就是它<strong>同时包含正向计算逻辑和反向传播逻辑</strong></p><p>我们可以通过继承torch.autograd.Function来创建这种支持反向传播的Function</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyReLU</span>(torch.autograd.Function):</span><br><span class="line">    <span class="comment"># 正向传播逻辑，可以用ctx存储一些值，供反向传播使用</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span></span>):</span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span>.clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播逻辑</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        <span class="built_in">input</span>, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[<span class="built_in">input</span> &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.tensor([[-<span class="number">1.0</span>,-<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>,<span class="number">3.0</span>]])</span><br><span class="line"></span><br><span class="line">relu = MyReLU.apply <span class="comment"># relu现在也可以具有正向传播和反向传播功能</span></span><br><span class="line">Y_hat = relu(X@w.t() + b)</span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(w.grad)<span class="comment">#tensor([[4.5000, 4.5000]])</span></span><br><span class="line"><span class="built_in">print</span>(b.grad)<span class="comment">#tensor([[4.5000]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Y_hat的梯度函数即是我们自己所定义的 MyReLU.backward</span></span><br><span class="line"><span class="built_in">print</span>(Y_hat.grad_fn)<span class="comment">#&lt;torch.autograd.function.MyReLUBackward object at 0x1205a46c8&gt;</span></span><br></pre></td></tr></table></figure><h4 id="计算图与反向传播"><a href="#计算图与反向传播" class="headerlink" title="计算图与反向传播"></a>计算图与反向传播</h4><p>简单理解反向传播的原理和过程（链式法则）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x + <span class="number">1</span></span><br><span class="line">y2 = <span class="number">2</span>*x</span><br><span class="line">loss = (y1-y2)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure><p>loss.backward()语句调用后，依次发生以下计算过程。</p><ol><li><p>loss自己的grad梯度赋值为1，即对自身的梯度为1。</p></li><li><p>loss根据其自身梯度以及关联的backward方法，计算出其对应的自变量即y1和y2的梯度，将该值赋值到y1.grad和y2.grad。</p></li><li><p>y2和y1根据其自身梯度以及关联的backward方法, 分别计算出其对应的自变量x的梯度，x.grad将其收到的多个梯度值累加。</p></li></ol><p>（注意，1,2,3步骤的求梯度顺序和对多个梯度值的累加规则恰好是求导链式法则的程序表述）</p><p>正因为求导链式法则衍生的梯度累加规则，张量的grad梯度不会自动清零，在需要的时候需要手动置零。</p><h4 id="叶子节点和非叶子节点"><a href="#叶子节点和非叶子节点" class="headerlink" title="叶子节点和非叶子节点"></a>叶子节点和非叶子节点</h4><p>执行<a href="#jump">上述代码</a>，我们会发现loss.grad并不是我们期望的1，而是None。类似地 y1.grad 以及 y2.grad也是 None。</p><p>这是<strong>由于它们不是叶子节点张量</strong>。</p><p>在反向传播过程中，只有 <code>is_leaf=True</code> 的叶子节点，需要求导的张量的导数结果才会被最后保留下来。</p><p>那么什么是叶子节点张量呢？叶子节点张量需要满足两个条件。</p><p>1，叶子节点张量是由用户直接创建的张量，而非由某个Function通过计算得到的张量。</p><p>2，叶子节点张量的 requires_grad属性必须为True.</p><p>Pytorch设计这样的规则主要是为了节约内存或者显存空间，因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。</p><p>所有依赖于叶子节点张量的张量, 其requires_grad 属性必定是True的，但其梯度值只在计算过程中被用到，不会最终存储到grad属性中。</p><p>如果需要保留中间计算结果的梯度到grad属性中，可以使用 retain_grad方法。 如果仅仅是为了调试代码查看梯度值，可以利用register_hook打印日志。<span id="jump"></span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x + <span class="number">1</span></span><br><span class="line">y2 = <span class="number">2</span>*x</span><br><span class="line">loss = (y1-y2)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;loss.grad:&quot;</span>, loss.grad) <span class="comment"># loss.grad: None</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y1.grad:&quot;</span>, y1.grad)<span class="comment"># y1.grad: None</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y2.grad:&quot;</span>, y2.grad)<span class="comment"># y2.grad: None</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)<span class="comment"># tensor(4.)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.is_leaf) <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(y1.is_leaf) <span class="comment"># False</span></span><br><span class="line"><span class="built_in">print</span>(y2.is_leaf) <span class="comment"># False</span></span><br><span class="line"><span class="built_in">print</span>(loss.is_leaf)<span class="comment"># False</span></span><br></pre></td></tr></table></figure><p>利用retain_grad可以保留非叶子节点的梯度值，利用register_hook可以查看非叶子节点的梯度值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment">#正向传播</span></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x + <span class="number">1</span></span><br><span class="line">y2 = <span class="number">2</span>*x</span><br><span class="line">loss = (y1-y2)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#非叶子节点梯度显示控制</span></span><br><span class="line">y1.register_hook(<span class="keyword">lambda</span> grad: <span class="built_in">print</span>(<span class="string">&#x27;y1 grad: &#x27;</span>, grad))<span class="comment"># y2 grad:  tensor(4.)</span></span><br><span class="line">y2.register_hook(<span class="keyword">lambda</span> grad: <span class="built_in">print</span>(<span class="string">&#x27;y2 grad: &#x27;</span>, grad))<span class="comment"># y1 grad:  tensor(-4.)</span></span><br><span class="line">loss.retain_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;loss.grad:&quot;</span>, loss.grad)<span class="comment"># loss.grad: tensor(1.)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x.grad:&quot;</span>, x.grad) <span class="comment"># loss.grad: tensor(1.)</span></span><br></pre></td></tr></table></figure><h4 id="计算图在TensorBoard中的可视化"><a href="#计算图在TensorBoard中的可视化" class="headerlink" title="计算图在TensorBoard中的可视化"></a>计算图在TensorBoard中的可视化</h4><p>可以利用 torch.utils.tensorboard 将计算图导出到 TensorBoard进行可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.w = nn.Parameter(torch.randn(<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">        self.b = nn.Parameter(torch.zeros(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = x@self.w + self.b</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./data/tensorboard&#x27;</span>)</span><br><span class="line">writer.add_graph(net,input_to_model = torch.rand(<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line">%load_ext tensorboard</span><br><span class="line"><span class="comment">#%tensorboard --logdir ./data/tensorboard</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorboard <span class="keyword">import</span> notebook</span><br><span class="line">notebook.<span class="built_in">list</span>() </span><br><span class="line"></span><br><span class="line"><span class="comment">#在tensorboard中查看模型</span></span><br><span class="line">notebook.start(<span class="string">&quot;--logdir ./data/tensorboard&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://www.pytorchmaster.com/data/2-3-%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt="img"></p><h3 id="pytorch的层次结构"><a href="#pytorch的层次结构" class="headerlink" title="pytorch的层次结构"></a>pytorch的层次结构</h3><p>5个不同的层次结构：即硬件层，内核层，低阶API，中阶API，高阶API [torchkeras]</p><p>Pytorch的层次结构从低到高可以分成如下五层。</p><p>最底层为硬件层，Pytorch支持CPU、GPU加入计算资源池。</p><p>第二层为C++实现的内核。</p><p>第三层为Python实现的操作符，提供了封装C++内核的低级API指令，主要包括各种张量操作算子、自动微分、变量管理. 如torch.tensor,torch.cat,torch.autograd.grad,nn.Module. 如果把模型比作一个房子，那么第三层API就是【模型之砖】。</p><p>第四层为Python实现的模型组件，对低级API进行了函数封装，主要包括各种模型层，损失函数，优化器，数据管道等等。 如torch.nn.Linear,torch.nn.BCE,torch.optim.Adam,torch.utils.data.DataLoader. 如果把模型比作一个房子，那么第四层API就是【模型之墙】。</p><p>第五层为Python实现的模型接口。Pytorch没有官方的高阶API。为了便于训练模型，<a href="https://github.com/lyhue1991/eat_pytorch_in_20_days">博客作者</a>仿照keras中的模型接口，使用了不到300行代码，封装了Pytorch的高阶模型接口torchkeras.Model。如果把模型比作一个房子，那么第五层API就是模型本身，即【模型之屋】。</p><h4 id="低阶API示范"><a href="#低阶API示范" class="headerlink" title="低阶API示范"></a>低阶API示范</h4><p>下面的范例使用Pytorch的低阶API实现线性回归模型和DNN二分类模型</p><p>低阶API主要包括<strong>张量操作</strong>，<strong>计算图</strong>和<strong>自动微分</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">printbar</span>():</span><br><span class="line">    nowtime = datetime.datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;==========&quot;</span>*<span class="number">8</span> + <span class="string">&quot;%s&quot;</span>%nowtime)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mac系统上pytorch和matplotlib在jupyter中同时跑需要更改环境变量</span></span><br><span class="line"><span class="comment"># os.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;]=&quot;TRUE&quot; </span></span><br></pre></td></tr></table></figure><h5 id="一、线性回归模型"><a href="#一、线性回归模型" class="headerlink" title="一、线性回归模型"></a>一、线性回归模型</h5><ol><li>data prepare</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#样本数量</span></span><br><span class="line">n = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成测试用数据集</span></span><br><span class="line">X = <span class="number">10</span>*torch.rand([n,<span class="number">2</span>])-<span class="number">5.0</span>  <span class="comment">#torch.rand是均匀分布 </span></span><br><span class="line">w0 = torch.tensor([[<span class="number">2.0</span>],[-<span class="number">3.0</span>]])</span><br><span class="line">b0 = torch.tensor([[<span class="number">10.0</span>]])</span><br><span class="line">Y = X@w0 + b0 + torch.normal( <span class="number">0.0</span>,<span class="number">2.0</span>,size = [n,<span class="number">1</span>])  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;svg&#x27;</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))  <span class="comment"># 图形窗口的宽度为 12 英寸，高度为 5 英寸</span></span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line"><span class="comment">#画散点图 </span></span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>].numpy(),Y[:,<span class="number">0</span>].numpy(), c = <span class="string">&quot;b&quot;</span>,label = <span class="string">&quot;samples&quot;</span>)</span><br><span class="line"><span class="comment">#设置图例</span></span><br><span class="line">ax1.legend()  <span class="comment"># 没有此不会有samples图例</span></span><br><span class="line">plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>].numpy(),Y[:,<span class="number">0</span>].numpy(), c = <span class="string">&quot;g&quot;</span>,label = <span class="string">&quot;samples&quot;</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>,rotation = <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230119174857563.png" alt="image-20230119174857563"></p><p>注：**<code>plt.subplot(121)</code> 是 matplotlib 库中用于创建多个子图的函数之一。其中，<code>121</code> 是参数，表示将整个图分成 1 行 2 列，并在第 1 个位置创建子图。因此，这条语句将创建一个 1 行 2 列的子图网格，并在第一个子图中绘制图形。**</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建数据管道迭代器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">features, labels, batch_size=<span class="number">8</span></span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    np.random.shuffle(indices)  <span class="comment">#样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        indexs = torch.LongTensor(indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span>  features.index_select(<span class="number">0</span>, indexs), labels.index_select(<span class="number">0</span>, indexs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据管道效果   </span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">(features,labels) = <span class="built_in">next</span>(data_iter(X,Y,batch_size))</span><br><span class="line"><span class="built_in">print</span>(features)</span><br><span class="line"><span class="built_in">print</span>(labels)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 1.0449, -0.3581],</span></span><br><span class="line"><span class="string">        [-3.0645, -2.9230],</span></span><br><span class="line"><span class="string">        [ 3.7969, -4.5846],</span></span><br><span class="line"><span class="string">        [-0.2429,  0.5349],</span></span><br><span class="line"><span class="string">        [ 2.2708,  0.1713],</span></span><br><span class="line"><span class="string">        [ 4.6910,  4.3684],</span></span><br><span class="line"><span class="string">        [ 2.1360, -4.7411],</span></span><br><span class="line"><span class="string">        [ 3.3687,  0.3648]])</span></span><br><span class="line"><span class="string">tensor([[15.6397],</span></span><br><span class="line"><span class="string">        [14.1632],</span></span><br><span class="line"><span class="string">        [31.6240],</span></span><br><span class="line"><span class="string">        [ 7.4723],</span></span><br><span class="line"><span class="string">        [11.8881],</span></span><br><span class="line"><span class="string">        [ 6.8064],</span></span><br><span class="line"><span class="string">        [30.4618],</span></span><br><span class="line"><span class="string">        [15.2579]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ol start="2"><li>define the model</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegression</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.w = torch.randn_like(w0,requires_grad=<span class="literal">True</span>)</span><br><span class="line">        self.b = torch.zeros_like(b0,requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">#正向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>): </span><br><span class="line">        <span class="keyword">return</span> x@self.w + self.b</span><br><span class="line">    <span class="comment"># 损失函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss_func</span>(<span class="params">self,y_pred,y_true</span>):  </span><br><span class="line">        <span class="keyword">return</span> torch.mean((y_pred - y_true)**<span class="number">2</span>/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model = LinearRegression()</span><br></pre></td></tr></table></figure><ol start="3"><li>model training</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">model, features, labels</span>):</span><br><span class="line">    predictions = model.forward(features)</span><br><span class="line">    loss = model.loss_func(predictions,labels)</span><br><span class="line">    <span class="comment"># 反向传播求梯度</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用torch.no_grad()避免梯度记录，也可以通过操作 model.w.data 实现避免梯度记录 </span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 梯度下降法更新参数</span></span><br><span class="line">        model.w -= <span class="number">0.001</span>*model.w.grad</span><br><span class="line">        model.b -= <span class="number">0.001</span>*model.b.grad</span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        model.w.grad.zero_()</span><br><span class="line">        model.b.grad.zero_()</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试train_step效果</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">(features,labels) = <span class="built_in">next</span>(data_iter(X,Y,batch_size))</span><br><span class="line">train_step(model,features,labels)</span><br></pre></td></tr></table></figure><p>Out[]: tensor(141.2280, grad_fn&#x3D;<MeanBackward0>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">model,epochs</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> data_iter(X,Y,<span class="number">10</span>):</span><br><span class="line">            loss = train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">200</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;epoch =&quot;</span>,epoch,<span class="string">&quot;loss = &quot;</span>,loss.item())</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;model.w =&quot;</span>,model.w.data)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;model.b =&quot;</span>,model.b.data)</span><br><span class="line"></span><br><span class="line">train_model(model,epochs = <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">================================================================================2023-01-20 17:03:37</span><br><span class="line">epoch = 200 loss =  2.0936481952667236</span><br><span class="line">model.w = tensor([[ 2.0185],</span><br><span class="line">        [-2.9519]])</span><br><span class="line">model.b = tensor([[10.0203]])</span><br><span class="line"></span><br><span class="line">================================================================================2023-01-20 17:03:38</span><br><span class="line">epoch = 400 loss =  1.3962600231170654</span><br><span class="line">model.w = tensor([[ 2.0212],</span><br><span class="line">        [-2.9512]])</span><br><span class="line">model.b = tensor([[10.0241]])</span><br><span class="line"></span><br><span class="line">================================================================================2023-01-20 17:03:39</span><br><span class="line">epoch = 600 loss =  0.3997553586959839</span><br><span class="line">model.w = tensor([[ 2.0167],</span><br><span class="line">        [-2.9528]])</span><br><span class="line">model.b = tensor([[10.0237]])</span><br><span class="line"></span><br><span class="line">================================================================================2023-01-20 17:03:41</span><br><span class="line">epoch = 800 loss =  1.8717002868652344</span><br><span class="line">model.w = tensor([[ 2.0194],</span><br><span class="line">        [-2.9495]])</span><br><span class="line">model.b = tensor([[10.0240]])</span><br><span class="line"></span><br><span class="line">================================================================================2023-01-20 17:03:42</span><br><span class="line">epoch = 1000 loss =  1.8542640209197998</span><br><span class="line">model.w = tensor([[ 2.0176],</span><br><span class="line">        [-2.9507]])</span><br><span class="line">model.b = tensor([[10.0237]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>].numpy(),Y[:,<span class="number">0</span>].numpy(), c = <span class="string">&quot;b&quot;</span>,label = <span class="string">&quot;samples&quot;</span>)</span><br><span class="line">ax1.plot(X[:,<span class="number">0</span>].numpy(),(model.w[<span class="number">0</span>].data*X[:,<span class="number">0</span>]+model.b[<span class="number">0</span>].data).numpy(),<span class="string">&quot;-r&quot;</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">&quot;model&quot;</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>].numpy(),Y[:,<span class="number">0</span>].numpy(), c = <span class="string">&quot;g&quot;</span>,label = <span class="string">&quot;samples&quot;</span>)</span><br><span class="line">ax2.plot(X[:,<span class="number">1</span>].numpy(),(model.w[<span class="number">1</span>].data*X[:,<span class="number">1</span>]+model.b[<span class="number">0</span>].data).numpy(),<span class="string">&quot;-r&quot;</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">&quot;model&quot;</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230120170539911.png" alt="image-20230120170539911"></p><h5 id="二、DNN二分类模型"><a href="#二、DNN二分类模型" class="headerlink" title="二、DNN二分类模型"></a>二、DNN二分类模型</h5><ol><li>data prepare</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;svg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正负样本数量</span></span><br><span class="line">n_positive,n_negative = <span class="number">2000</span>,<span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正样本, 小圆环分布</span></span><br><span class="line">r_p = <span class="number">5.0</span> + torch.normal(<span class="number">0.0</span>,<span class="number">1.0</span>,size = [n_positive,<span class="number">1</span>]) </span><br><span class="line">theta_p = <span class="number">2</span>*np.pi*torch.rand([n_positive,<span class="number">1</span>])</span><br><span class="line">Xp = torch.cat([r_p*torch.cos(theta_p),r_p*torch.sin(theta_p)],axis = <span class="number">1</span>)</span><br><span class="line">Yp = torch.ones_like(r_p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成负样本, 大圆环分布</span></span><br><span class="line">r_n = <span class="number">8.0</span> + torch.normal(<span class="number">0.0</span>,<span class="number">1.0</span>,size = [n_negative,<span class="number">1</span>]) </span><br><span class="line">theta_n = <span class="number">2</span>*np.pi*torch.rand([n_negative,<span class="number">1</span>])</span><br><span class="line">Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = <span class="number">1</span>)</span><br><span class="line">Yn = torch.zeros_like(r_n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#汇总样本</span></span><br><span class="line">X = torch.cat([Xp,Xn],axis = <span class="number">0</span>)</span><br><span class="line">Y = torch.cat([Yp,Yn],axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize = (<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">&quot;r&quot;</span>)</span><br><span class="line">plt.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">&quot;g&quot;</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;positive&quot;</span>,<span class="string">&quot;negative&quot;</span>])</span><br></pre></td></tr></table></figure><p><img src="https://www.pytorchmaster.com/data/3-1-%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt="img"></p>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python小知识点</title>
      <link href="/2023/01/04/python%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
      <url>/2023/01/04/python%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p>python小知识点</p><h4 id="可变类型拷贝"><a href="#可变类型拷贝" class="headerlink" title="可变类型拷贝"></a>可变类型拷贝</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;raw data.txt&quot;</span>,<span class="string">&quot;r&quot;</span>)<span class="keyword">as</span> f:</span><br><span class="line">raw data f.readlines()</span><br><span class="line">raw_data [[<span class="built_in">int</span>(i)<span class="keyword">for</span> i <span class="keyword">in</span> data.strip().split(<span class="string">&quot;&quot;</span>)<span class="keyword">for</span> data <span class="keyword">in</span> raw data]</span><br><span class="line"><span class="comment">#[[1,2,3,4,5],[6,7,8,9,0],[2,2,2,2,2],[4,4,4,4,4],[6,7,1,9,8]]</span></span><br><span class="line"></span><br><span class="line">raw_data_copy = rawdata.copy() <span class="comment"># 一维数据copy样拷贝还会改变原数据</span></span><br><span class="line">raw_data_copy = copy.deepcopy(raw_data)  <span class="comment">#多维数据拷贝</span></span><br><span class="line">fraw_data_copy[<span class="number">0</span>][<span class="number">0</span>]<span class="number">99999</span></span><br><span class="line"><span class="built_in">print</span>(raw_data)</span><br><span class="line"><span class="comment">#[1,2,3,4,5],[6,7,8,9,0],[2,2,2,2,2],[4,4,</span></span><br><span class="line"><span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>],<span class="number">6</span>,<span class="number">7</span>,<span class="number">1</span>,<span class="number">9</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>函数默认参数为可变类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_fruit</span>(<span class="params">fruit,fruit <span class="built_in">list</span>=[]</span>):</span><br><span class="line">fruit_list.append(fruit)</span><br><span class="line"><span class="built_in">print</span>(fruit_list)</span><br><span class="line">fruits=[<span class="string">&#x27;banana&#x27;</span>,<span class="string">&#x27;apple&#x27;</span>]</span><br><span class="line">add_fruit(<span class="string">&#x27;watermelon&#x27;</span>,fruits)</span><br><span class="line">[<span class="string">&#x27;banana&#x27;</span>,<span class="string">&#x27;apple&#x27;</span>,<span class="string">&#x27;watermelon&#x27;</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_fruit</span>(<span class="params">fruit,fruit <span class="built_in">list</span>=[]</span>):</span><br><span class="line">fruit_list.append(fruit)</span><br><span class="line"><span class="built_in">print</span>(fruit_list)</span><br><span class="line"></span><br><span class="line">add fruit(<span class="string">&#x27;watermelon&#x27;</span>) <span class="comment">#[&#x27;watermelon&#x27;]</span></span><br><span class="line">add fruit(<span class="string">&#x27;banana&#x27;</span>)  <span class="comment">#[&#x27;watermelon&#x27;, &#x27;banana&#x27;]</span></span><br></pre></td></tr></table></figure><p>由于可变，所以fruit_list会在前面的基础上调用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_fruit</span>(<span class="params">fruit,fruit <span class="built_in">list</span>=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> fruit_list <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        fruit_list = []</span><br><span class="line">fruit_list.append(fruit)</span><br><span class="line"><span class="built_in">print</span>(fruit_list)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(add_fruit.__defaults__ ) <span class="comment"># (None, )</span></span><br><span class="line">add_fruit(<span class="string">&#x27;watermelon&#x27;</span>) <span class="comment"># [&#x27;watermelon&#x27;]</span></span><br><span class="line"><span class="built_in">print</span>(add_fruit.__defaults__ ) <span class="comment"># (None, )</span></span><br><span class="line">add_ fruit( <span class="string">&#x27;banana&#x27;</span> ) <span class="comment">#[&#x27;banana&#x27;]</span></span><br><span class="line"><span class="built_in">print</span>(add_fruit.__defaults__ ) <span class="comment"># (None, )</span></span><br></pre></td></tr></table></figure><p>所以尽可能避免将可变类型作为默认参数，而是在内部判断</p><h4 id="函数默认值属性"><a href="#函数默认值属性" class="headerlink" title="函数默认值属性"></a>函数默认值属性</h4><p><strong>在定义时就被确定了</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">display_time</span>(<span class="params">data=datetime.now(<span class="params"> </span>)</span>) :</span><br><span class="line"><span class="built_in">print</span>(data.strftime(<span class="string">&#x27;%B %d, %Y %H:%M:%S&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span> (display_time.__defaults__ ) <span class="comment"># (datetime.datetime (2022,11,26,17, 4, 53, 360783), )</span></span><br><span class="line">display_time() <span class="comment"># November 26, 2022 17 :04:53</span></span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line">display_time() <span class="comment"># November 26, 2022 17 :04:53 </span></span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line">display_time() <span class="comment"># November 26, 2022 17 :04:53</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>修改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">display_time</span>(<span class="params">data=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> data <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        data = datatime.Now()</span><br><span class="line"><span class="built_in">print</span>(data.strftime(<span class="string">&#x27;%B %d, %Y %H:%M:%S&#x27;</span>))</span><br></pre></td></tr></table></figure><h4 id="下划线的含义"><a href="#下划线的含义" class="headerlink" title="下划线的含义"></a>下划线的含义</h4><p>· 单引号下划线： <code>_var</code></p><p>单下划线是一种Python命名约定，表示某个名称是供内部使用的，只是对程序员的提示，不会有多余的效果</p><p>· 单尾划线： <code>var_</code></p><p>一个变量最合适的名字已经被一个关键字代替了等情况，打破命名冲突</p><p>· 双领先下划线： <code>__var</code></p><p>双下划线前缀导致Python解释器重写属性名，以避免子类中的命名冲突</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Test</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.foo = <span class="number">11</span></span><br><span class="line">        self._bar = <span class="number">23</span></span><br><span class="line">        self.__baz = <span class="number">23</span></span><br><span class="line">        </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = Test()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">dir</span>(t)</span><br><span class="line">[<span class="string">&#x27;_Test__baz&#x27;</span>, <span class="string">&#x27;__class__&#x27;</span>, <span class="string">&#x27;__delattr__&#x27;</span>, <span class="string">&#x27;__dict__&#x27;</span>, <span class="string">&#x27;__dir__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__doc__&#x27;</span>, <span class="string">&#x27;__eq__&#x27;</span>, <span class="string">&#x27;__format__&#x27;</span>, <span class="string">&#x27;__ge__&#x27;</span>, <span class="string">&#x27;__getattribute__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__gt__&#x27;</span>, <span class="string">&#x27;__hash__&#x27;</span>, <span class="string">&#x27;__init__&#x27;</span>, <span class="string">&#x27;__le__&#x27;</span>, <span class="string">&#x27;__lt__&#x27;</span>, <span class="string">&#x27;__module__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__ne__&#x27;</span>, <span class="string">&#x27;__new__&#x27;</span>, <span class="string">&#x27;__reduce__&#x27;</span>, <span class="string">&#x27;__reduce_ex__&#x27;</span>, <span class="string">&#x27;__repr__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__setattr__&#x27;</span>, <span class="string">&#x27;__sizeof__&#x27;</span>, <span class="string">&#x27;__str__&#x27;</span>, <span class="string">&#x27;__subclasshook__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__weakref__&#x27;</span>, <span class="string">&#x27;_bar&#x27;</span>, <span class="string">&#x27;foo&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t.__dict__</span><br><span class="line">&#123;<span class="string">&#x27;foo&#x27;</span>: <span class="number">11</span>, <span class="string">&#x27;_bar&#x27;</span>: <span class="number">23</span>, <span class="string">&#x27;_Test__baz&#x27;</span>: <span class="number">23</span>&#125;</span><br></pre></td></tr></table></figure><p>补充<code>__dict__</code>的博客：<a href="http://c.biancheng.net/view/2374.html">http://c.biancheng.net/view/2374.html</a></p><p>可以看到<code>foo</code>和<code>_bar</code>的变量属性均未被修改，但是<code>__baz</code>被修改为<code>_Test__baz</code>,为了保护变量不被子类覆盖</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ManglingTest</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.__mangled = <span class="string">&#x27;hello&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_mangled</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.__mangled</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ManglingTest().get_mangled()</span><br><span class="line"><span class="string">&#x27;hello&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ManglingTest().__mangled</span><br><span class="line">AttributeError: <span class="string">&quot;&#x27;ManglingTest&#x27; object has no attribute &#x27;__mangled&#x27;&quot;</span></span><br></pre></td></tr></table></figure><p>· 领先和落后双下划线： <code>__var__</code></p><p>Python中存在一些特殊的方法，有些方法以双下划线<code>__</code>开头和结尾，它们是Python的魔法函数，比如<code>__init__()</code>和<code>__str__</code>等等。<strong>不用要这种方式命名自己的变量或者函数</strong></p><hr><p>魔法函数是指类内部以双下划线开头，并且以双下划线结尾的函数，在特定时刻，Python会自动调用这些函数</p><hr><p><a href="https://www.cnblogs.com/chenhuabin/p/13752770.html#_label0">https://www.cnblogs.com/chenhuabin/p/13752770.html#_label0</a></p><p>· 单下划线： <code>_</code></p><p>一个单独的下划线有时用作一个名称，表示一个变量是临时的或是不重要的</p><h4 id="filter、map、reduce、apply"><a href="#filter、map、reduce、apply" class="headerlink" title="filter、map、reduce、apply"></a>filter、map、reduce、apply</h4><ul><li>filter(function，sequence)</li></ul><p><strong>过滤掉序列中不符合函数条件的元素</strong>,返回迭代器,需要list转列表等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x:x%<span class="number">2</span>==<span class="number">0</span>,x))) <span class="comment"># 找出偶数。python3.*之后filter函数返回的不再是列表而是迭代器，所以需要用list转换。</span></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">[<span class="number">2</span>, <span class="number">4</span>]</span><br></pre></td></tr></table></figure><ul><li>map(function,iterable1,iterable2)</li></ul><p><strong>求一个序列或者多个序列进行函数映射之后的值</strong>，就该想到map这个函数,返回迭代器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">y = [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x,y:(x*y)+<span class="number">2</span>,x,y)))</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">[<span class="number">4</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">22</span>, <span class="number">32</span>]</span><br></pre></td></tr></table></figure><ul><li>reduce（function，iterable）</li></ul><p><strong>对一个序列进行压缩运算，得到一个值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line">arr = [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">reduce(<span class="keyword">lambda</span> x,y: x + y,arr) <span class="comment"># 直接返回一个值</span></span><br><span class="line"><span class="comment"># 20</span></span><br></pre></td></tr></table></figure><p>其计算原理：<br>先计算头两个元素：f(2, 3)，结果为5；<br>再把结果和第3个元素计算：f(5, 4)，结果为9；<br>再把结果和第4个元素计算：f(9, 5)，结果为14；<br>再把结果和第5个元素计算：f(14, 6)，结果为20；<br>由于没有更多的元素了，计算结束，返回结果20。</p><ul><li>apply(function,axis)</li></ul><p>pandas中的函数，eg:   <code>data.apply(lambda x:x*10)</code></p><hr><p>filter和map都是python内置的函数，可以直接调用，reduce在functools模块，apply在pandas模块</p><hr><h4 id="yield"><a href="#yield" class="headerlink" title="yield"></a>yield</h4><p>带有 yield 的函数在 Python 中被称之为 generator（生成器）</p><p>博客：<a href="https://blog.csdn.net/mieleizhi0522/article/details/82142856">https://blog.csdn.net/mieleizhi0522/article/details/82142856</a></p><h4 id="python数值"><a href="#python数值" class="headerlink" title="python数值"></a>python数值</h4><ul><li><strong>整型(int)</strong> - 通常被称为是整型或整数，是正或负整数，不带小数点。Python3 整型是没有限制大小的，可以当作 Long 类型使用，所以 Python3 没有 Python2 的 Long 类型。布尔(bool)是整型的子类型。</li><li><strong>浮点型(float)</strong> - 浮点型由整数部分与小数部分组成，浮点型也可以使用科学计数法表示（2.5e2 &#x3D; 2.5 x 102 &#x3D; 250）</li><li><strong>复数( (complex))</strong> - 复数由实数部分和虚数部分构成，可以用a + bj,或者complex(a,b)表示， 复数的实部a和虚部b都是浮点型。</li></ul><h4 id="list-append-无返回值"><a href="#list-append-无返回值" class="headerlink" title="list.append()无返回值"></a>list.append()无返回值</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>case1 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(case1.append(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># list.append没有返回值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 还有clear, insert, sort, reverse, remove, extend</span></span><br></pre></td></tr></table></figure><p>所以<code>case = case.append(1)</code>这样的操作不可行</p><h4 id="列表是可变的"><a href="#列表是可变的" class="headerlink" title="列表是可变的"></a>列表是可变的</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> = [<span class="number">9</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">case</span>:</span><br><span class="line">   <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">     <span class="keyword">case</span>.remove(i)</span><br><span class="line">     </span><br><span class="line"><span class="built_in">print</span>(<span class="keyword">case</span>)</span><br><span class="line"></span><br><span class="line">&gt;&gt; [<span class="number">9</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>因为列表是可变对象，当第一个8被删除后，第二个8补上了前面的位置，自然而然就被跳过了</p><p>修改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> = [<span class="number">9</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">case1 = [x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="keyword">case</span> <span class="keyword">if</span> x%<span class="number">2</span> != <span class="number">0</span>]</span><br></pre></td></tr></table></figure><h4 id="字符串常量用空格连接"><a href="#字符串常量用空格连接" class="headerlink" title="字符串常量用空格连接"></a>字符串常量用空格连接</h4><p>表示字符串合并</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> = <span class="string">&#x27;a&#x27;</span> <span class="string">&#x27;b&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="keyword">case</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ab</span></span><br></pre></td></tr></table></figure><h4 id="tuple只有一个元素要在末尾加逗号"><a href="#tuple只有一个元素要在末尾加逗号" class="headerlink" title="tuple只有一个元素要在末尾加逗号"></a>tuple只有一个元素要在末尾加逗号</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">isinstance</span>((<span class="string">&#x27;bilibili&#x27;</span>), <span class="built_in">tuple</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">isinstance</span>((<span class="string">&#x27;bilibili&#x27;</span>,), <span class="built_in">tuple</span>)</span><br><span class="line"></span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"></span><br><span class="line">a = (<span class="string">&#x27;bilibili&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line">b</span><br><span class="line">i</span><br><span class="line">l</span><br><span class="line">i</span><br><span class="line">b</span><br><span class="line">i</span><br><span class="line">l</span><br><span class="line">i</span><br><span class="line"></span><br><span class="line">b = (<span class="string">&#x27;bilibli&#x27;</span>,)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> b:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line">bilibli</span><br></pre></td></tr></table></figure><h4 id="if-else表达式优先级高于逗号"><a href="#if-else表达式优先级高于逗号" class="headerlink" title="if-else表达式优先级高于逗号"></a>if-else表达式优先级高于逗号</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x, y = (<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">if</span> <span class="literal">True</span> <span class="keyword">else</span> <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure><h4 id="使用enumerate遍历："><a href="#使用enumerate遍历：" class="headerlink" title="使用enumerate遍历："></a>使用enumerate遍历：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line"><span class="keyword">for</span> idx, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">    <span class="keyword">if</span> num % <span class="number">2</span>:</span><br><span class="line">        data[idx] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure><h4 id="字典的get方法"><a href="#字典的get方法" class="headerlink" title="字典的get方法"></a>字典的get方法</h4><p>在工程文件中经常会注意到使用get方法，避免因键不存在而引起的程序崩溃，若索引不到，将返回在第二个位置定义的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = &#123;<span class="string">&quot;name&quot;</span> : <span class="string">&quot;sds&quot;</span>, <span class="string">&quot;age&quot;</span> : <span class="string">&quot;18&quot;</span>&#125;</span><br><span class="line">uid = data.get(<span class="string">&quot;uid&quot;</span>, <span class="string">&quot;6688&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(uid)</span><br><span class="line"></span><br><span class="line"><span class="number">6688</span></span><br></pre></td></tr></table></figure><h4 id="f-string新格式化方法"><a href="#f-string新格式化方法" class="headerlink" title="f-string新格式化方法"></a>f-string新格式化方法</h4><p>Python3.6开始支持的新的格式化操作，相比以前更简洁方便。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">9</span></span><br><span class="line">data = <span class="string">f&quot;<span class="subst">&#123;i&#125;</span> * <span class="subst">&#123;i&#125;</span> = <span class="subst">&#123;i * i&#125;</span>&quot;</span></span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">9</span> * <span class="number">9</span> = <span class="number">81</span></span><br></pre></td></tr></table></figure><p>我们也常常这样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">&#x27;xhm&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;hello,&#x27;</span>+name+<span class="string">&#x27;!&#x27;</span>)</span><br></pre></td></tr></table></figure><p>改为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;hello,<span class="subst">&#123;name&#125;</span>!&#x27;</span>)</span><br><span class="line"><span class="comment"># hello,xhm!</span></span><br></pre></td></tr></table></figure><h4 id="合并两个字典"><a href="#合并两个字典" class="headerlink" title="合并两个字典"></a>合并两个字典</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data_1 = &#123;<span class="string">&quot;name&quot;</span> : <span class="string">&quot;sds&quot;</span>, <span class="string">&quot;age&quot;</span> : <span class="string">&quot;18&quot;</span>&#125;</span><br><span class="line">data_2 = &#123;<span class="string">&quot;name&quot;</span> : <span class="string">&quot;sds&quot;</span>, <span class="string">&quot;uid&quot;</span> : <span class="string">&quot;6688&quot;</span>&#125;</span><br><span class="line">out_data = &#123;**data_1, **data_2&#125;</span><br><span class="line"><span class="built_in">print</span>(out_data)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;sds&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="string">&#x27;18&#x27;</span>, <span class="string">&#x27;uid&#x27;</span>: <span class="string">&#x27;6688&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><h4 id="判断某对象是否为某些值"><a href="#判断某对象是否为某些值" class="headerlink" title="判断某对象是否为某些值"></a>判断某对象是否为某些值</h4><p>如果需要在if中将某对象与多个其他对象进行对比判断，你可能会进行如下定义</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = <span class="string">&quot;a&quot;</span></span><br><span class="line"><span class="keyword">if</span> data == <span class="string">&quot;a&quot;</span> <span class="keyword">or</span> data == <span class="string">&quot;b&quot;</span> <span class="keyword">or</span> data == <span class="string">&quot;c&quot;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;HHH&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>HHH</span><br></pre></td></tr></table></figure><p>简化修改为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">datas = [<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>]</span><br><span class="line">data = <span class="string">&quot;a&quot;</span></span><br><span class="line"><span class="keyword">if</span> data <span class="keyword">in</span> datas:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;HHH&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>HHH</span><br></pre></td></tr></table></figure><h4 id="注意-和-的区别"><a href="#注意-和-的区别" class="headerlink" title="注意^和**的区别"></a>注意^和**的区别</h4><p>按位异或  和   次幂</p><h4 id="写文件时请用with"><a href="#写文件时请用with" class="headerlink" title="写文件时请用with"></a>写文件时请用with</h4><p>当写入出错时会报错，文件会关闭</p><h4 id="数字中的下划线"><a href="#数字中的下划线" class="headerlink" title="数字中的下划线"></a>数字中的下划线</h4><p><code>x=10000000</code>和<code>x=10_000_000</code>等价，但是后者明显更加清晰</p><h4 id="没穿衣服的元组"><a href="#没穿衣服的元组" class="headerlink" title="没穿衣服的元组"></a>没穿衣服的元组</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">1</span>,<span class="number">2</span></span><br><span class="line">d1 = x[<span class="number">0</span>]</span><br><span class="line">d2 = x[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h4 id="使用isinstance代替-x3D-x3D-号检查类型"><a href="#使用isinstance代替-x3D-x3D-号检查类型" class="headerlink" title="使用isinstance代替&#x3D;&#x3D;号检查类型"></a>使用isinstance代替&#x3D;&#x3D;号检查类型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">type</span>(name) == <span class="built_in">tuple</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">isinstance</span>(name, <span class="built_in">tuple</span>)</span><br></pre></td></tr></table></figure><h4 id="b-a-x3D-a-b快速值交换"><a href="#b-a-x3D-a-b快速值交换" class="headerlink" title="b,a  &#x3D; a,b快速值交换"></a>b,a  &#x3D; a,b快速值交换</h4>]]></content>
      
      
      
        <tags>
            
            <tag> python小知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python三剑客及一些常用函数</title>
      <link href="/2023/01/04/python%E4%B8%89%E5%89%91%E5%AE%A2%E5%8F%8A%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/"/>
      <url>/2023/01/04/python%E4%B8%89%E5%89%91%E5%AE%A2%E5%8F%8A%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h3><h4 id="str"><a href="#str" class="headerlink" title="str"></a>str</h4><ol><li>string.count(str, beg&#x3D;0, end&#x3D;len(string))    返回 str 在 string 里面出现的次数，如果 beg 或者 end 指定则返回指定范围内 str 出现的次数</li><li>string.find(str, beg&#x3D;0, end&#x3D;len(string))       检测str是否包含在string中，如果beg和end指定范围，则检查是否包含在指定范围内，如果是返回开始的索引值，否则返回-1</li><li>string.format()</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="string">&quot;&#123;&#125; &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot;hello&quot;</span>, <span class="string">&quot;world&quot;</span>)    <span class="comment"># 不设置指定位置，按默认顺序</span></span><br><span class="line"><span class="string">&#x27;hello world&#x27;</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&quot;&#123;0&#125; &#123;1&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot;hello&quot;</span>, <span class="string">&quot;world&quot;</span>)  <span class="comment"># 设置指定位置</span></span><br><span class="line"><span class="string">&#x27;hello world&#x27;</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&quot;&#123;1&#125; &#123;0&#125; &#123;1&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot;hello&quot;</span>, <span class="string">&quot;world&quot;</span>)  <span class="comment"># 设置指定位置</span></span><br><span class="line"><span class="string">&#x27;world hello world&#x27;</span></span><br></pre></td></tr></table></figure><p>也可以设置参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;网站名：&#123;name&#125;, 地址 &#123;url&#125;&quot;</span>.<span class="built_in">format</span>(name=<span class="string">&quot;菜鸟教程&quot;</span>, url=<span class="string">&quot;www.runoob.com&quot;</span>))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 通过字典设置参数</span></span><br><span class="line">site = &#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;菜鸟教程&quot;</span>, <span class="string">&quot;url&quot;</span>: <span class="string">&quot;www.runoob.com&quot;</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;网站名：&#123;name&#125;, 地址 &#123;url&#125;&quot;</span>.<span class="built_in">format</span>(**site))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过列表索引设置参数</span></span><br><span class="line">my_list = [<span class="string">&#x27;菜鸟教程&#x27;</span>, <span class="string">&#x27;www.runoob.com&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;网站名：&#123;0[0]&#125;, 地址 &#123;0[1]&#125;&quot;</span>.<span class="built_in">format</span>(my_list))  <span class="comment"># &quot;0&quot; 是必须的</span></span><br></pre></td></tr></table></figure><p>也可以向 <strong>str.format()</strong> 传入对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AssignValue</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, value</span>):</span><br><span class="line">        self.value = value</span><br><span class="line">my_value = AssignValue(<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;value 为: &#123;0.value&#125;&#x27;</span>.<span class="built_in">format</span>(my_value))  <span class="comment"># &quot;0&quot; 是可选的</span></span><br></pre></td></tr></table></figure><p>格式化数字的方法</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230102114624443.png" alt="image-20230102114624443"></p><p><code>d</code> means expecting an int:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&quot;&#123;:d&#125;&quot;</span>.<span class="built_in">format</span>(<span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;3&#x27;</span></span><br></pre></td></tr></table></figure><p><code>2d</code> means formats to 2 characters using padding (whitespace by default)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&quot;&#123;:2d&#125;&quot;</span>.<span class="built_in">format</span>(<span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27; 3&#x27;</span></span><br></pre></td></tr></table></figure><p><code>0&gt;</code> means using <code>0</code> as padding, and right adjust the result:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&quot;&#123;:0&gt;2d&#125;&quot;</span>.<span class="built_in">format</span>(<span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;03&#x27;</span></span><br></pre></td></tr></table></figure><ol start="4"><li>string.join(seq)  以 string 作为分隔符，将 seq 中所有的元素(的字符串表示)<strong>合并为一个新的字符串</strong></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_seq = <span class="string">&#x27; &#x27;</span>.join(seq)</span><br></pre></td></tr></table></figure><ol start="5"><li>string.replace(str1, str2, num&#x3D;string.count(str1))   把string中<strong>str1替换为str2</strong>，如果num指定，则替换不超过num次</li><li>string.split(str&#x3D;””, num&#x3D;string.count(str))  以 str 为分隔符<strong>切片 string为列表</strong>，如果 num 有指定值，则仅分隔 <strong>num+1</strong> 个子字符串</li><li>string.strip([obj])    在 string 上执行 lstrip()和 rstrip()</li></ol><h4 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h4><ol><li>list(seq)     将元组转换成列表</li><li>list.append()    在列表末尾添加新的对象</li><li>list.count()     统计某个元素在列表中出现的次数</li><li>list.extend(seq) 在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）</li></ol><h4 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h4><p>tuple(iterable)   将可迭代系列转换为元组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>list1= [<span class="string">&#x27;Google&#x27;</span>, <span class="string">&#x27;Taobao&#x27;</span>, <span class="string">&#x27;Runoob&#x27;</span>, <span class="string">&#x27;Baidu&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tuple1=<span class="built_in">tuple</span>(list1)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tuple1</span><br><span class="line">(<span class="string">&#x27;Google&#x27;</span>, <span class="string">&#x27;Taobao&#x27;</span>, <span class="string">&#x27;Runoob&#x27;</span>, <span class="string">&#x27;Baidu&#x27;</span>)</span><br></pre></td></tr></table></figure><p>元组不变是指     <strong>元组所指向的内存中的内容不可变</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tup = (<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tup[<span class="number">0</span>] = <span class="string">&#x27;g&#x27;</span>     <span class="comment"># 不支持修改元素</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">TypeError: <span class="string">&#x27;tuple&#x27;</span> <span class="built_in">object</span> does <span class="keyword">not</span> support item assignment</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">id</span>(tup)     <span class="comment"># 查看内存地址</span></span><br><span class="line"><span class="number">4440687904</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tup = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">id</span>(tup)</span><br><span class="line"><span class="number">4441088800</span>    <span class="comment"># 内存地址不一样了</span></span><br></pre></td></tr></table></figure><h4 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">d=&#123;<span class="number">1</span>:<span class="string">&quot;a&quot;</span>,<span class="number">2</span>:<span class="string">&quot;b&quot;</span>,<span class="number">3</span>:<span class="string">&quot;c&quot;</span>&#125;</span><br><span class="line">result=[]</span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> d.items():</span><br><span class="line">    result.append(k)</span><br><span class="line">    result.append(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line">&gt;&gt; [<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>, <span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>, <span class="number">3</span>, <span class="string">&#x27;c&#x27;</span>]</span><br></pre></td></tr></table></figure><h4 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h4><p>集合运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data_1 = &#123;<span class="string">&#x27;Mathematics&#x27;</span>, <span class="string">&#x27;Chinese&#x27;</span>, <span class="string">&#x27;English&#x27;</span>, <span class="string">&#x27;Physics&#x27;</span>, <span class="string">&#x27;Chemistry&#x27;</span>, <span class="string">&#x27;Biology&#x27;</span>&#125;</span><br><span class="line">data_2 = &#123;<span class="string">&#x27;Mathematics&#x27;</span>, <span class="string">&#x27;Chinese&#x27;</span>, <span class="string">&#x27;English&#x27;</span>, <span class="string">&#x27;Politics&#x27;</span>, <span class="string">&#x27;Geography&#x27;</span>, <span class="string">&#x27;History&#x27;</span>&#125;</span><br><span class="line"><span class="comment"># 交集</span></span><br><span class="line">data_1 &amp; data_2</span><br><span class="line"><span class="comment"># 并集</span></span><br><span class="line">data_1 | data_2</span><br><span class="line"><span class="comment"># 差集</span></span><br><span class="line">data_1 - data_2</span><br><span class="line"><span class="comment"># 异或（不同时包含于两集合中的数据）</span></span><br><span class="line">data_1 ^ data_2</span><br></pre></td></tr></table></figure><h4 id="sorted"><a href="#sorted" class="headerlink" title="sorted()"></a>sorted()</h4><p>如果你需要对可迭代对象进行排序，比如列表、元组、字典，首先以列表为例子，可以直接使用内置函数sorted完成任务</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = [-<span class="number">1</span>, -<span class="number">10</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">5</span>]</span><br><span class="line">new_data = <span class="built_in">sorted</span>(data)</span><br><span class="line"><span class="comment"># new_data = sorted(data, reverse=True)降序</span></span><br><span class="line"><span class="built_in">print</span>(new_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [-10, -1, 0, 5, 9]</span></span><br></pre></td></tr></table></figure><p>对元组使用之后输出类型会变成列表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = (-<span class="number">1</span>, -<span class="number">10</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">5</span>)</span><br><span class="line">new_data = <span class="built_in">sorted</span>(data, reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(new_data)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="number">9</span>, <span class="number">5</span>, <span class="number">0</span>, -<span class="number">1</span>, -<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p>字典</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = [</span><br><span class="line">    &#123;<span class="string">&quot;name&quot;</span> : <span class="string">&quot;jia&quot;</span>, <span class="string">&quot;age&quot;</span> : <span class="number">18</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;name&quot;</span> : <span class="string">&quot;yi&quot;</span>, <span class="string">&quot;age&quot;</span> : <span class="number">60</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;name&quot;</span> : <span class="string">&quot;bing&quot;</span>, <span class="string">&quot;age&quot;</span> : <span class="number">20</span>&#125;</span><br><span class="line">]</span><br><span class="line">new_data = <span class="built_in">sorted</span>(data, key=<span class="keyword">lambda</span> x: x[<span class="string">&quot;age&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(new_data)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;jia&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">18</span>&#125;, &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;bing&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>&#125;, &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;yi&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">60</span>&#125;]</span><br></pre></td></tr></table></figure><h3 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h3><h4 id="get-dummies"><a href="#get-dummies" class="headerlink" title="get_dummies()"></a>get_dummies()</h4><p>pandas实现one hot encode的方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pandas.get_dummies(data, prefix=<span class="literal">None</span>, prefix_sep=<span class="string">&#x27;_&#x27;</span>, dummy_na=<span class="literal">False</span>, columns=<span class="literal">None</span>, sparse=<span class="literal">False</span>, drop_first=<span class="literal">False</span>)[source]</span><br></pre></td></tr></table></figure><p>例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame([</span><br><span class="line">    [<span class="string">&#x27;green&#x27;</span>, <span class="string">&#x27;A&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;B&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;A&#x27;</span>]</span><br><span class="line">])</span><br><span class="line">df.columns = [<span class="string">&#x27;color&#x27;</span>, <span class="string">&#x27;class&#x27;</span>]</span><br><span class="line">df = pd.get_dummies(df)</span><br></pre></td></tr></table></figure><p>get_dummies前</p><table><thead><tr><th></th><th>color</th><th>class</th></tr></thead><tbody><tr><td>0</td><td>green</td><td>A</td></tr><tr><td>1</td><td>red</td><td>B</td></tr><tr><td>2</td><td>blue</td><td>A</td></tr></tbody></table><p>get_dummies后</p><table><thead><tr><th align="left"></th><th align="center">color_blue</th><th align="center">color_green</th><th align="center">color_red</th><th align="center">color_A</th><th align="center">color_B</th></tr></thead><tbody><tr><td align="left">0</td><td align="center">0</td><td align="center">1</td><td align="center">0</td><td align="center">1</td><td align="center">0</td></tr><tr><td align="left">1</td><td align="center">0</td><td align="center">0</td><td align="center">1</td><td align="center">0</td><td align="center">1</td></tr><tr><td align="left">2</td><td align="center">1</td><td align="center">0</td><td align="center">0</td><td align="center">1</td><td align="center">0</td></tr></tbody></table><p>可以对指定列进行get_dummies</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.get_dummies(df.color)</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>blue</th><th>green</th><th>red</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>1</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><td>2</td><td>1</td><td>0</td><td>0</td></tr></tbody></table><p>将指定列进行get_dummies 后合并到元数据中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.join(pd.get_dummies(df.color))</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>color</th><th>class</th><th>blue</th><th>green</th><th>red</th></tr></thead><tbody><tr><td>0</td><td>green</td><td>A</td><td>0</td><td>1</td><td>0</td></tr><tr><td>1</td><td>red</td><td>B</td><td>0</td><td>0</td><td>1</td></tr><tr><td>2</td><td>blue</td><td>A</td><td>1</td><td>0</td><td>0</td></tr></tbody></table><h4 id="concat"><a href="#concat" class="headerlink" title="concat()"></a>concat()</h4><p><strong>连接内容 objs</strong></p><p>需要连接的数据，可以是多个 DataFrame 或者 Series。必传参数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Series 或 DataFrame 对象的序列或映射</span><br><span class="line">s1 = pd.Series([&#x27;a&#x27;, &#x27;b&#x27;])</span><br><span class="line">s2 = pd.Series([&#x27;c&#x27;, &#x27;d&#x27;])</span><br><span class="line">pd.concat([s1, s2])</span><br><span class="line"></span><br><span class="line"># df</span><br><span class="line">df1 = pd.DataFrame([[&#x27;a&#x27;, 1], [&#x27;b&#x27;, 2]], columns=[&#x27;letter&#x27;, &#x27;number&#x27;])</span><br><span class="line">df2 = pd.DataFrame([[&#x27;c&#x27;, 3], [&#x27;d&#x27;, 4]], columns=[&#x27;letter&#x27;, &#x27;number&#x27;])</span><br><span class="line">pd.concat([df1, df2])</span><br></pre></td></tr></table></figure><p><strong>轴方向 axis</strong></p><p>连接轴的方法，默认是 0，按行连接，追加在行后边，为 1 时追加到列后边。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># &#123;0/’index’, 1/’columns’&#125;, default 0</span><br><span class="line">pd.concat([df1, df4], axis=1) # 按列</span><br></pre></td></tr></table></figure><p><strong>合并方式 join</strong></p><p>其他轴上的数据是按交集（inner）还是并集（outer）进行合并。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># &#123;‘inner’, ‘outer’&#125;, default ‘outer’</span><br><span class="line">pd.concat([df1, df3], join=&quot;inner&quot;) # 按交集</span><br></pre></td></tr></table></figure><p><strong>保留索引 ignore_index</strong></p><p>是否保留原表索引，默认保留，为 True 会自动增加自然索引。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># bool, default False</span><br><span class="line">pd.concat([df1, df3], ignore_index=True) # 不保留索引</span><br></pre></td></tr></table></figure><h4 id="dropna"><a href="#dropna" class="headerlink" title="dropna()"></a>dropna()</h4><p>DataFrame.dropna(axis&#x3D;0, how&#x3D;’any’, thresh&#x3D;None, subset&#x3D;None, inplace&#x3D;False)</p><p>axis:</p><ul><li>axis&#x3D;0: 删除包含缺失值的行</li><li>axis&#x3D;1: 删除包含缺失值的列</li></ul><p>how: 与axis配合使用</p><ul><li>how&#x3D;‘any’ :只要有缺失值出现，就删除该行货列</li><li>how&#x3D;‘all’: 所有的值都缺失，才删除行或列</li></ul><p>thresh： axis中至少有thresh个非缺失值，否则删除<br>比如 axis&#x3D;0，thresh&#x3D;10：标识如果该行中非缺失值的数量小于10，将删除改行</p><p>subset: list<br>在哪些列中查看是否有缺失值</p><p>inplace: 是否在原数据上操作。如果为真，返回None否则返回新的copy，去掉了缺失值</p><h4 id="drop"><a href="#drop" class="headerlink" title="drop()"></a>drop()</h4><p>DataFrame.drop(labels&#x3D;None, axis&#x3D;0, index&#x3D;None, columns&#x3D;None, level&#x3D;None, inplace&#x3D;False, errors&#x3D;’raise’)</p><ul><li>labels: 要删除行或列的列表</li><li>axis: 0 行 ；1 列</li></ul><h4 id="fillna"><a href="#fillna" class="headerlink" title="fillna()"></a>fillna()</h4><p>DataFrame.fillna(value&#x3D;None, method&#x3D;None, axis&#x3D;None, inplace&#x3D;False, limit&#x3D;None, downcast&#x3D;None, **kwargs)</p><ul><li><p>value: scalar, dict, Series, or DataFrame<br>dict 可以指定每一行或列用什么值填充</p></li><li><p>method： {‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}, default None<br>在列上操作</p><ul><li>ffill &#x2F; pad: 使用前一个值来填充缺失值</li><li>backfill &#x2F; bfill :使用后一个值来填充缺失值</li></ul></li><li><p>limit 填充的缺失值个数限制。应该不怎么用</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用0代替所有的缺失值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.fillna(<span class="number">0</span>)</span><br><span class="line">    A   B   C   D</span><br><span class="line"><span class="number">0</span>   <span class="number">0.0</span> <span class="number">2.0</span> <span class="number">0.0</span> <span class="number">0</span></span><br><span class="line"><span class="number">1</span>   <span class="number">3.0</span> <span class="number">4.0</span> <span class="number">0.0</span> <span class="number">1</span></span><br><span class="line"><span class="number">2</span>   <span class="number">0.0</span> <span class="number">0.0</span> <span class="number">0.0</span> <span class="number">5</span></span><br><span class="line"><span class="number">3</span>   <span class="number">0.0</span> <span class="number">3.0</span> <span class="number">0.0</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用后边或前边的值填充缺失值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.fillna(method=<span class="string">&#x27;ffill&#x27;</span>)</span><br><span class="line">    A   B   C   D</span><br><span class="line"><span class="number">0</span>   NaN <span class="number">2.0</span> NaN <span class="number">0</span></span><br><span class="line"><span class="number">1</span>   <span class="number">3.0</span> <span class="number">4.0</span> NaN <span class="number">1</span></span><br><span class="line"><span class="number">2</span>   <span class="number">3.0</span> <span class="number">4.0</span> NaN <span class="number">5</span></span><br><span class="line"><span class="number">3</span>   <span class="number">3.0</span> <span class="number">3.0</span> NaN <span class="number">4</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;df.fillna(method=<span class="string">&#x27;bfill&#x27;</span>)</span><br><span class="line">     ABCD</span><br><span class="line"><span class="number">0</span><span class="number">3.0</span><span class="number">2.0</span>NaN<span class="number">0</span></span><br><span class="line"><span class="number">1</span><span class="number">3.0</span><span class="number">4.0</span>NaN<span class="number">1</span></span><br><span class="line"><span class="number">2</span>NaN<span class="number">3.0</span>NaN<span class="number">5</span></span><br><span class="line"><span class="number">3</span>NaN<span class="number">3.0</span>NaN<span class="number">4</span></span><br></pre></td></tr></table></figure><h4 id="csv操作"><a href="#csv操作" class="headerlink" title="csv操作"></a>csv操作</h4><ol><li>读csv不要索引</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&quot;filename.csv&quot;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>,index_col=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><ol start="2"><li>写csv不要索引</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.to_csv(<span class="string">&quot;xxx.csv&quot;</span>,index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><ol start="3"><li>删除有空值的行</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1 = df.dropna(subset=[<span class="string">&#x27;列名&#x27;</span>])</span><br></pre></td></tr></table></figure><ol start="4"><li>先把带有时间的列转为date_time格式，再进行排序。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1[<span class="string">&#x27;time&#x27;</span>] = pd.to_datetime(df1[<span class="string">&#x27;time&#x27;</span>])</span><br><span class="line">df1.sort_values(<span class="string">&#x27;time&#x27;</span>, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>inplace代表是否更改数据，默认是False，要保存结果的话需要inplace&#x3D;True。</p><ol start="5"><li>增加一列并赋值</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;xxx number&#x27;</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure><ol start="6"><li>插入列</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1.insert(<span class="number">3</span>, <span class="string">&#x27;users number&#x27;</span>, df2[<span class="string">&#x27;users number&#x27;</span>])</span><br><span class="line">//df.insert(插入到哪一列, <span class="string">&#x27;列名&#x27;</span>, another_df[<span class="string">&#x27;需要被插入的那一列&#x27;</span>])</span><br></pre></td></tr></table></figure><ol start="7"><li>Pandas sample()用于从DataFrame中随机选择行和列。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrame.sample(n=<span class="literal">None</span>, frac=<span class="literal">None</span>, replace=<span class="literal">False</span>, weights=<span class="literal">None</span>, random_state=<span class="literal">None</span>, axis=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p> 参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">n：这是一个可选参数, 由整数值组成, 并定义生成的随机行数。</span><br><span class="line">frac：它也是一个可选参数, 由浮点值组成, 并返回浮点值*数据帧值的长度。不能与参数n一起使用。</span><br><span class="line">replace：由布尔值组成。如果为true, 则返回带有替换的样本。替换的默认值为false。</span><br><span class="line">权重：它也是一个可选参数, 由类似于<span class="built_in">str</span>或ndarray的参数组成。默认值”无”将导致相等的概率加权。</span><br><span class="line">如果正在通过系列赛；它将与索引上的目标对象对齐。在采样对象中找不到的权重索引值将被忽略, 而在采样对象中没有权重的索引值将被分配零权重。</span><br><span class="line">如果在轴= <span class="number">0</span>时正在传递DataFrame, 则返回<span class="number">0</span>。它将接受列的名称。</span><br><span class="line">如果权重是系列；然后, 权重必须与被采样轴的长度相同。</span><br><span class="line">如果权重不等于<span class="number">1</span>；它将被标准化为<span class="number">1</span>的总和。</span><br><span class="line">权重列中的缺失值被视为零。</span><br><span class="line">权重栏中不允许无穷大。</span><br><span class="line">random_state：它也是一个可选参数, 由整数或numpy.random.RandomState组成。如果值为<span class="built_in">int</span>, 则为随机数生成器或numpy RandomState对象设置种子。</span><br><span class="line">axis：它也是由整数或字符串值组成的可选参数。 <span class="number">0</span>或”行”和<span class="number">1</span>或”列”。</span><br></pre></td></tr></table></figure><ol start="8"><li>通过.fillna()填充空值。</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs = inputs.fillna(inputs.mean())</span><br></pre></td></tr></table></figure><h4 id="pandas按列遍历Dataframe"><a href="#pandas按列遍历Dataframe" class="headerlink" title="pandas按列遍历Dataframe"></a>pandas按列遍历Dataframe</h4><ul><li>iterrows(): 按行遍历，将DataFrame的每一行迭代为(index, Series)对，可以通过row[name]对元素进行访问。</li><li>itertuples(): 按行遍历，将DataFrame的每一行迭代为元祖，可以通过row[name]对元素进行访问，比iterrows()效率高。</li><li>iteritems():按列遍历，将DataFrame的每一列迭代为(列名, Series)对，可以通过row[index]对元素进行访问。</li></ul><p>示例数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">inp = [&#123;<span class="string">&#x27;c1&#x27;</span>:<span class="number">10</span>, <span class="string">&#x27;c2&#x27;</span>:<span class="number">100</span>&#125;, &#123;<span class="string">&#x27;c1&#x27;</span>:<span class="number">11</span>, <span class="string">&#x27;c2&#x27;</span>:<span class="number">110</span>&#125;, &#123;<span class="string">&#x27;c1&#x27;</span>:<span class="number">12</span>, <span class="string">&#x27;c2&#x27;</span>:<span class="number">123</span>&#125;]</span><br><span class="line">df = pd.DataFrame(inp)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190227143422984.png" alt="在这里插入图片描述"></p><p><strong>按行遍历iterrows():</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index, row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    <span class="built_in">print</span>(index) <span class="comment"># 输出每行的索引值</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190227143612870.png" alt="在这里插入图片描述"></p><p>row[‘name’]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于每一行，通过列名name访问对应的元素</span></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    <span class="built_in">print</span>(row[<span class="string">&#x27;c1&#x27;</span>], row[<span class="string">&#x27;c2&#x27;</span>]) <span class="comment"># 输出每一行</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190227143716567.png" alt="在这里插入图片描述"></p><p><strong>按行遍历itertuples():</strong><br>getattr(row, ‘name’)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df.itertuples():</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">getattr</span>(row, <span class="string">&#x27;c1&#x27;</span>), <span class="built_in">getattr</span>(row, <span class="string">&#x27;c2&#x27;</span>)) <span class="comment"># 输出每一行</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190227143835738.png" alt="在这里插入图片描述"></p><p><strong>按列遍历iteritems():</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index, row <span class="keyword">in</span> df.iteritems():</span><br><span class="line">    <span class="built_in">print</span>(index) <span class="comment"># 输出列名</span></span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>c1<br>c2</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df.iteritems():</span><br><span class="line">    <span class="built_in">print</span>(row[<span class="number">0</span>], row[<span class="number">1</span>], row[<span class="number">2</span>]) <span class="comment"># 输出各列</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190227144037269.png" alt="在这里插入图片描述"></p><h3 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h3><h4 id="axis理解"><a href="#axis理解" class="headerlink" title="axis理解"></a>axis理解</h4><p>参考： <a href="https://zhuanlan.zhihu.com/p/31275071">https://zhuanlan.zhihu.com/p/31275071</a></p><p>简单来说就是：</p><ul><li><strong>Axis就是数组层级</strong></li><li><strong>设axis&#x3D;i，则Numpy沿着第i个下标变化的方向进行操作</strong></li></ul><h4 id="reshape"><a href="#reshape" class="headerlink" title="reshape()"></a>reshape()</h4><p>重新定义矩阵的形状</p><p><strong>相当于pytorch中的view()</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">v1 = torch.<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">16</span>)</span><br><span class="line">v2 = v1.view(<span class="number">4</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>参数使用-1，<strong>view中一个参数定为-1，代表动态调整这个维度上的元素个数，以保证元素的总数不变</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">v1 = torch.<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">16</span>)</span><br><span class="line">v2 = v1.view(-<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 代码效果同上</span></span><br></pre></td></tr></table></figure><h4 id="np-r-和-np-c"><a href="#np-r-和-np-c" class="headerlink" title="np.r_ 和 np.c_"></a>np.r_ 和 np.c_</h4><p>np.r_是按列连接两个<a href="https://so.csdn.net/so/search?q=%E7%9F%A9%E9%98%B5&spm=1001.2101.3001.7020">矩阵</a>，就是把两矩阵上下相加，要求列数相等。</p><p>np.c_是按行连接两个矩阵，就是把两矩阵左右相加，要求行数相等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line">b=np.array([[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line"> </span><br><span class="line">&gt;&gt;a</span><br><span class="line">Out[<span class="number">4</span>]: </span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"> </span><br><span class="line">&gt;&gt;b</span><br><span class="line">Out[<span class="number">5</span>]: </span><br><span class="line">array([[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line"> </span><br><span class="line">c=np.c_[a,b]</span><br><span class="line"> </span><br><span class="line">&gt;&gt;c</span><br><span class="line">Out[<span class="number">7</span>]: </span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">       [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure><h4 id="eye"><a href="#eye" class="headerlink" title="eye()"></a>eye()</h4><p>numpy.eye(N,M&#x3D;None,k&#x3D;0,dtype&#x3D;&lt;class ‘float’&gt;,order&#x3D;’C)</p><p>返回的是一个二维2的数组(N,M)，对角线的地方为1，其余的地方为0.</p><p>参数介绍：</p><p>（1）N:int型，表示的是输出的行数</p><p>（2）M：int型，可选项，输出的列数，如果没有就默认为N</p><p>（3）k：int型，可选项，对角线的下标，默认为0表示的是主对角线，负数表示的是低对角，正数表示的是高对角。</p><p>（4）dtype：数据的类型，可选项，返回的数据的数据类型</p><p>（5）order：{‘C’，‘F’}，可选项，也就是输出的数组的形式是按照C语言的行优先’C’，还是按照Fortran形式的列优先‘F’存储在内存中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line">a=np.eye(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"> </span><br><span class="line">a=np.eye(<span class="number">4</span>,k=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"> </span><br><span class="line">a=np.eye(<span class="number">4</span>,k=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"> </span><br><span class="line">a=np.eye(<span class="number">4</span>,k=-<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">[[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]]</span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure><h5 id="深度学习高级用法"><a href="#深度学习高级用法" class="headerlink" title="深度学习高级用法"></a>深度学习高级用法</h5><p>将数组转化为 one-hot形式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">labels = np.array([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">0</span>],[<span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;label的大小:&#x27;</span>,labels.shape,<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#因为我们的类别是从0-2，所以这里是3个类</span></span><br><span class="line">a=np.eye(<span class="number">3</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;如果对应的类别号是1，那么转成one-hot的形式&quot;</span>,a,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"> </span><br><span class="line">a=np.eye(<span class="number">3</span>)[<span class="number">2</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;如果对应的类别号是2，那么转成one-hot的形式&quot;</span>,a,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"> </span><br><span class="line">a=np.eye(<span class="number">3</span>)[<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1转成one-hot的数组的第一个数字是：&quot;</span>,a,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#这里和上面的结果的区别，注意!!!</span></span><br><span class="line">a=np.eye(<span class="number">3</span>)[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>]]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;如果对应的类别号是1,2,0,1，那么转成one-hot的形式\n&quot;</span>,a)</span><br><span class="line"> </span><br><span class="line">res=np.eye(<span class="number">3</span>)[labels.reshape(-<span class="number">1</span>)]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;labels转成one-hot形式的结果：\n&quot;</span>,res,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;labels转化成one-hot后的大小：&quot;</span>,res.shape)</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">labels的大小： (<span class="number">4</span>, <span class="number">1</span>) </span><br><span class="line"> </span><br><span class="line">如果对应的类别号是<span class="number">1</span>，那么转成one-hot的形式 [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>] </span><br><span class="line"> </span><br><span class="line">如果对应的类别号是<span class="number">2</span>，那么转成one-hot的形式 [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>] </span><br><span class="line"> </span><br><span class="line"><span class="number">1</span>转成one-hot的数组的第一个数字是： <span class="number">0.0</span> </span><br><span class="line"> </span><br><span class="line">如果对应的类别号是<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>，那么转成one-hot的形式</span><br><span class="line"> [[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]]</span><br><span class="line">labels转成one-hot形式的结果：</span><br><span class="line"> [[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]] </span><br><span class="line"> </span><br><span class="line">labels转化成one-hot后的大小： (<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#注：</span></span><br><span class="line">label.reshape(-<span class="number">1</span>)</span><br><span class="line">--&gt;   array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>])变成了一维数组</span><br></pre></td></tr></table></figure><h4 id="identity"><a href="#identity" class="headerlink" title="identity()"></a>identity()</h4><p>与eye()的区别在于只能创建方阵</p><h4 id="np-linalg-norm求范数"><a href="#np-linalg-norm求范数" class="headerlink" title="np.linalg.norm求范数"></a>np.linalg.norm求范数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_norm=np.linalg.norm(x, <span class="built_in">ord</span>=<span class="literal">None</span>, axis=<span class="literal">None</span>, keepdims=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">6</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="comment">#默认参数ord=None，axis=None，keepdims=False</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;默认参数(矩阵整体元素平方和开根号，不保留矩阵二维特性)：&quot;</span>,np.linalg.norm(x)</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;矩阵整体元素平方和开根号，保留矩阵二维特性：&quot;</span>,np.linalg.norm(x,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;矩阵每个行向量求向量的2范数：&quot;</span>,np.linalg.norm(x,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;矩阵每个列向量求向量的2范数：&quot;</span>,np.linalg.norm(x,axis=<span class="number">0</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;矩阵1范数：&quot;</span>,np.linalg.norm(x,<span class="built_in">ord</span>=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;矩阵2范数：&quot;</span>,np.linalg.norm(x,<span class="built_in">ord</span>=<span class="number">2</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;矩阵∞范数：&quot;</span>,np.linalg.norm(x,<span class="built_in">ord</span>=np.inf,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;矩阵每个行向量求向量的1范数：&quot;</span>,np.linalg.norm(x,<span class="built_in">ord</span>=<span class="number">1</span>,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="squeeze"><a href="#squeeze" class="headerlink" title="squeeze()"></a>squeeze()</h4><p><strong>作用</strong>：从数组的形状中删除单维度条目，即把shape中为1的维度去掉,<strong>对非单维的维度不起作用</strong></p><p>np.squeeze(a, axis &#x3D; None)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1）a表示输入的数组；</span><br><span class="line">2）axis用于指定需要删除的维度，但是指定的维度必须为单维度，否则将会报错；</span><br><span class="line">3）axis的取值可为None 或 int 或 tuple of ints, 可选。若axis为空，则删除所有单维度的条目；</span><br><span class="line">4）返回值：数组</span><br><span class="line">5) 不会修改原数组；</span><br></pre></td></tr></table></figure><p>eg:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(10).reshape(1, 10)</span><br><span class="line"># array([[0,1,2,3,4,5,6,7,8,9]])</span><br><span class="line"></span><br><span class="line">a.shape</span><br><span class="line"># (1,10)</span><br><span class="line"></span><br><span class="line">b = np.squeeze(a)</span><br><span class="line"># array([0,1,2,3,4,5,6,7,8,9])</span><br><span class="line"></span><br><span class="line">b.shape</span><br><span class="line"># (10,)</span><br></pre></td></tr></table></figure><h4 id="dot"><a href="#dot" class="headerlink" title="dot()"></a>dot()</h4><p>向量点积    和     多维矩阵乘法</p><h4 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h4><p><strong>一维数组</strong></p><p>通过冒号分隔切片参数 start:stop:step 来进行切片操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = a[<span class="number">2</span>:<span class="number">7</span>:<span class="number">2</span>]   <span class="comment"># 从索引 2 开始到索引 7 停止，间隔为 2</span></span><br></pre></td></tr></table></figure><p>冒号 : 的解释：如果只放置一个参数，如 [2]，将返回与该索引相对应的单个元素。如果为 [2:]，表示从该索引开始以后的所有项都将被提取。如果使用了两个参数，如 [2:7]，那么则提取两个索引(不包括停止索引)之间的项。</p><p><strong>注意1：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">7</span>:]</span><br><span class="line">array([<span class="number">8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">7</span>]</span><br><span class="line"><span class="number">8</span></span><br></pre></td></tr></table></figure><p><strong>注意2：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(a[<span class="number">1</span>:<span class="number">3</span>])  <span class="comment">#从索引1开始，也就是第二个元素2，到索引3，不包括索引3</span></span><br><span class="line">[<span class="number">2</span> <span class="number">3</span>]</span><br></pre></td></tr></table></figure><p><strong>二维数组</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>]</span><br><span class="line">array([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>:]</span><br><span class="line">array([[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">      [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[:<span class="number">2</span>]</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">      [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>:<span class="number">2</span>]</span><br><span class="line">array([[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#进阶</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>,]</span><br><span class="line">array([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>:,]</span><br><span class="line">array([[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">      [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[:<span class="number">2</span>,]</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">      [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>:<span class="number">2</span>,]</span><br><span class="line">array([[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>总结：</p><hr><p>这是numpy的切片操作，一般结构如num[a:b,c:d]，分析时以逗号为分隔符，<br>逗号之前为要取的num行的下标范围(a到b-1)，逗号之后为要取的num列的下标范围(c到d-1)；<br>前面是行索引，后面是列索引。<br>如果是这种num[:b,c:d]，a的值未指定，那么a为最小值0；<br>如果是这种num[a:,c:d]，b的值未指定，那么b为最大值；c、d的情况同理可得。</p><hr><p>所以重点就是看逗号，没逗号，就是看行了，冒号呢，就看成一维数组的形式啦。那上面逗号后面没有树，也就是不对列操作咯。<br>当然也可以这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a[:<span class="number">2</span>:<span class="number">1</span>]</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure><p>首先没有逗号，那切片就是只看行了，这里的意思是，从0开始到2（2不算），间隔为1。</p><h3 id="结合matplotlib画图"><a href="#结合matplotlib画图" class="headerlink" title="结合matplotlib画图"></a>结合matplotlib画图</h3><p><code>%matplotlib inline</code> 可以在Ipython编译器里直接使用，功能是可以内嵌绘图，并且可以省略掉plt.show()这一步。</p><h4 id="pyplot库"><a href="#pyplot库" class="headerlink" title="pyplot库"></a>pyplot库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p>绘制直线</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">xpoints = np.array([<span class="number">0</span>, <span class="number">6</span>])</span><br><span class="line">ypoints = np.array([<span class="number">0</span>, <span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;TITLE&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x - label&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y - label&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(xpoints, ypoints)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.grid()        网格线</span></span><br><span class="line"><span class="comment"># plt.grid(axis=&#x27;x&#x27;)        设置y轴方向显示网格线</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>plt.plot()函数是绘制二维函数的最基本函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>plot(x, y)        <span class="comment"># 创建 y 中数据与 x 中对应值的二维线图，使用默认样式</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>plot(x, y, <span class="string">&#x27;bo&#x27;</span>)  <span class="comment"># 创建 y 中数据与 x 中对应值的二维线图，使用蓝色实心圈绘制</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>plot(y)           <span class="comment"># x 的值为 0..N-1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>plot(y, <span class="string">&#x27;r+&#x27;</span>)     <span class="comment"># 使用红色 + 号</span></span><br></pre></td></tr></table></figure><h4 id="绘制多图"><a href="#绘制多图" class="headerlink" title="绘制多图"></a>绘制多图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot 1:</span></span><br><span class="line">x = np.array([<span class="number">0</span>, <span class="number">6</span>])</span><br><span class="line">y = np.array([<span class="number">0</span>, <span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">&quot;plot 1&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot 2:</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">&quot;plot 2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot 3:</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">&quot;plot 3&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot 4:</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">&quot;plot 4&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.suptitle(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/%E4%B8%8B%E8%BD%BD.png" alt="plt"></p><p>注： <code>plt.subplot(2,2,1)</code> &lt;&#x3D;&gt; <code>plt.subplot(221)</code></p><h4 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">23</span>, <span class="number">18</span>])</span><br><span class="line">sizes = np.array([<span class="number">20</span>,<span class="number">50</span>,<span class="number">100</span>,<span class="number">200</span>,<span class="number">500</span>,<span class="number">1000</span>,<span class="number">60</span>,<span class="number">90</span>])</span><br><span class="line">plt.scatter(x, y, s=sizes)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://www.runoob.com/wp-content/uploads/2021/07/pl-scatter-5.png" alt="img"></p><h4 id="柱形图"><a href="#柱形图" class="headerlink" title="柱形图"></a>柱形图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="string">&quot;num-1&quot;</span>, <span class="string">&quot;num-2&quot;</span>, <span class="string">&quot;num-3&quot;</span>, <span class="string">&quot;num-4&quot;</span>])</span><br><span class="line">y = np.array([<span class="number">12</span>, <span class="number">22</span>, <span class="number">6</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">plt.bar(x,y)</span><br><span class="line"><span class="comment"># plt.barh(x,y)水平柱状图</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="饼图"><a href="#饼图" class="headerlink" title="饼图"></a>饼图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">y = np.array([<span class="number">35</span>, <span class="number">25</span>, <span class="number">25</span>, <span class="number">15</span>])</span><br><span class="line"></span><br><span class="line">plt.pie(y,</span><br><span class="line">        labels=[<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;C&#x27;</span>,<span class="string">&#x27;D&#x27;</span>], <span class="comment"># 设置饼图标签</span></span><br><span class="line">        colors=[<span class="string">&quot;#d5695d&quot;</span>, <span class="string">&quot;#5d8ca8&quot;</span>, <span class="string">&quot;#65a479&quot;</span>, <span class="string">&quot;#a564c9&quot;</span>], <span class="comment"># 设置饼图颜色</span></span><br><span class="line">       )</span><br><span class="line">plt.title(<span class="string">&quot;Pie Test&quot;</span>) <span class="comment"># 设置标题</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="三维图"><a href="#三维图" class="headerlink" title="三维图"></a>三维图</h4><p>最基本的三维图是线图与<a href="https://so.csdn.net/so/search?q=%E6%95%A3%E7%82%B9%E5%9B%BE&spm=1001.2101.3001.7020">散点图</a>，可以用<code>ax.plot3D</code>和<code>ax.scatter3D</code>函数来创建</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制三角螺旋线</span></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ax = plt.axes(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#三维线的数据</span></span><br><span class="line">zline = np.linspace(<span class="number">0</span>, <span class="number">15</span>, <span class="number">1000</span>)</span><br><span class="line">xline = np.sin(zline)</span><br><span class="line">yline = np.cos(zline)</span><br><span class="line">ax.plot3D(xline, yline, zline, <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 三维散点的数据</span></span><br><span class="line">zdata = <span class="number">15</span> * np.random.random(<span class="number">100</span>)</span><br><span class="line">xdata = np.sin(zdata) + <span class="number">0.1</span> * np.random.randn(<span class="number">100</span>)</span><br><span class="line">ydata = np.cos(zdata) + <span class="number">0.1</span> * np.random.randn(<span class="number">100</span>)</span><br><span class="line">ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap=<span class="string">&#x27;Greens&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230104104737680.png" alt="image-20230104104737680"></p><h4 id="热图"><a href="#热图" class="headerlink" title="热图"></a>热图</h4><p>imshow()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">plt.imshow(X)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230104164258648.png" alt="image-20230104164258648"></p><p>博客：<a href="https://blog.csdn.net/qq_21763381/article/details/100169288">https://blog.csdn.net/qq_21763381/article/details/100169288</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> python三剑客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>machine_code（1）</title>
      <link href="/2022/12/27/csapp-machine-code-md/"/>
      <url>/2022/12/27/csapp-machine-code-md/</url>
      
        <content type="html"><![CDATA[<h4 id="处理器模型发展"><a href="#处理器模型发展" class="headerlink" title="处理器模型发展"></a>处理器模型发展</h4><p><a href="https://hansimov.gitbook.io/csapp/part1/ch03-machine-level-representing-of-programs/3.1-a-historial-perspective">https://hansimov.gitbook.io/csapp/part1/ch03-machine-level-representing-of-programs/3.1-a-historial-perspective</a></p><h4 id="摩尔定律"><a href="#摩尔定律" class="headerlink" title="摩尔定律"></a>摩尔定律</h4><h4 id="芯片构造"><a href="#芯片构造" class="headerlink" title="芯片构造"></a>芯片构造</h4><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20221227205856257.png" alt="image-20221227205856257"></p><p>当时标准桌面型号有四个核心，服务器级别的机器有八个核心（上图）</p><p>芯片周围连接外围设备的接口：</p><ul><li>DDR是连接到主存的方式，即所谓的DRAM（Dynamic动态 RAM随机访问机）</li><li>PCI是一种同步的独立于处理器的32位或64位局部总线，主要用于连接显示卡、网卡、声卡</li><li>SATA是与不同类型盘连接</li><li>USB接口与USB设备连接</li><li>ethernet网络连接</li></ul><p>集成到芯片上的不止是处理器还有很多逻辑单元</p><h4 id="处理器架构"><a href="#处理器架构" class="headerlink" title="处理器架构"></a>处理器架构</h4><table><thead><tr><th><strong>架构</strong></th><th><strong>特点</strong></th><th><strong>代表性的厂商</strong></th><th><strong>运营机构</strong></th></tr></thead><tbody><tr><td><strong>X86</strong></td><td><strong>性能高，速度快，兼容性好</strong></td><td><strong>英特尔，AMD</strong></td><td><strong>英特尔</strong></td></tr><tr><td><strong>ARM</strong></td><td><strong>成本低，低功耗</strong></td><td><strong>苹果，谷歌，IBM，华为</strong></td><td><strong>英国ARM公司</strong></td></tr><tr><td><strong>RISC-V</strong></td><td><strong>模块化，极简，可拓展</strong></td><td><strong>三星，英伟达，西部数据</strong></td><td><strong>RISC-V基金会</strong></td></tr><tr><td><strong>MIPS</strong></td><td><strong>简洁，优化方便，高拓展性</strong></td><td><strong>龙芯</strong></td><td><strong>MIPS科技公司</strong></td></tr></tbody></table><p><strong>X86在PC上占据大部分份额，ARM在手机处理器上占据绝对份额</strong></p><h4 id="c代码运行的过程"><a href="#c代码运行的过程" class="headerlink" title="c代码运行的过程"></a>c代码运行的过程</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gcc -Og -S sum.c</span><br><span class="line"><span class="comment">// 将c代码转换为assembly代码</span></span><br></pre></td></tr></table></figure><p>-s为-stop停在把c转化为汇编的时刻</p><p>-Og是我希望编译器做什么样的优化的规范，这样才能读懂</p><p>具体过程：<a href="https://www.cnblogs.com/carpenterlee/p/5994681.html">https://www.cnblogs.com/carpenterlee/p/5994681.html</a></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objdump -d sum &gt; sum.d</span><br></pre></td></tr></table></figure><p>反汇编，sum.d的内容</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a.out:     file format elf64-x86<span class="number">-64</span></span><br><span class="line">     </span><br><span class="line">Disassembly of section .init:</span><br><span class="line"></span><br><span class="line"><span class="number">0000000000001000</span> &lt;_init&gt;:</span><br><span class="line">    <span class="number">1000</span>:   f3 <span class="number">0f</span> <span class="number">1</span>e fa             endbr64</span><br><span class="line">    <span class="number">1004</span>:   <span class="number">48</span> <span class="number">83</span> ec <span class="number">08</span>             sub    $<span class="number">0x8</span>,%rsp</span><br><span class="line">    <span class="number">1008</span>:   <span class="number">48</span> <span class="number">8b</span> <span class="number">05</span> d9 <span class="number">2f</span> <span class="number">00</span> <span class="number">00</span>    mov    <span class="number">0x2fd9</span>(%rip),%rax        # <span class="number">3f</span>e8 &lt;__gmon_start__&gt;</span><br><span class="line">    <span class="number">100f</span>:   <span class="number">48</span> <span class="number">85</span> c0                test   %rax,%rax</span><br><span class="line">    <span class="number">1012</span>:   <span class="number">74</span> <span class="number">02</span>                   je     <span class="number">1016</span> &lt;_init+<span class="number">0x16</span>&gt;</span><br><span class="line">    <span class="number">1014</span>:   ff d0                   callq  *%rax</span><br><span class="line">    <span class="number">1016</span>:   <span class="number">48</span> <span class="number">83</span> c4 <span class="number">08</span>             add    $<span class="number">0x8</span>,%rsp</span><br><span class="line">    <span class="number">101</span>a:   c3                      retq</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p><strong>反汇编程序无法访问源代码，甚至无法访问汇编代码，它只是通过实际目标代码文件中的字节来辨别出来的</strong></p><p>或者使用GDB</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; gdb</span><br><span class="line">...</span><br><span class="line">(gdb)</span><br><span class="line">(gdb) disassemble bitXor</span><br><span class="line">Dump of assembler code <span class="keyword">for</span> function bitXor:</span><br><span class="line">   <span class="number">0x0000000000001149</span> &lt;+<span class="number">0</span>&gt;:     endbr64</span><br><span class="line">   <span class="number">0x000000000000114d</span> &lt;+<span class="number">4</span>&gt;:     push   %rbp</span><br><span class="line">   <span class="number">0x000000000000114e</span> &lt;+<span class="number">5</span>&gt;:     mov    %rsp,%rbp</span><br><span class="line">   <span class="number">0x0000000000001151</span> &lt;+<span class="number">8</span>&gt;:     mov    %edi,<span class="number">-0x4</span>(%rbp)</span><br><span class="line">   <span class="number">0x0000000000001154</span> &lt;+<span class="number">11</span>&gt;:    mov    %esi,<span class="number">-0x8</span>(%rbp)</span><br><span class="line">   <span class="number">0x0000000000001157</span> &lt;+<span class="number">14</span>&gt;:    mov    <span class="number">-0x4</span>(%rbp),%eax</span><br><span class="line">   <span class="number">0x000000000000115a</span> &lt;+<span class="number">17</span>&gt;:    xor    <span class="number">-0x8</span>(%rbp),%eax</span><br><span class="line">   <span class="number">0x000000000000115d</span> &lt;+<span class="number">20</span>&gt;:    pop    %rbp</span><br><span class="line">   <span class="number">0x000000000000115e</span> &lt;+<span class="number">21</span>&gt;:    retq</span><br><span class="line">End of assembler dump.</span><br></pre></td></tr></table></figure><p><strong>此方法前面显示的是16进制地址而非像objdum那样的字节级编码</strong></p><h4 id="Assembly-Characteristics-Data-Types-汇编语言特性"><a href="#Assembly-Characteristics-Data-Types-汇编语言特性" class="headerlink" title="Assembly Characteristics: Data Types 汇编语言特性"></a>Assembly Characteristics: Data Types 汇编语言特性</h4><ul><li>“Integer” data type of 1,2,4,or 8 bytes，不区分unsigned和signed</li><li>floating point 有4,8,10,bytes</li><li>没有数组以及一些数据结构，只是内存中连续存储的单元</li></ul><h4 id="x86-64-Integer-Registers"><a href="#x86-64-Integer-Registers" class="headerlink" title="x86-64 Integer Registers"></a>x86-64 Integer Registers</h4><p>有16个寄存器</p><p><img src="https://img-blog.csdnimg.cn/20190723112517340.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p>注意到 %r代表62位操作 %e代表了32位的操作，%e版本只是%r实体的低32位。</p><p><strong>%rsp寄存器存的是栈指针，它能告诉你程序执行到哪儿了</strong></p><h4 id="移动数据"><a href="#移动数据" class="headerlink" title="移动数据"></a>移动数据</h4><h5 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">moveq Source Dest</span><br></pre></td></tr></table></figure><h5 id="操作数类型"><a href="#操作数类型" class="headerlink" title="操作数类型"></a>操作数类型</h5><p><img src="https://img-blog.csdnimg.cn/20190723113325695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><h5 id="操作数组合"><a href="#操作数组合" class="headerlink" title="操作数组合"></a>操作数组合</h5><p><img src="https://img-blog.csdnimg.cn/20190723113456800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><h4 id="理解Swap-函数"><a href="#理解Swap-函数" class="headerlink" title="理解Swap()函数"></a>理解Swap()函数</h4><p><img src="https://img-blog.csdnimg.cn/20190723125635539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p>使用x86-64的时候，函数参数总是出现在某些特定的寄存器中，**%rdi将是第一个参数寄存器，%rsi将是第二个参数寄存器**。最多可以有6个</p><h4 id="完整的内存地址模式"><a href="#完整的内存地址模式" class="headerlink" title="完整的内存地址模式"></a>完整的内存地址模式</h4><p><img src="https://img-blog.csdnimg.cn/20190723125841927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p>下面是内存完整模式的一个例子</p><p><img src="https://img-blog.csdnimg.cn/20190723130008460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><h4 id="地址计算"><a href="#地址计算" class="headerlink" title="地址计算"></a>地址计算</h4><p><img src="https://img-blog.csdnimg.cn/20190723131714576.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20221229121353910.png" alt="image-20221229121353910"></p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20221229121547636.png" alt="image-20221229121547636"></p><p><img src="https://img-blog.csdnimg.cn/20190723131745599.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><h3 id="control"><a href="#control" class="headerlink" title="control"></a>control</h3><h4 id="处理器状态-x86-64-Partial"><a href="#处理器状态-x86-64-Partial" class="headerlink" title="处理器状态 (x86-64, Partial)"></a>处理器状态 (x86-64, Partial)</h4><p><img src="https://img-blog.csdnimg.cn/20190724111040785.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p>图上的CF,ZF,SF,OF就是微机学过的状态位，其中各自代表的意思如下</p><p><img src="https://img-blog.csdnimg.cn/20190724111626651.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20190724112028471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20190724112037452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/2019072411205365.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20190724112113371.png" alt="img"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>csapp_CouseOverview</title>
      <link href="/2022/12/23/csapp-CouseOverview-md/"/>
      <url>/2022/12/23/csapp-CouseOverview-md/</url>
      
        <content type="html"><![CDATA[<hr><hr><p>写在前面; 今天是12.23，经历了新冠的抗争后开始学习csapp，教材课程及练习都使用CMU 15-213。希望能在这个寒假学习完毕</p><hr><h3 id="Ints-are-not-Interagers-Float-are-not-Reals"><a href="#Ints-are-not-Interagers-Float-are-not-Reals" class="headerlink" title="Ints are not Interagers, Float are not Reals"></a>Ints are not Interagers, Float are not Reals</h3><p>Example 1:</p><ul><li>Float’s : Yes!</li><li>Int’s:<ul><li>40000 * 40000 -&gt; 1600000000</li><li>50000 * 50000 -&gt; ??</li></ul></li></ul><p>Example 2:  Is (x + y) + z &#x3D; x + (y  + z )   ?</p><ul><li>Int’s:: Yes!</li><li>Float’s : Not Sure!!</li></ul><p>Float will throw the more number!</p><h3 id="Memory-Referencing-Bug-Example"><a href="#Memory-Referencing-Bug-Example" class="headerlink" title="Memory Referencing Bug Example"></a>Memory Referencing Bug Example</h3><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202212231500118.png" alt="image-20221223150035984"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202212231513136.png" alt="image-20221223151356105"></p><p><strong>For sure, this is influenced by your gcc version and IDE</strong>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> csapp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch深度学习总结</title>
      <link href="/2022/11/09/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93-md/"/>
      <url>/2022/11/09/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93-md/</url>
      
        <content type="html"><![CDATA[<h2 id="深度学习流程"><a href="#深度学习流程" class="headerlink" title="深度学习流程"></a>深度学习流程</h2><h3 id="流程简述"><a href="#流程简述" class="headerlink" title="流程简述"></a>流程简述</h3><pre class="mermaid">graph TD a[Build the dataset]-->b[preprocessing] b[preprocessing]-->c[training and validation] c-->d[Parameter tuning && optimization]</pre><h3 id="构建数据集："><a href="#构建数据集：" class="headerlink" title="构建数据集："></a>构建数据集：</h3><ol><li>流程</li></ol><p>first step: 收集数据：去大量抽样调查收集，爬虫（<a href="https://github.com/MrS0m30n3/youtube-dl-gui">youtube爬虫工具</a>），众包（<del>花钱找工具人</del>）等等</p><p>second: 数据格式整理</p><p>third: 导入代码进行处理</p><ol start="2"><li><p>code</p><p>一般处理csv或者json等格式的文本格式文件，下为举例</p><p>sklearn</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_json(<span class="string">&#x27;data.json&#x27;</span>)</span><br><span class="line"><span class="comment"># data = pd.read_csv(&#x27;data.csv&#x27;)</span></span><br></pre></td></tr></table></figure><p>pytorch</p><p>​pytorch的项目预处理的时候可以用pandas、json等库处理，之后生成新的文件在构建模型前构造DataSet和DataLoader时直接读取数据集来load</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = MyDataset(csv_file=<span class="string">&#x27;../data/data.csv&#x27;</span>,</span><br><span class="line">root_dir=<span class="string">&#x27;../data&#x27;</span>,</span><br><span class="line">transform=torchvision.transforms.ToTensor())</span><br></pre></td></tr></table></figure></li></ol><h3 id="数据集划分："><a href="#数据集划分：" class="headerlink" title="数据集划分："></a>数据集划分：</h3><ol><li>基本知识</li></ol><ul><li><p>训练集（Train Set）: 模型用于训练和调整模型参数</p></li><li><p>验证集（Validation Set）: 用来验证模型精度和调整</p></li><li><p>测试集（Test Set）: 验证模型的泛化能力</p></li></ul><blockquote><p>训练集和验证集有时候是从同一数据集中分开的，但是在划分验证集时需要注意验证集的分布需和测试集尽量保持一致，保证其泛化性</p></blockquote><p>几种划分方式：</p><ul><li><p>留出法（Hold-Out）：直接将训练集划分为新的训练集和验证集。优点简单。缺点只得到了一份验证集，有可能导致模型在验证集上过拟合，适用于数据量比较大点的情况。</p></li><li><p>交叉验证法（Cross Validation,CV）：将训练集划分成K份，将其中的K-1份作为训练集，剩余的1份作为验证集，循环K训练。这种划分方式是所有的训练集都是验证集，最终模型验证精度是K份平均得到。这种方式的优点是验证集精度比较可靠，训练K次可以得到K个有多样性差异的模型；CV验证的缺点是需要训练K次，不适合数据量很大的情况。</p></li><li><p>自助采样法（BootStrap）：通过有放回的采样方式得到新的训练集和验证集，每次的训练集和验证集都是有区别的。这种划分方式一般适用于数据量较小的情况。</p></li></ul><ol start="2"><li><p>code: </p><p>sklearn</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line">os.chdir(<span class="string">&#x27;breast_cancer_data&#x27;</span>)</span><br><span class="line">data = pd.read_csv(<span class="string">r&#x27;data.csv&#x27;</span>)</span><br><span class="line">data.drop(<span class="string">&#x27;Unnamed: 32&#x27;</span>,inplace = <span class="literal">True</span>,axis = <span class="number">1</span>)</span><br><span class="line">data.drop(<span class="string">&#x27;id&#x27;</span>,inplace = <span class="literal">True</span>,axis=<span class="number">1</span>)</span><br><span class="line">y = data[<span class="string">&#x27;diagnosis&#x27;</span>]</span><br><span class="line">x = data.drop(<span class="string">&#x27;diagnosis&#x27;</span>,axis = <span class="number">1</span>)</span><br><span class="line">model = RandomForestClassifier()</span><br></pre></td></tr></table></figure><p>留出法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">14x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><p>k折交叉验证：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line">kf = KFold(n_splits = <span class="number">10</span>)</span><br><span class="line">accuracy = []</span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> kf.split(x):</span><br><span class="line">     x_train, x_test = x.loc[train_index],x.loc[test_index]</span><br><span class="line">     y_train, y_test = y.loc[train_index],y.loc[test_index]</span><br><span class="line">     model.fit(x_train, y_train)</span><br><span class="line">     prediction = model.predict(x_test)</span><br><span class="line">     acc=metrics.accuracy_score(predocton, y_test)</span><br><span class="line">     accuracy,append(acc)</span><br><span class="line"><span class="built_in">print</span>(accuracy)</span><br><span class="line"><span class="built_in">print</span>(np.average(accuracy))</span><br></pre></td></tr></table></figure><p>pytorch:</p><p><strong>torch.utils.data.Subset</strong>或者<strong>random_split</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])</span><br></pre></td></tr></table></figure><p>或者自定义分类数据集</p><p>eg: 文本分类中可以根据文本数字先进行排序然后按照顺序每10个前9个放入训练集后1个放入测试集(若为9:1)，然后训练时再进行shuffle，这样保证了分布均匀的问题</p></li></ol><h3 id="模型训练和验证"><a href="#模型训练和验证" class="headerlink" title="模型训练和验证"></a>模型训练和验证</h3><ol><li><p>仔细检查数据：</p><p>花时间去检查数据是一件比较重要的工作。因为数据中往往可能存在异常值，而且了解它们的分布可以有利于我们找到一个更好的模型。同时也可以对数据进行一开始的手动调整。</p></li><li><p>搭建模型并开始训练验证</p><p>评估框架提示</p><ul><li>固定随机种子：始终使用固定的随机种子来确保两次运行代码时，您将获得相同的结果。</li><li>简化：去除不必要的一些操作</li><li>验证损失：验证损失是否从正确的损失值开始</li><li>设定一个好的初始化</li><li>人类基线：监控除损失之外的指标，这些指标是人类可以解释和检查的（例如准确性）。尽可能评估自己（人类）的准确性并与之进行比较。</li><li>可视化预测动态。训练过程中可视化固定测试批次上的模型预测对模型调整有很大帮助。</li></ul></li><li><p>过度拟合</p></li></ol><p>找到一个好的模型的方法有两个阶段：首先获得一个足够大的模型以使其可以过度拟合（即专注于训练损失），然后适当地对其进行正则化（放弃一些训练损失以提高验证损失）。</p><p>此阶段的一些提示和技巧：</p><ul><li>选择模型：为了减少训练损失，您需要为数据选择合适的体系结构。</li><li>Adam是安全的。在设定基准的早期阶段，我喜欢以3e-4的学习率使用Adam 。以我的经验，亚当更宽容超参数，包括不良的学习速度。对于ConvNets，调整良好的SGD几乎总是比Adam稍胜一筹，但是最佳学习率区域要狭窄得多且针对特定问题。</li><li>一次只使一个复杂化。如果您有多个信号要插入您的分类器，我建议您将它们一个接一个地插入，并每次确保获得预期的性能提升。</li><li>不要相信学习率衰减的默认值。如果您要重新使用其他领域的代码，请务必小心学习率。</li></ul><p><strong>4. 正则化</strong></p><p>此阶段的一些提示和技巧：</p><ul><li>获取更多数据</li><li>数据扩充</li><li>创意增强：如果半假数据没有做到这一点，伪造数据也可能会有所作为。人们正在寻找扩展数据集的创新方法。例如，领域随机化，模拟的使用，巧妙的混合，例如将（潜在模拟的）数据插入场景，甚至GAN。</li><li>使用预训练网络</li><li>坚持监督学习</li><li>减小输入维数</li><li>减小模型尺寸</li><li>减小批量大小</li><li>Dropout</li><li>提早停止训练。根据您测得的验证损失提前停止训练，以在模型快要过拟合的时候捕获模型。</li><li>尝试更大的模型。大型模型大多数最终会过拟合，但是它们的“早期停止”性能通常会比小型模型好得多。</li></ul><p><strong>5. 微调</strong></p><p>此阶段的一些提示和技巧：</p><ul><li>随机网格搜索</li><li>超参数优化</li></ul><p><strong>6. 进一步提高精确率</strong></p><ul><li>模型集成</li></ul><p>代码参考搭建过的一些项目</p>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>github fork仓库向主工程提交代码</title>
      <link href="/2022/09/22/github-fork%E4%BB%93%E5%BA%93%E5%90%91%E4%B8%BB%E5%B7%A5%E7%A8%8B%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%81/"/>
      <url>/2022/09/22/github-fork%E4%BB%93%E5%BA%93%E5%90%91%E4%B8%BB%E5%B7%A5%E7%A8%8B%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h2 id="1-fork并关联本地"><a href="#1-fork并关联本地" class="headerlink" title="1. fork并关联本地"></a>1. fork并关联本地</h2><p>进入我的主页，找到这个仓库</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209220208551.png" alt="image-20220922020848487"></p><p>点击右上角的fork，然后你的主页里就多了一个同样的仓库了，相当于做了一个镜像开了个分支</p><p>然后在本地合适位置（最好别带中文）建立一个同名文件夹（名字不影响，但是为了一致嘛），然后在文件夹中打开git bash(path配置好了的话，powershell也可以)，然后按照如下流程输入（有梯子的话最好打开梯子）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 克隆fork后仓库到本地,yourname为你的github名</span></span><br><span class="line">git <span class="built_in">clone</span> （fork后的url）</span><br></pre></td></tr></table></figure><p>然后你的文件夹下就会出现本项目已有所有文件，然后你就可以在本地仓库的对应文件夹（你的名字）添加你的学习文件了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add到本地暂存区, .是add所有新文件的意思</span></span><br><span class="line">git add .</span><br><span class="line"></span><br><span class="line"><span class="comment"># commit到本地仓库</span></span><br><span class="line">git commit -m <span class="string">&quot;first_commit&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 关联到你的远程仓库</span></span><br><span class="line">git remote add origin your_url</span><br><span class="line"></span><br><span class="line"><span class="comment"># push到你的远程仓库</span></span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209220214651.webp" alt="img"></p><p>然后你fork的仓库会出现你的新增文件</p><h2 id="2-关联主工程"><a href="#2-关联主工程" class="headerlink" title="2.关联主工程"></a>2.关联主工程</h2><p>关联主工程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote add okex(自定义分支名) (主工程的git url)</span><br><span class="line"><span class="comment"># 查看关联情况</span></span><br><span class="line">git remote -v</span><br></pre></td></tr></table></figure><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209220224884.png" alt="在这里插入图片描述"></p><p>拉取主工程各分支信息到本地：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git fetch okex(自定义分支名)</span><br></pre></td></tr></table></figure><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209220224876.png" alt="在这里插入图片描述"></p><p>在本地切换到主分支的某分支（比如develop）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout develop</span><br></pre></td></tr></table></figure><p>在此分支的基础上创建一个自己的分支：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b michael.w</span><br></pre></td></tr></table></figure><p>开始做代码修改。</p><p>代码commit后向自己的repo push代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push</span><br></pre></td></tr></table></figure><p>这里可能报错，请根据报错内容自行纠正</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209220224883.png" alt="在这里插入图片描述"></p><ol><li>从自己的repo中向主工程发起request pull：<br><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209220224879.png" alt="在这里插入图片描述"><br>选择要提交的目标分支：<br><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209220224980.png" alt="在这里插入图片描述"></li></ol><h3 id="如何将主分支的更新进度同步到我的repo中"><a href="#如何将主分支的更新进度同步到我的repo中" class="headerlink" title="如何将主分支的更新进度同步到我的repo中"></a>如何将主分支的更新进度同步到我的repo中</h3><p>假设主工程的开发分支时main</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">切到本地的main分支</span></span><br><span class="line">git checkout main</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将okex的的main分支拉取下来并与本地现在所处分支合并</span></span><br><span class="line">git pull okex main</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">推到我的repo</span></span><br><span class="line">git push</span><br></pre></td></tr></table></figure><hr><blockquote><p><strong>本文参考了<a href="https://blog.csdn.net/michael_wgy_/article/details/104589800">wgy的博客</a>，侵删</strong></p></blockquote><blockquote><p><strong>由于github默认分支改变，以上master记得改为main</strong></p></blockquote><hr>]]></content>
      
      
      
        <tags>
            
            <tag> github代码提交 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>github+hexo+butterfly搭建博客</title>
      <link href="/2022/09/21/github-hexo-butterfly%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/2022/09/21/github-hexo-butterfly%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<hr><p>突然想到搭建一个博客玩，其实之前也在csdn上发过一点，但是没坚持下来，太失败了</p><p>希望这次可以坚持下来，下面记录一下搭建过程</p><hr><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><ol><li><p>github账号</p></li><li><p>nodejs, npm（版本别太低）</p></li></ol><p>上网搜具体的安装教程，肯定比我写得好</p><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><h4 id="创建username-github-io的项目"><a href="#创建username-github-io的项目" class="headerlink" title="创建username.github.io的项目"></a>创建<strong>username.github.io</strong>的项目</h4><p>（记住<strong>username</strong>跟你<strong>github</strong>名称同名）</p><p>在合适的地方新建一个文件夹，用来存放自己的博客文件，我的放在<code>D:\blog</code>下</p><p><strong>在该目录下</strong></p><h4 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装<strong>Hexo</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-cli -g</span><br></pre></td></tr></table></figure><p>可能会有几个报错，忽略</p><p>安装完后用 <strong>hexo -v</strong> 验证是否安装成功</p><h4 id="初始化并生成网页"><a href="#初始化并生成网页" class="headerlink" title="初始化并生成网页"></a><strong>初始化</strong>并生成网页</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br><span class="line"></span><br><span class="line">npm install <span class="comment"># 安装必备组件</span></span><br><span class="line"></span><br><span class="line">hexo g <span class="comment"># 生成静态网页</span></span><br><span class="line"></span><br><span class="line">hexo s <span class="comment"># 打开本地服务器,打开http://localhost:4000/,就有效果了</span></span><br></pre></td></tr></table></figure><p><strong>ctrl</strong> + <strong>c</strong>关闭本地服务器</p><h4 id="连接github和本地"><a href="#连接github和本地" class="headerlink" title="连接github和本地"></a>连接github和本地</h4><p>在根目录下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name <span class="string">&quot;HaomingX&quot;</span></span><br><span class="line">git config --global user.email <span class="string">&quot;978545377@qq.com&quot;</span></span><br><span class="line"><span class="comment"># 根据你注册github的信息替换成你自己的</span></span><br></pre></td></tr></table></figure><p>生成密钥SSH key</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;978545377@qq.com&quot;</span></span><br></pre></td></tr></table></figure><p>打开<a href="http://github.com/">github</a>，点击<code>settings</code>，再点击<code>SSH and GPG keys</code>，新建一个SSH，名字任意</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure><p>复制到ssh密匙框中，保存</p><p>输入<code>ssh -T git@github.com</code>，如果说了Hi 用户名!,你就成功了</p><p>打开博客根目录下的<code>_config.yml</code>文件，这是博客的配置文件</p><p>修改最后一行的配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repository: https://github.com/HaomingX/HaomingX.github.io</span><br><span class="line">  branch: main</span><br></pre></td></tr></table></figure><h4 id="写文章"><a href="#写文章" class="headerlink" title="写文章"></a>写文章</h4><p>根目录下安装扩展</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-deployer-git</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文章</span></span><br><span class="line">hexo new post <span class="string">&quot;文章名&quot;</span></span><br></pre></td></tr></table></figure><p>打开<code>D:\blog\source\_posts</code>的目录，可以发现下面多了一个<code>.md</code>文件</p><p>编写完后</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br><span class="line">hexo s</span><br><span class="line"></span><br><span class="line">hexo d <span class="comment"># 上传到github</span></span><br></pre></td></tr></table></figure><p>打开你的<a href="https://github.io/">github.io</a>主页就能看到发布的文章</p><h3 id="butterfly美化"><a href="#butterfly美化" class="headerlink" title="butterfly美化"></a>butterfly美化</h3><p><a href="https://tzy1997.com/articles/hexo1603/">可以跟这个博主的教程走，写得很好</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 博客搭建 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
