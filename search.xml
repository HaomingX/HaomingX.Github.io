<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>NLP学习</title>
      <link href="/2023/03/08/nlp/"/>
      <url>/2023/03/08/nlp/</url>
      
        <content type="html"><![CDATA[<p>摘抄得刘知远老师的仓库：<a href="https://github.com/zibuyu/research_tao">https://github.com/zibuyu/research_tao</a></p><h2 id="What-is-NLP"><a href="#What-is-NLP" class="headerlink" title="What is NLP?"></a>What is NLP?</h2><p>理解并处理自然语言</p><h3 id="研究内容："><a href="#研究内容：" class="headerlink" title="研究内容："></a>研究内容：</h3><p>语法分析、语义分析、篇章理解等</p><h3 id="难题："><a href="#难题：" class="headerlink" title="难题："></a>难题：</h3><p>歧义解决</p><h3 id="nlp-amp-cv"><a href="#nlp-amp-cv" class="headerlink" title="nlp &amp; cv"></a>nlp &amp; cv</h3><p><img src="https://github.com/zibuyu/research_tao/raw/master/figures/00_nlp_vs_cv.jpg" alt="image"></p><p>进入高层任务后，两个领域都将面临共同的关键挑战，都可以归结为复杂语境下的多对象（图像中是不同对象，文本中是不同概念）的语义组合问题</p><h3 id="中英文nlp差异"><a href="#中英文nlp差异" class="headerlink" title="中英文nlp差异"></a>中英文nlp差异</h3><p>在词性分析、隐性表示上具有很大</p><h3 id="学术期刊-amp-会议"><a href="#学术期刊-amp-会议" class="headerlink" title="学术期刊&amp;会议"></a>学术期刊&amp;会议</h3><p>NLP&#x2F;CL的高水平学术成果主要分布在ACL、NAACL、EMNLP和COLING等几个学术会议上</p><p><a href="https://github.com/zibuyu/research_tao/blob/master/01_community.md">https://github.com/zibuyu/research_tao/blob/master/01_community.md</a></p><p>根据Google Scholar Metrics 2018年发布的NLP&#x2F;CL学术期刊和会议论文引用排名，ACL、EMNLP、NAACL、SemEval、TACL、LREC位于前6位，基本反映了本领域学者的关注程度。其中ACL、EMNLP、NAACL的H5-Index和H5-Median明显高于其他会议和期刊，也是该领域每年参会人数最多的会议，可谓NLP&#x2F;CL的三大顶级国际会议。</p><p>ACL wiki:<a href="https://aclweb.org/aclwiki/Main_Page">https://aclweb.org/aclwiki/Main_Page</a></p><h4 id="人工智能领域"><a href="#人工智能领域" class="headerlink" title="人工智能领域"></a>人工智能领域</h4><p>人工智能领域相关学术会议包括IJCAI和AAAI。AAAI全称美国人工智能年会，IJCAI全称人工智能国际联合大会。这两个会议方向非常广泛，涵盖机器人、知识、规划、自然语言处理、机器学习、计算机视觉等几乎所有AI子领域，是AI领域“奥运会”式的学术会议。近年来，由于AI领域备受社会各界关注，这两个会议的录用论文数也成倍增长。以AAAI 2019为例，投稿数猛增至7000多篇，最终录用1150篇，录用率降低至16.2%。有些老师在社交媒体上如此评价，AAAI&#x2F;IJCAI更像花样齐全的“奥运会”，而ACL&#x2F;EMNLP&#x2F;NAACL更像专业领域的“锦标赛”，所以一般对专业领域任务的精细研究，更多发表在锦标赛式的专业会议上。由于知识表示等方向没有更权威的专门学术会议，所以更多发表在AAAI&#x2F;IJCAI上。人工智能领域相关学术期刊包括Artificial Intelligence、Journal of AI Research。</p><h4 id="机器学习领域"><a href="#机器学习领域" class="headerlink" title="机器学习领域"></a>机器学习领域</h4><p>机器学习领域相关学术会议包括ICML，NIPS，ICLR、AISTATS等。其中NIPS全称是Conference on Neural Information Processing Systems，由于最近这波AI浪潮就源自以神经网络技术为基础的深度学习，所以近年来备受关注，参会人数倍增，近几年会议注册页面刚开放就会被抢注一空。树大招风，2018年由于NIPS缩写有性别歧视的意味，所以从2019年开始更名为了NeurIPS。ICLR是深度学习兴起后在2013年创立的年轻会议，采用的开放审稿模式，整个审稿过程的审稿意见、作者回复全部实时公开，也允许其他围观用户评论，面貌一新，关注者众，颇领一时风气之先。机器学习领域相关学术期刊主要包括Journal of Machine Learning Research（JMLR）和Machine Learning（ML）等。</p><h4 id="信息检索和数据挖掘领域"><a href="#信息检索和数据挖掘领域" class="headerlink" title="信息检索和数据挖掘领域"></a>信息检索和数据挖掘领域</h4><p>信息检索和数据挖掘领域相关学术会议主要由美国计算机学会（ACM）主办，包括SIGIR、KDD、WWW（从2018年开始更名为The Web Conference）、WSDM。信息检索和数据挖掘领域相关学术期刊包括ACM TOIS、IEEE TKDE、ACM TKDD、ACM TIST等。其中ACM TOIS和IEEE TKDE历史比较悠久，地位卓然；ACM TKDD则创立于2007年，ACM TIST创立于2010年，均为新兴的著名期刊，特别是ACM TIST创刊时就邀请了LibSVM等有影响力的成果发表，现在SCI影响因子比较高。</p><h4 id="国内"><a href="#国内" class="headerlink" title="国内"></a>国内</h4><blockquote><p>中国计算机学会（CCF）制定了“中国计算机学会推荐国际学术会议和期刊目录”，基本公允地列出了每个领域的高水平期刊与会议。大家可以通过这个列表，迅速了解每个领域的主要期刊与学术会议。</p></blockquote><p>与国际学术组织和会议相似，国内也有一家与NLP&#x2F;CL相关的专业学术组织，中国中文信息学会（CIPS，<a href="http://www.cipsc.org.cn/">http://www.cipsc.org.cn/</a> ），是国内最大的自然语言处理学术组织，最早由著名科学家钱伟长先生发起成立。通过学会的理事名单（<a href="http://www.cipsc.org.cn/lingdao.php">http://www.cipsc.org.cn/lingdao.php</a> ）基本可以了解国内从事NLP&#x2F;CL的主要单位和学者。中文信息学会每年组织很多学术会议，例如全国计算语言学学术会议（CCL）、中国自然语言处理青年学者研讨会（YSSNLP）、全国信息检索学术会议（CCIR）、全国机器翻译研讨会（CWMT）等，是国内NLP&#x2F;CL学者进行学术交流的重要平台。尤其值得一提的是，YSSNLP是专门面向国内NLP&#x2F;CL青年学者的研讨交流会，采用邀请制参加，大家自愿报名在研讨会上报告学术前沿动态，是国内NLP&#x2F;CL青年学者进行学术交流、建立学术合作的绝佳平台。2010年的COLING和2015年的ACL在北京召开，均由中文信息学会负责组织工作，这在一定程度上反映了学会在国内NLP&#x2F;CL领域的重要地位。此外，计算机学会中文信息技术专委会组织的自然语言处理与中文计算会议（NLP&amp;CC）是最近崛起的国内重要NLP&#x2F;CL学术会议。中文信息学会主编了一份历史悠久的《中文信息学报》，是国内该领域的重要学术期刊，发表过很多篇重量级论文。此外，国内著名的《计算机学报》、《软件学报》等期刊上也经常有NLP&#x2F;CL论文发表，值得关注。</p><h5 id="全国计算语言学大会（CCL）"><a href="#全国计算语言学大会（CCL）" class="headerlink" title="全国计算语言学大会（CCL）"></a>全国计算语言学大会（CCL）</h5><h5 id="全国知识图谱与语义计算大会（CCKS）"><a href="#全国知识图谱与语义计算大会（CCKS）" class="headerlink" title="全国知识图谱与语义计算大会（CCKS）"></a>全国知识图谱与语义计算大会（CCKS）</h5><h5 id="全国社会媒体处理大会（SMP）"><a href="#全国社会媒体处理大会（SMP）" class="headerlink" title="全国社会媒体处理大会（SMP）"></a>全国社会媒体处理大会（SMP）</h5><h5 id="全国信息检索学术会议（CCIR）"><a href="#全国信息检索学术会议（CCIR）" class="headerlink" title="全国信息检索学术会议（CCIR）"></a>全国信息检索学术会议（CCIR）</h5><h5 id="全国机器翻译研讨会（CWMT）"><a href="#全国机器翻译研讨会（CWMT）" class="headerlink" title="全国机器翻译研讨会（CWMT）"></a>全国机器翻译研讨会（CWMT）</h5><h5 id="自然语言处理青年学者研讨会（YSSNLP）"><a href="#自然语言处理青年学者研讨会（YSSNLP）" class="headerlink" title="自然语言处理青年学者研讨会（YSSNLP）"></a>自然语言处理青年学者研讨会（YSSNLP）</h5><h5 id="CIPS暑期学校（CIPS-Summer-School）"><a href="#CIPS暑期学校（CIPS-Summer-School）" class="headerlink" title="CIPS暑期学校（CIPS Summer School）"></a>CIPS暑期学校（CIPS Summer School）</h5><h5 id="CCF国际自然语言处理与中文计算会议（NLPCC）"><a href="#CCF国际自然语言处理与中文计算会议（NLPCC）" class="headerlink" title="CCF国际自然语言处理与中文计算会议（NLPCC）"></a>CCF国际自然语言处理与中文计算会议（NLPCC）</h5><h3 id="Reading-paper"><a href="#Reading-paper" class="headerlink" title="Reading paper"></a>Reading paper</h3><p><a href="https://github.com/zibuyu/research_tao/blob/master/02_reading_paper.md">https://github.com/zibuyu/research_tao/blob/master/02_reading_paper.md</a></p><p>阅读论文也不必需要每篇都从头到尾看完。一篇学术论文通常包括以下结构，我们用序号来标记建议的阅读顺序：</p><ul><li>题目（1）</li><li>摘要（2）</li><li>正文：导论（3）、相关工作（6）、本文工作（5）、实验结果（4）、结论（7）</li><li>参考文献（6）</li><li>附录</li></ul><p>按照这个顺序，基本在读完题目和摘要后，大致可以判断这篇论文与自己研究课题的相关性，然后就可以决定是否要精读导论和实验结果判断学术价值，是否阅读本文工作了解方法细节。此外，如果希望了解相关工作和未来工作，则可以有针对性地阅读“相关工作”和“结论”等部分。</p><h3 id="Where’s-Idea"><a href="#Where’s-Idea" class="headerlink" title="Where’s Idea?"></a>Where’s Idea?</h3><p><strong>实践法</strong>。即在研究任务上实现已有最好的算法，通过分析实验结果，例如发现这些算法计算复杂度特别高、训练收敛特别慢，或者发现该算法的错误样例呈现明显的规律，都可以启发你改进已有算法的思路。现在很多自然语言处理任务的Leaderboard上的最新算法，就是通过分析错误样例来有针对性改进算法的 [1]。</p><p><strong>类比法</strong>。即将研究问题与其他任务建立类比联系，调研其他相似任务上最新的有效思想、算法或工具，通过合理的转换迁移，运用到当前的研究问题上来。例如，当初注意力机制在神经网络机器翻译中大获成功，当时主要是在词级别建立注意力，后来我们课题组的林衍凯和沈世奇提出建立句子级别的注意力解决关系抽取的远程监督训练数据的标注噪音问题 [2]，这就是一种类比的做法。</p><p><strong>组合法</strong>。即将新的研究问题分解为若干已被较好解决的子问题，通过有机地组合这些子问题上的最好做法，建立对新的研究问题的解决方案。例如，我们提出的融合知识图谱的预训练语言模型，就是将BERT和TransE等已有算法融合起来建立的新模型 [3]。</p><p>正如武侠中的最高境界是无招胜有招，好的研究想法并不拘泥于以上的路径，很多时候是在研究者对研究问题深刻认知的基础上，综合丰富的研究阅历和聪明才智产生”顿悟“的结果。这对初学者而言恐怕还很难一窥门径，需要从基本功做起，经过大量科研实践训练后，才能有登堂入室之感。</p><p>在科研实践过程中，除了通过大量文献阅读了解历史，通过深入思考总结产生洞察力外，还有一项必不可少的工作，那就是主动开放的学术交流和合作意识。不同研究领域思想和成果交流碰撞，既为创新思想提供了新的来源，也为”类比“和”顿悟“提供了机会。了解一下历史就可以知晓，人工智能的提出，就是数学、计算机科学、控制论、信息论、脑科学等学科交叉融合的产物。而当红的深度学习的起源，1980年代的Parallel Distributed Processing （PDP），也是计算机科学、脑认知科学、心理学、生物学等领域研究者通力合作的产物。</p><h3 id="如何写一篇论文"><a href="#如何写一篇论文" class="headerlink" title="如何写一篇论文"></a>如何写一篇论文</h3><p><a href="https://github.com/zibuyu/research_tao/blob/master/04_writing_paper.md">https://github.com/zibuyu/research_tao/blob/master/04_writing_paper.md</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>位置编码</title>
      <link href="/2023/03/08/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"/>
      <url>/2023/03/08/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h1 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h1><h2 id="起源-Transformer"><a href="#起源-Transformer" class="headerlink" title="起源-Transformer"></a>起源-Transformer</h2><p>transformer提出了经典的Sinusoidal位置编码，然后各种位置编码被陆续提出，也产生了一些变体Transformer如（Reformer）</p><h4 id="思考一：位置编码是干嘛，什么是好的位置编码方案"><a href="#思考一：位置编码是干嘛，什么是好的位置编码方案" class="headerlink" title="思考一：位置编码是干嘛，什么是好的位置编码方案"></a><strong>思考一</strong>：位置编码是干嘛，什么是好的位置编码方案</h4><p>位置编码是表示字句时序位置的编码，因为Transformer这种纯靠attention的模型，无法天然的获取位置信息（即改变attention的内部顺序得到的结果没有变）</p><p>好的位置编码：</p><ol><li>对于给定的位置，它的位置编码是唯一的 （绝对和相对按道理都应该这样）</li><li>不同长度的句子之间，任何两个时间步之间的距离应该尽量一致</li><li>模型是很容易泛化到更长句子的  （最近的Longtext研究给了一些泛化方案）</li></ol><p>但是具体来说还需要考虑计算复杂度，具体下游任务的实际实验效果</p><h4 id="疑惑一：-为什么position-encoding就直接加到embedding向量上了"><a href="#疑惑一：-为什么position-encoding就直接加到embedding向量上了" class="headerlink" title="疑惑一： 为什么position encoding就直接加到embedding向量上了"></a>疑惑一： 为什么position encoding就直接加到embedding向量上了</h4><p>根据网上的理解，embedding本质就是onehot进行全连接，所以coding之后相加其实等价于coding之前torch.cat之后再进行一个大的全连接。<strong>所以相加相当于一个特征的融合</strong>，相加也符合向量空间关系的一种折中，Bert coding的时候相加也可以相同理解</p><p>那为什么不能是向量相乘呢（后续也有相关工作）</p><h2 id="绝对位置编码"><a href="#绝对位置编码" class="headerlink" title="绝对位置编码"></a>绝对位置编码</h2><h3 id="三角式"><a href="#三角式" class="headerlink" title="三角式"></a>三角式</h3><p><a href="https://kexue.fm/archives/8231">Sinusoidal位置编码</a></p><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230713211624459.png" alt="image-20230713211624459"></p><p>其中 pk,2i, pk,2i+1 分别是位置 k 的编码向量的第 2i,2i+1 个分量，d 是位置向量的维度。</p><p>是绝对位置编码，但含有相对位置信息，推导可以从泰勒展开理解，但仅限二维，所以可解释性差</p><h3 id="递归式"><a href="#递归式" class="headerlink" title="递归式"></a>递归式</h3><p>论文：<a href="https://arxiv.org/abs/2003.09229">Learning to Encode Position for Transformer with Continuous Dynamical Model</a></p><p>思想就是通过 RNN 结构来学习一种编码方案，外推性较好，但牺牲了并行性，可能会带来速度瓶颈</p><h3 id="相乘式"><a href="#相乘式" class="headerlink" title="相乘式"></a>相乘式</h3><p>博客：<a href="https://zhuanlan.zhihu.com/p/183234823">https://zhuanlan.zhihu.com/p/183234823</a></p><h3 id="RoPE旋转位置编码"><a href="#RoPE旋转位置编码" class="headerlink" title="RoPE旋转位置编码"></a>RoPE旋转位置编码</h3><p>也是绝对位置编码。</p><p>二维形式：<img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230713212328868.png" alt="image-20230713212328868"></p><p>根据矩阵乘法的几何意义可以知道，此时实际上代表着<strong>query向量的旋转</strong>，所以取名旋转位置编码。</p><p>动机：原来的Sinusoidal编码不够好</p><p>作者苏剑林是从向量的内积表示两个向量的位置关系出发，通过复数表示（两个复向量的内积为复向量*复向量的共轭），推导出了这样一个旋转位置编码，更有可解释性，从预训练模型 RoFormer 的结果来看，RoPE 具有良好的外推性，应用到 Transformer 中体现出较好的处理长文本的能力。且能作用于<strong>线性attention</strong>（Transformer的attention为二阶复杂度），因为编码矩阵是正交矩阵且直接作用于query和key，不改变向量模长。</p><p>偶数多维：</p><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230713213830123.png" alt="image-20230713213830123"></p><p>由于$R_{n-m}$是正交矩阵，不改变向量模长，所以应该不会改变模型的稳定性</p><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230713214036206.png" alt="image-20230713214036206"></p><p>苏剑林还想到将这样一个稀疏矩阵乘积化成</p><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230713214257770.png" alt="image-20230713214257770"></p><p>既降低计算的复杂度，使RoPE可以视为<strong>乘性位置编码的变体</strong></p><p>最近几天国外网友推出的NTK-Aware Scaled RoPE，使苏剑林提出了：从 <code>β</code> 进制编码的角度理解 RoPE，放在后面讲</p><p>一些使用RoPE的模型（<a href="https://github.com/ZhuiyiTechnology/roformer">ReFormer</a>（苏剑林自己开源的）、<a href="https://github.com/bojone/GlobalPointer">GlobalPoint</a>）</p><h2 id="相对位置编码"><a href="#相对位置编码" class="headerlink" title="相对位置编码"></a>相对位置编码</h2><h3 id="经典式"><a href="#经典式" class="headerlink" title="经典式"></a>经典式</h3><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230710111543547.png" alt="image-20230710111543547"></p><p><strong>只需要有限个位置编码，就可以表达出任意长度的相对位置（因为进行了截断）</strong></p><h3 id="XLNET式"><a href="#XLNET式" class="headerlink" title="XLNET式"></a>XLNET式</h3><p><a href="https://arxiv.org/abs/1901.02860">《Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context》</a></p><p>位置编码：</p><p>$\boldsymbol{q}<em>{i} \boldsymbol{k}</em>{j}^{\top}&#x3D;\boldsymbol{x}<em>{i} \boldsymbol{W}</em>{Q} \boldsymbol{W}<em>{K}^{\top} \boldsymbol{x}</em>{j}^{\top}+\boldsymbol{x}<em>{i} \boldsymbol{W}</em>{Q} \boldsymbol{W}<em>{K}^{\top} \boldsymbol{p}</em>{j}^{\top}+\boldsymbol{p}<em>{i} \boldsymbol{W}</em>{Q} \boldsymbol{W}<em>{K}^{\top} \boldsymbol{x}</em>{j}^{\top}+\boldsymbol{p}<em>{i} \boldsymbol{W}</em>{Q} \boldsymbol{W}<em>{K}^{\top} \boldsymbol{p}</em>{j}^{\top} —(*)$</p><p>最终：</p><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230710111624848.png" alt="image-20230710111624848"></p><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230710111655900.png" alt="image-20230710111655900"></p><p>相对位置矩阵只加到 attention 矩阵上，不加到 $v_j$ 上去了，后续的工作也都如此</p><h3 id="T5-式"><a href="#T5-式" class="headerlink" title="T5 式"></a>T5 式</h3><p><a href="https://arxiv.org/abs/1910.10683">《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》</a></p><p>*式可以理解为“输入-输入”、“输入-位置”、“位置-输入”、“位置-位置”四项注意力的组合，如果我们认为输入信息与位置信息应该是独立（解耦）的，那么它们就不应该有过多的交互，所以“输入-位置”、“位置-输入”两项 Attention 可以删掉。</p><p>而 <img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230710111848513.png" alt="image-20230710111848513"> 实际上只是一个只依赖于(i, j)的标量，我们可以直接将它作为参数训练出来，即简化为 <img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230710111907614.png" alt="image-20230710111907614"></p><p>说白了，它仅仅是在 Attention 矩阵的基础上加一个可训练的偏置项而已，而跟 XLNET 式一样，在 $v_j$ 上的位置偏置则直接被去掉了</p><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230710125334026.png" alt="image-20230710125334026"></p><p>包含同样的思想的还有微软在ICLR 2021的论文<a href="https://arxiv.org/abs/2006.15595">《Rethinking Positional Encoding in Language Pre-training》</a>中提出的TUPE位置编码</p><p>TUPE位置编码中还通过重置与[CLS]相关的位置相关性来解除[CLS]</p><h3 id="DoBERTa-式"><a href="#DoBERTa-式" class="headerlink" title="DoBERTa 式"></a>DoBERTa 式</h3><p>DeBERTa 和 T5 刚刚相反，它扔掉了第 4 项，保留第 2、3 项并且替换为相对位置编码</p><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230710125841040.png" alt="image-20230710125841040"></p><p>语录：&#x3D;&#x3D;科研就是枚举所有排列组合看哪个更优&#x3D;&#x3D;</p><h3 id="LongText最新进展："><a href="#LongText最新进展：" class="headerlink" title="LongText最新进展："></a>LongText最新进展：</h3><ol><li>baseline 直接外推</li><li>SuperHOT LoRA 线性内插+微调  同时还有Meta的<a href="https://arxiv.org/abs/2306.15595">《Extending Context Window of Large Language Models via Positional Interpolation》</a></li></ol><p>首先是进制思想</p><p>其次线性内插其实简单来说就是将2000以内压缩到1000以内</p><p><img src="https://kexue.fm/usr/uploads/2023/07/4113541717.png" alt="img"></p><p>原本模型已经学会了875&gt;874, 现在泛化一个874.5&gt;874应该不会太难</p><ol start="3"><li>NBCE (Naive Bayes-based Context Extension)   <a href="https://kexue.fm/archives/9617">https://kexue.fm/archives/9617</a>  (之前苏剑林根据朴素贝叶斯提出的一个东西，他测试不微调就可以扩展Context长度)</li></ol><p><a href="https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E6%95%B0%E5%AD%A6%E8%AF%BE/10%20%20%E4%BF%A1%E6%81%AF%E7%86%B5%EF%BC%9A%E4%BA%8B%E4%BB%B6%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%EF%BC%9F.md">条件熵</a></p><ol start="4"><li>NTK-Aware Scaled RoPE (不微调就很好，微调了可能更好)    <a href="https://kexue.fm/archives/9675">Transformer 升级之路：10、RoPE 是一种β进制编码</a></li></ol><p>一方面可以从进制方面理解，另一方面可以从高频外推，低频内插理解</p><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230714000941973.png" alt="image-20230714000941973"></p><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230714001057302.png" alt="image-20230714001057302"></p><p>这个扩增方案就能解释直接外推方案就是啥也不改，内插方案就是将n换成n&#x2F;k。</p><p>进制转换，就是要扩大k倍表示范围，那么原本的β进制至少要扩大成$β(k^{2&#x2F;d})$进制或者等价地原来的底数10000换成10000k</p><p>这其实就是NTK-Aware Scaled RoPE （苏剑林的推导）</p><p>提出者的推导：高频外推、低频内插</p><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230714001923448.png" alt="image-20230714001923448"></p><p>苏剑林的实验中还发现NTK-RoPE在“重复”外推上比“不重复”外推效果明显好，表明这样修改之后是保留了全局依赖，而不是单纯将注意力局部化</p><ol start="5"><li><a href="https://arxiv.org/pdf/2307.03172.pdf">Myth of Context Length</a> ：  Stanford  &amp;  UC Berkeley  &amp;   Samaya AI</li></ol><ul><li>扩展上下文的模型不一定更擅长利用其输入上下文</li></ul><p>eg: longchat在140个键值设置中，longchat是一个显著的异常值；当相关信息在输入上下文的开头时， 它倾向于生成用于检索键的代码，而不是输出值本身。</p><ul><li>与其基准模型（即在指令微调之前）MPT30B相比，MPT-30B-Instruct在多文档问答中的性能表现进行了对比。这两个模型都具有一个呈U型的性能曲线，当相关信息出现在输入上下文的开头 或结尾时，性能显著提高，这表明指令调优过程本 身不一定是造成这些性能趋势的原因</li><li>查询query和数据data的顺序对于decoder-only模型？（decoder-only模型在每个时间步只能关注先前标记的方式来处理）</li></ul><p>Query-Aware Contextualization显著提高key-value retrieval task， 对多文档问题提升不大（放在开头稍好）</p><p>结论： 有监督 的指令微调数据中，任务规范和&#x2F;或指令通常 放置在输入上下文的开头，这可能导致经过指 令微调的语言模型更重视输入上下文的开头部 分</p><ul><li>根据下游任务权衡上游模型。提供更多信息给经过训练的 指令型语言模型，可能有助于提高下游任务的 性能，但也会增加模型需要处理的内容量。</li></ul><p>&#x3D;&#x3D;只做了实验探究，没有给出合理的解释，只给出了一个人类心理学现象作为类比&#x3D;&#x3D;</p><p>​6. softmax_1:<a href="https://www.evanmiller.org/attention-is-off-by-one.html?continueFlag=5d0e431f4edf1d8cccea47871e82fbc4">https://www.evanmiller.org/attention-is-off-by-one.html?continueFlag=5d0e431f4edf1d8cccea47871e82fbc4</a></p><h4 id="思考二："><a href="#思考二：" class="headerlink" title="思考二："></a>思考二：</h4><ol><li>线性内插当处理范围更大时，内插方案的维度（先是个位，后十位）会压缩得更拥挤，每个维度的极限密度（达到性能瓶颈）是多少</li></ol><p>​        这应该取决于具体的计算资源、内存限制和线性内插算法的效率</p><ol start="2"><li><p>在具体的下游任务上评估线性内插压缩的程度的影响，不同的下游任务可能是不是选不同的k</p></li><li><p>为什么在transformer这类模型中，长文本时同样更容易注意两端文本</p></li><li><p>transformer的改进（一直都在进行的工作）</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>使用 2048 源代码构建 2048 镜像</title>
      <link href="/2023/03/08/%E4%BD%BF%E7%94%A8%202048%20%E6%BA%90%E4%BB%A3%E7%A0%81%E6%9E%84%E5%BB%BA%202048%20%E9%95%9C%E5%83%8F/"/>
      <url>/2023/03/08/%E4%BD%BF%E7%94%A8%202048%20%E6%BA%90%E4%BB%A3%E7%A0%81%E6%9E%84%E5%BB%BA%202048%20%E9%95%9C%E5%83%8F/</url>
      
        <content type="html"><![CDATA[<ol><li><p>使用 2048 源代码构建 2048 镜像 </p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230628154549433.png" alt="image-20230628154549433"></p></li><li><p>通过 Docker 运行单机版 2048</p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230628154614031.png" alt="image-20230628154614031"></p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230628154648383.png" alt="image-20230628154648383"></p></li><li><p>在 2048 镜像中，使用 RUN 命令安装 iputils-ping （apt update &amp;&amp; apt install -y iputilsping) </p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230628192236742.png" alt="image-20230628192236742"></p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230628192258458.png" alt="image-20230628192258458"></p></li><li><p>使用 docker exec 命令，进入到容器中，执行 ping 命令</p></li></ol><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230628192428643.png" alt="image-20230628192428643"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>语言模型</title>
      <link href="/2023/03/08/%E5%A4%8F%E5%AD%A3%E5%88%9B%E6%96%B0%E8%AF%BE%E6%8A%A5%E5%91%8A/"/>
      <url>/2023/03/08/%E5%A4%8F%E5%AD%A3%E5%88%9B%E6%96%B0%E8%AF%BE%E6%8A%A5%E5%91%8A/</url>
      
        <content type="html"><![CDATA[<h1 id="夏季创新课报告-语言模型"><a href="#夏季创新课报告-语言模型" class="headerlink" title="夏季创新课报告-语言模型"></a>夏季创新课报告-语言模型</h1><h2 id="学号：2021112905姓名：徐浩铭"><a href="#学号：2021112905姓名：徐浩铭" class="headerlink" title="学号：2021112905姓名：徐浩铭"></a>学号：2021112905姓名：徐浩铭</h2><ol><li><p>从智能体出发</p><ol><li>介绍了chatGPT的背景和作用。chatGPT是OpenAI公司开发的对话机器人,标志着自然语言处理技术取得突破,从弱AI向强AI转变。</li><li>从语言模型的角度分析chatGPT。chatGPT的核心是大型语言模型,可以处理自然语言的远距离依赖关系,进行语义计算,具备多任务处理能力。</li><li>从知识的角度分析chatGPT。chatGPT实现了知识的统一,是大知识和富知识,改变了人类的知识获取方式。</li><li>分析了chatGPT对互联网内容生成、社交媒体、各行各业的影响。</li><li>探讨了人工智能产业的发展趋势。chatGPT带来新的chatGPT+X模式,形成新的产业集群和创新网络。</li></ol><p>&#x3D;&#x3D;智能体才是未来&#x3D;&#x3D;</p></li><li><p>n-gram</p><ol><li><p>介绍了语言现象、语料库、如何从语料库中学习语言知识的基本思路。</p></li><li><p>介绍了语言模型n-gram的相关内容: </p><p>(1) 定义了语言模型的两个作用: (a) 计算一个词序列s出现的概率p(s),判断s是否是一个合法的语言现象。 (b) 在给定上下文context的条件下,预测下一个词w的概率p(w|context)。</p><p> (2) 介绍了利用语料库训练语言模型n-gram的过程:</p><p>​ (a) 对语料库中的文本进行分词,获得词序列。</p><p>​ (b) 统计词频,得到每个词w的出现次数c(w)。</p><p>​ (c) 统计bigram频率,得到每个词对(w1,w2)的出现次数c(w1,w2)。<br>​(d) 统计trigram频率,得到每个词组(w1,w2,w3)的出现次数c(w1,w2,w3)。</p><p>​ (e) 将这些统计结果保存为unigram、bigram、trigram模型。</p><p> (3) 介绍了如何利用n-gram模型计算句子概率,以及平滑处理方法。 (4) 介绍了n-gram模型的评价方法和下一词预测方法。 (5) 介绍了困惑度可以评价n-gram模型的效果，分析了n-gram模型的优缺点。</p><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230724102136724.png" alt="image-20230724102136724"></p><p>(6)总结了n-gram模型的优缺点: (a) 优点:无监督训练,利用近义词相关性。 (b) 缺点:n限制在3以下,只用了短距离相关性。</p></li><li><p>介绍了词典tokenize的两种方法:中文分词和BPE英文子词切分法。</p><p>BPE英文字词分法：<a href="https://zhuanlan.zhihu.com/p/383650769">https://zhuanlan.zhihu.com/p/383650769</a></p></li><li><p>结尾部分,讨论了语言模型的why、what和how三个问题。 why:阐述了学习语言模型的意义在于服务于智能体。 what:概述了语言模型的发展历程。 how:分析了语言模型学习资料的局限性,需要多方查阅。</p></li></ol></li><li><p>词向量</p><ol><li><p>导言部分提出语言模型的三大任务:学习词语义、文本理解和文本生成。</p></li><li><p>介绍了表示词语义的两种方法: </p><p>(1) 建立知识库的方法获取词语义, <strong>类似字典</strong>，面临语料覆盖有限、无法判断上下文词义的问题。</p><p>(2) 统计语言模型n-gram，对词进行了符号表示。</p><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230724103037912.png" alt="image-20230724103037912"></p><p>(3) one-hot编码：给定一个词典V，将每个词表示为一个二进制向量。**向量的维度为|V|**，它采用词表中每个词的索引作为二进制编码的位置，只有该位置上取值为1，其他位置的取值都为0。这种词的编码，称为one-hot编码。</p><ul><li><p>one-hot向量是词的一种符号表示，没有表示出词的语义。</p></li><li><p>one-hot编码是个高维、稀疏的向量。向量维度是词典的大小。</p></li><li><p>one-hot向量是正交的，不能计算两个词之间的相似度。</p></li></ul><p> (4) 利用上下文学习词向量的<strong>分布式表示方法</strong>(<strong>上下文相似的词，有着相似的语义</strong>)。</p></li><li><p>介绍了词编码的两种表示: </p><p>(1) One-hot 高维稀疏的离散表示,无法计算词义相关性。<br>(2) &#x3D;&#x3D;词向量&#x3D;&#x3D;低维稠密的连续实数表示,可以表示词义。</p></li><li><p>详细介绍了Bengio等人提出的神经网络语言模型NNLM: </p><p>(1) 模型结构:输入层、词向量层、隐层、输出层。详述了每个层的计算公式。</p><p> (2) 模型参数:词向量矩阵E,隐层矩阵W、偏置b,输出层矩阵W、偏置b。</p><p>(3) 目标函数设计采用最大对数似然估计。</p><p> (4) 通过学习词向量矩阵E获得词向量。</p></li><li><p>介绍了Mikolov的CBOW模型: </p><p>(1) 模型结构:输入层、词向量层、隐层、输出层。详述了每个层的计算公式。</p><p> (2) 模型参数:上下文词向量矩阵E,输出词向量矩阵E’。 </p><p>(3) 目标函数采用对数似然估计。 </p><p>(4) 通过输入词上下文,学习输出词的词向量。</p><ul><li><p>每个词的词向量是一个低维稠密的实数向量，学习了词的语义表示。</p></li><li><p>每个词被映射到一个固定维度的向量空间。语义相近的词在空间中的距离较近。通过计算两个词向量之间的相似度，也学习到两个词之间的语义相关性。</p></li></ul></li></ol></li><li><p>神经网络</p><ol><li>神经网络的思想从神经元得来</li><li>激活函数、前馈网络、文本分类、情感分析</li></ol></li><li><p>RNN</p><ol><li><p>介绍了循环神经网络语言模型RNN LM:</p><p>(1) RNN模型结构包括输入层、隐层和输出层。隐层保存历史信息。</p><p>(2) RNN语言模型的输入为完整的上下文序列,输出为下一个词。克服了固定窗口长度的限制。</p><p>(3) 详细介绍了RNN语言模型的输入层、隐层和输出层的计算公式。</p><p>(4) RNN语言模型可以用来文本生成。</p></li><li><p>分析了RNN的不同任务:</p><p>(1) one-one:多层感知机MLP。</p><p>(2) one-many:图像标题生成。</p><p>(3) many-one:文本生成、情感分类。</p><p>(4) many-many:序列标注、seq2seq编解码</p><p>(5) Encoder-Decoder:机器翻译。</p></li><li><p>介绍了RNN的改进模型LSTM:</p><p><img src="C:\Users\97854\AppData\Roaming\Typora\typora-user-images\image-20230724105758026.png" alt="image-20230724105758026"></p><p>(1) LSTM在RNN基础上增加了记忆单元。</p><p>(2) LSTM单元详细介绍了输入、输出和各个门的计算。</p><p>(3) LSTM引入了门控机制解决了RNN的梯度问题。</p></li><li><p>RNN的本质分析:</p><p>(1) RNN是一个序列模型,可以处理变长序列,编码历史序列信息。</p><p>(2) RNN通过隐层保存历史,实现序列间的依赖学习。</p><p>(3) RNN是一个通用的序列学习框架,可用于多种不同的序列任务。</p><p>(4) RNN的改进模型LSTM增强了 RNN 在较长序列上的记忆能力。</p></li></ol></li><li><p>attention+Transformer</p><ol><li><p>编码器-解码器结构的机器翻译模型（seq2seq）存在的问题:</p><p>（1）Encoder（RNN）生成的上下文向量，缺乏远距离的语言依赖关系。（2）对Decoder的当前状态所对应的语言依赖关系，也缺乏指导。</p></li><li><p>注意力机制的原理:计算查询和键值之间的相关性,对值进行加权求和作为输出。可以建模远距离依赖关系。</p><p>eg: TextRNN+attention进行文本分类任务</p></li><li><p>自注意力机制的本质:利用上下文中的词为目标词编码,计算目标词与上下文词的相关性作为权重,上下文词的词向量作为特征值,加权求和作为目标词的表示。</p></li><li><p>掩码自注意力机制:防止目标词看到后面的词,只利用前面的上下文进行编码。</p></li><li><p>多头自注意力机制:使用多个不同的键、值、查询投影,获得多个子空间的表示,提高模型表达能力。</p></li><li><p>Transformer的组成:输入模块、Encoder模块、Decoder模块、输出模块。核心是stacked的Encoder和Decoder中的多头自注意力机制和前馈全连接网络。</p></li><li><p>Transformer的预训练:最大化语料概率;微调:最大化人工标注语料的概率。</p></li><li><p>典型的预训练语言模型GPT和BERT的区别:前者自回归预测下一个词,后者掩码预测被遮蔽的词。</p></li><li><p>Transformer利用注意力机制有效建模全局依赖关系,是当前自然语言处理中主流的模型结构。</p></li></ol></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>assembly_code</title>
      <link href="/2023/03/08/assembly-code/"/>
      <url>/2023/03/08/assembly-code/</url>
      
        <content type="html"><![CDATA[<p>本文参考了<a href="https://www.ruanyifeng.com/blog/2018/01/assembly-language-primer.html">阮一峰汇编语言入门教程</a></p><p>引：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc -Og -s hello.c</span><br></pre></td></tr></table></figure><p>实际上-Og只是为了让机器在产生汇编和机器代码时不要过于优化导致代码严重变形，便于理解</p><p>工程中追求性能时常用 -O1 或 -O2</p><h2 id="一、汇编是什么"><a href="#一、汇编是什么" class="headerlink" title="一、汇编是什么"></a>一、汇编是什么</h2><ol><li><p>cpu执行的指令是二进制的,称作<strong>操作码(opcode)</strong></p></li><li><p>编译器的作用就是将高级语言程序翻译成一条条操作码</p></li><li><p>二进制对人类是不可读的,所以产生<strong>汇编语言</strong></p><blockquote><p>汇编语言是二进制指令的文本形式,和其是<strong>一一对应</strong>的关系.比如，加法指令<code>00000011</code>写成汇编语言就是 ADD。只要还原成二进制，汇编语言就可以被 CPU 直接执行，所以它是最底层的低级语言。</p></blockquote></li></ol><h2 id="二、汇编的来历"><a href="#二、汇编的来历" class="headerlink" title="二、汇编的来历"></a>二、汇编的来历</h2><p>为解决二进制指令的可读性问题,产生了文本指令,执行时需要把文本指令翻译为二进制,这被称为<strong>assembling</strong>。所以就产生了<strong>assembly code(汇编语言)</strong></p><p>目前主流的是<strong>x86汇编语言</strong>,即Intel公司的cpu所使用。</p><h2 id="三、寄存器"><a href="#三、寄存器" class="headerlink" title="三、寄存器"></a>三、寄存器</h2><p>CPU 本身只负责运算，不负责储存数据。数据一般都储存在内存之中，CPU 要用的时候就去内存读写数据。但是，CPU 的运算速度远高于内存的读写速度，为了避免被拖慢，CPU 都自带一级缓存和二级缓存。基本上，CPU 缓存可以看作是读写速度较快的内存。</p><p>但是，CPU 缓存还是不够快，另外数据在缓存里面的地址是不固定的，CPU 每次读写都要寻址也会拖慢速度。因此，除了缓存之外，CPU 还自带了寄存器（register），用来储存最常用的数据。也就是说，那些最频繁读写的数据（比如循环变量），都会放在寄存器里面，CPU 优先读写寄存器，再由寄存器跟内存交换数据。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/bg2018012206.png" alt="img"></p><p>寄存器不依靠地址区分数据，而依靠名称。每一个寄存器都有自己的名称，我们告诉 CPU 去具体的哪一个寄存器拿数据，这样的速度是最快的。有人比喻寄存器是 CPU 的零级缓存。</p><h2 id="四、寄存器的种类"><a href="#四、寄存器的种类" class="headerlink" title="四、寄存器的种类"></a>四、寄存器的种类</h2><p>早期的 x86 CPU 只有8个寄存器，而且每个都有不同的用途。现在的寄存器已经有100多个了，都变成通用寄存器，不特别指定用途了，但是早期寄存器的名字都被保存了下来。</p><blockquote><ul><li>EAX</li><li>EBX</li><li>ECX</li><li>EDX</li><li>EDI</li><li>ESI</li><li>EBP</li><li>ESP</li></ul></blockquote><p>且现在的机器都是64位的了，上述E–&gt;R</p><p>上面这8个寄存器之中，前面七个都是通用的。ESP 寄存器有特定用途，保存当前 Stack 的地址（详见下一节）。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/bg2018012207.png" alt="img"></p><p>我们常常看到 32位 CPU、64位 CPU 这样的名称，其实指的就是寄存器的大小。32 位 CPU 的寄存器大小就是4个字节。</p><h2 id="五、内存模型：堆（Heap）"><a href="#五、内存模型：堆（Heap）" class="headerlink" title="五、内存模型：堆（Heap）"></a>五、内存模型：堆（Heap）</h2><p>寄存器只能存放很少量的数据，大多数时候，CPU 要指挥寄存器，直接跟内存交换数据。所以，除了寄存器，还必须了解内存怎么储存数据。</p><p>程序运行的时候，操作系统会给它分配一段内存，用来储存程序和运行产生的数据。这段内存有起始地址和结束地址，比如从<code>0x1000</code>到<code>0x8000</code>，起始地址是较小的那个地址，结束地址是较大的那个地址。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/bg2018012208.png" alt="img"></p><p>程序运行过程中，对于动态的内存占用请求（比如新建对象，或者使用<code>malloc</code>命令），系统就会从预先分配好的那段内存之中，划出一部分给用户，具体规则是从起始地址开始划分（实际上，起始地址会有一段静态数据，这里忽略）。举例来说，用户要求得到10个字节内存，那么从起始地址<code>0x1000</code>开始给他分配，一直分配到地址<code>0x100A</code>，如果再要求得到22个字节，那么就分配到<code>0x1020</code>。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/bg2018012209.png" alt="img"></p><p>这种因为用户主动请求而划分出来的内存区域，叫做 Heap（堆）。它由起始地址开始，从低位（地址）向高位（地址）增长。Heap 的一个重要特点就是不会自动消失，必须手动释放，或者由垃圾回收机制来回收。</p><h2 id="六、内存模型：栈（Stack）"><a href="#六、内存模型：栈（Stack）" class="headerlink" title="六、内存模型：栈（Stack）"></a>六、内存模型：栈（Stack）</h2><p>简单说，Stack 是由于函数运行而临时占用的内存区域。结束后会自动回收··</p><p>Stack 是由内存区域的结束地址开始，从高位（地址）向低位（地址）分配。比如，内存区域的结束地址是<code>0x8000</code>，第一帧假定是16字节，那么下一次分配的地址就会从<code>0x7FF0</code>开始；第二帧假定需要64字节，那么地址就会移动到<code>0x7FB0</code>。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/bg2018012215.png" alt="img"></p><h2 id="七、cpu指令"><a href="#七、cpu指令" class="headerlink" title="七、cpu指令"></a>七、cpu指令</h2><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// file:example.c</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">add_a_and_b</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> add_a_and_b(<span class="number">2</span>,<span class="number">3</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>gcc将这个程序转成汇编语言</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc -S example.c</span><br></pre></td></tr></table></figure><p>上面的命令执行以后，会生成一个文本文件<code>example.s</code>，里面就是汇编语言，包含了几十行指令。这么说吧，一个高级语言的简单操作，底层可能由几个，甚至几十个 CPU 指令构成。CPU 依次执行这些指令，完成这一步操作。</p><p><code>example.s</code>经过简化以后，大概是下面的样子。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">_add_a_and_b:</span><br><span class="line">   push   %ebx</span><br><span class="line">   mov    %eax, [%esp+8] </span><br><span class="line">   mov    %ebx, [%esp+12]</span><br><span class="line">   add    %eax, %ebx </span><br><span class="line">   pop    %ebx </span><br><span class="line">   ret  </span><br><span class="line"></span><br><span class="line">_main:</span><br><span class="line">   push   3</span><br><span class="line">   push   2</span><br><span class="line">   call   _add_a_and_b </span><br><span class="line">   add    %esp, 8</span><br><span class="line">   ret</span><br></pre></td></tr></table></figure><p>​    </p><p>可以看到，原程序的两个函数<code>add_a_and_b</code>和<code>main</code>，对应两个标签<code>_add_a_and_b</code>和<code>_main</code>。每个标签里面是该函数所转成的 CPU 运行流程。</p><p>每一行就是 CPU 执行的一次操作。它又分成两部分，就以其中一行为例。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">push   %ebx</span><br></pre></td></tr></table></figure><p>这一行里面，<code>push</code>是 CPU 指令，<code>%ebx</code>是该指令要用到的运算子。一个 CPU 指令可以有零个到多个运算子。</p><h3 id="push指令"><a href="#push指令" class="headerlink" title="push指令"></a>push指令</h3><p>根据约定，程序从<code>_main</code>标签开始执行，这时会在 Stack 上为<code>main</code>建立一个帧，并将 Stack 所指向的地址，写入 ESP 寄存器。后面如果有数据要写入<code>main</code>这个帧，就会写在 ESP 寄存器所保存的地址。</p><p>然后，开始执行第一行代码。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">push   3</span><br></pre></td></tr></table></figure><p><code>push</code>指令用于将运算子放入 Stack，这里就是将<code>3</code>写入<code>main</code>这个帧。</p><p>虽然看上去很简单，<code>push</code>指令其实有一个前置操作。它会先取出 ESP 寄存器里面的地址，将其减去4个字节，然后将新地址写入 ESP 寄存器。使用减法是因为 Stack 从高位向低位发展，4个字节则是因为<code>3</code>的类型是<code>int</code>，占用4个字节。得到新地址以后， 3 就会写入这个地址开始的四个字节。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">push   2</span><br></pre></td></tr></table></figure><p>第二行也是一样，<code>push</code>指令将<code>2</code>写入<code>main</code>这个帧，位置紧贴着前面写入的<code>3</code>。这时，ESP 寄存器会再减去 4个字节（累计减去8）。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/bg2018012216.png" alt="img"></p><h3 id="call指令"><a href="#call指令" class="headerlink" title="call指令"></a>call指令</h3><p>第三行的 call 的指令用来调用函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">call   _add_a_and_b</span><br></pre></td></tr></table></figure><p>上面的代码表示调用<code>add_a_and_b</code>函数。这时，程序就会去找<code>_add_a_and_b</code>标签，并为该函数建立一个新的帧。</p><p>下面就开始执行<code>_add_a_and_b</code>的代码。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">push   %ebx</span><br></pre></td></tr></table></figure><p>这一行表示将 EBX 寄存器里面的值，写入<code>_add_a_and_b</code>这个帧。这是因为后面要用到这个寄存器，就先把里面的值取出来，用完后再写回去。</p><p>这时，<code>push</code>指令会再将 ESP 寄存器里面的地址减去4个字节（累计减去12）。</p><h3 id="mov-指令"><a href="#mov-指令" class="headerlink" title="mov 指令"></a>mov 指令</h3><p><code>mov</code>指令用于将一个值写入某个寄存器。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mov  %eax, [%esp+8]</span><br></pre></td></tr></table></figure><p>这一行代码表示，先将 ESP 寄存器里面的地址加上8个字节，得到一个新的地址，然后按照这个地址在 Stack 取出数据。根据前面的步骤，可以推算出这里取出的是<code>2</code>，再将<code>2</code>写入 EAX 寄存器。</p><p>下一行代码也是干同样的事情。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mov    %ebx, [%esp+12] </span><br></pre></td></tr></table></figure><p>上面的代码将 ESP 寄存器的值加12个字节，再按照这个地址在 Stack 取出数据，这次取出的是<code>3</code>，将其写入 EBX 寄存器。</p><h3 id="add指令"><a href="#add指令" class="headerlink" title="add指令"></a>add指令</h3><p><code>add</code>指令用于将两个运算子相加，并将结果写入第一个运算子。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add    %eax, %ebx</span><br></pre></td></tr></table></figure><p>上面的代码将 EAX 寄存器的值（即2）加上 EBX 寄存器的值（即3），得到结果5，再将这个结果写入第一个运算子 EAX 寄存器。</p><h3 id="pop指令"><a href="#pop指令" class="headerlink" title="pop指令"></a>pop指令</h3><p><code>pop</code>指令用于取出 Stack 最近一个写入的值（即最低位地址的值），并将这个值写入运算子指定的位置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pop %ebx</span><br></pre></td></tr></table></figure><p>上面的代码表示，取出 Stack 最近写入的值（即 EBX 寄存器的原始值），再将这个值写回 EBX 寄存器（因为加法已经做完了，EBX 寄存器用不到了）。</p><p>注意，<code>pop</code>指令还会将 ESP 寄存器里面的地址加4，即回收4个字节。</p><h3 id="ret指令"><a href="#ret指令" class="headerlink" title="ret指令"></a>ret指令</h3><p>ret指令用终止当前函数的执行, 将运行权交还给上层函数。也就是，当前函数的帧将被回收。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ret</span><br></pre></td></tr></table></figure><p>可以看到,该指令没有运算子</p><p>随着add_a_and_b函数终止执行,系统就回到刚才main函数中断的地方,继续往下执行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add %esp, 8</span><br></pre></td></tr></table></figure><p>上面的代码表示，将ESP寄存器里面的地址，手动加上8个字节，再写回ESP寄存器。这是因为ESP寄存器的是Stack的写入开始地址，前面的<code>pop</code>操作已经回收了4个字节，这里再回收8个字节，等于全部回收。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ret</span><br></pre></td></tr></table></figure><p>最后，<code>main</code>函数运行结束，<code>ret</code>指令退出程序执行。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>csapp_data</title>
      <link href="/2023/02/28/csapp-data/"/>
      <url>/2023/02/28/csapp-data/</url>
      
        <content type="html"><![CDATA[<p>有符号数一般都是<strong>算数右移</strong>，无符号数一般都是<strong>逻辑右移</strong>。算数右移时如果符号位为1，左端应该补1。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>computer_composition1</title>
      <link href="/2023/02/22/computer-composition1/"/>
      <url>/2023/02/22/computer-composition1/</url>
      
        <content type="html"><![CDATA[<h3 id="1-计算机发展历程"><a href="#1-计算机发展历程" class="headerlink" title="1. 计算机发展历程"></a>1. 计算机发展历程</h3><h4 id="1-1-硬件的发展"><a href="#1-1-硬件的发展" class="headerlink" title="1.1 硬件的发展"></a>1.1 硬件的发展</h4><ul><li>第一代计算机：(使用电子管)，</li><li>第二代计算机：(使用晶体管)，</li><li>第三代计算机：(使用较小规模的集成)，</li><li>第四代计算机：(使用较大规模的集成)，</li></ul><h4 id="1-2-软件的发展"><a href="#1-2-软件的发展" class="headerlink" title="1.2 软件的发展"></a>1.2 软件的发展</h4><p>各种软件及操作系统的蓬勃发展</p><h4 id="1-3-计算机的分类和发展方向"><a href="#1-3-计算机的分类和发展方向" class="headerlink" title="1.3 计算机的分类和发展方向"></a>1.3 计算机的分类和发展方向</h4><p>分为：</p><ul><li>电子模拟计算机和电子数字计算机。</li></ul><p>数字计算机又可以按照用途分为：</p><ul><li>专用计算机和通用计算机</li></ul><p>通用计算机又分为：</p><ul><li>巨型机、大型机、中型机、小型机、微型机和单片机6类。</li></ul><p>按照指令和数据流可以分为：</p><ul><li><p>单指令流和单数据流系统（SISD），即传统的冯·诺依曼体系结构。</p></li><li><p>单指令流和多数据流系统（SIMD），包括阵列处理器和向量处理器系统。</p></li><li><p>多指令流和单数据流系统（MISD），这种计算机实际上不存在。</p></li><li><p>多指令流和多数据流系统（MIMD），包括多处理器和计算机系统。</p></li></ul><h3 id="2-计算机系统层次结构"><a href="#2-计算机系统层次结构" class="headerlink" title="2. 计算机系统层次结构"></a>2. 计算机系统层次结构</h3><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222092519018.png" alt="image-20230222092519018"></p><h4 id="2-1-计算机系统的组成"><a href="#2-1-计算机系统的组成" class="headerlink" title="2.1 计算机系统的组成"></a>2.1 计算机系统的组成</h4><p>由硬件系统和软件系统共同构建</p><h4 id="2-2-计算机硬件的基本组成"><a href="#2-2-计算机硬件的基本组成" class="headerlink" title="2.2 计算机硬件的基本组成"></a>2.2 计算机硬件的基本组成</h4><ol><li>早期的冯诺依曼机</li></ol><hr><p>程序存储原理：指令以代码的形式事先输入到计算机的主存储器中，然后按其在存储器中的首地址执行程序的第一条指令，以后就按该程序的规定顺序执行其他指令，直至程序执行结束。即按地址访问并顺序执行指令</p><p>计算机按照此原理应具有5大功能：数据传送功能、数据存储功能、数据处理功能、操作控制功能、操作判断功能</p><hr><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222092112536.png" alt="image-20230222092112536"></p><ul><li>冯诺曼体系结构特点:</li></ul><ol><li>计算机硬件系统由五大部件组成(存储器、运算器、控制器、输出设备、输入设备)</li><li>指令和数据以同等地位存于存储器，可按地址寻访</li><li>指令和数据用二进制表示</li><li>指令由操作码和地址码组成</li><li>存储程序</li><li>以<strong>运算器</strong>为中心</li></ol><p>早期的冯·诺依曼机以运算器为中心，且是单处理机，<strong>最根本的特征</strong>是采用“<strong>存储程序</strong>”原理，基本工作方式是控制流驱动方式！</p><ol start="2"><li>现代计算机的组织结构</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222092120955.png" alt="image-20230222092120955"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222092642060.png" alt="image-20230222092642060"></p><p><strong>注意各箭头表示的含义</strong></p><h4 id="2-3-计算机的功能部件"><a href="#2-3-计算机的功能部件" class="headerlink" title="2.3 计算机的功能部件"></a>2.3 计算机的功能部件</h4><p>主机：主存、运算器、控制器</p><p><img src="https://img-blog.csdnimg.cn/2021011714554763.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhb2ppZV9kdWFu,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol><li>存储器的基本组成</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222103737433.png" alt="image-20230222103737433"></p><p>设 <strong>MAR &#x3D; 4位</strong>   <strong>MDR &#x3D; 8位</strong></p><p>则 存储单元16个    每个存储单元存储字长8位</p><ol start="2"><li>运算器的基本组成及操作过程</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222104447192.png" alt="image-20230222104447192"></p><ol start="3"><li>控制器的基本组成</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222105027251.png" alt="image-20230222105027251"></p><p>PC指向的是下一次执行的指令，IR指向的是当前要执行的指令</p><hr><p>一般将<strong>运算器和控制器集成</strong>到同一个芯片上，称为中央处理器(<strong>CPU</strong>)。<strong>CPU和主存储器</strong>共同构成<strong>主机</strong>，而除主机外的其他硬件装置(外存、I&#x2F;O设备等)统称为外部设备，简称<strong>外设</strong></p><ol start="4"><li>主机构造和执行指令</li></ol><p><img src="https://img-blog.csdnimg.cn/20210121090349881.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhb2ppZV9kdWFu,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>CPU和主存之间通过一组总线相连，总线中有地址、控制和数据3组信号线。MAR中的地址信息会直接送到地址线上，用于指向读&#x2F;写操作的主存存储单元；控制线中有读&#x2F;写信号线，指出数据是从CPU写入主存还是从主存读出到CPU,根据是读操作还是写操作来控制将MDR中的数据是直接送到数据线上还是将数据线上的数据接收到MDR中。</p><p><strong>主机完成一次指令的过程</strong>:</p><p>取数操作为例</p><p><img src="https://img-blog.csdnimg.cn/20210117172058667.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhb2ppZV9kdWFu,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222110244683.png" alt="image-20230222110244683"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222111002515.png" alt="image-20230222111002515"></p><h4 id="2-4-计算机硬件的性能指标"><a href="#2-4-计算机硬件的性能指标" class="headerlink" title="2.4 计算机硬件的性能指标"></a>2.4 计算机硬件的性能指标</h4><p><img src="https://img-blog.csdnimg.cn/20210117153043725.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhb2ppZV9kdWFu,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol><li>机器字长： cpu 一次能处理数据的位数，与cpu中的 <strong>寄存器位数</strong>有关，机器字长<strong>一般等于内部寄存器的大小</strong>，它决定了<strong>计算机的运算精度</strong>。</li></ol><hr><p>机器字长：计算机能直接处理的二进制数据的位数，机器字长一般等于内部寄存器的大小，它决定了计算机的运算精度。<br>指令字长：一个指令字中包含的二进制代码的位数。<br>存储字长：一个存储单元存储的二进制代码的长度。等于MDR的位数， 它们都必须是字节的整数倍。<br>数据字长：数据总线一次能传送信息的位数，它可以不等于MDR的位数。</p><p>指令字长一般取存储字长的整数倍，若指令字长等于存储字长的2倍，则需要2次访存来取出一条指令，因此取指周期为机器周期的2倍；若指令字长等于存储字长，则取指周期等于机器周期。</p><p>早期的计算机存储字长一般和机器的指令字长与数据字长相等，因此访问一次主存便可取出一条指令或一个数据。随着计算机的发展，指令字长可变，数据字长也可变，但它们必须都是字节的整数倍。</p><p>请注意64位操作系统是指特别为64位架构的计算机而设计的操作系统，它能够利用64位处理器的优势。但64位机器既可以使用64位操作系统，又可以使用32位操作系统。而32位处理器是无法使用64位操作系统的。</p><ol start="2"><li>运算速度：</li></ol><ul><li><p>主频</p></li><li><p>吉普森法<br>$$<br>T_M &#x3D; \sum_{i&#x3D;1}^{n}f_it_i<br>$$</p></li><li><p>MIPS   每秒执行百万条指令</p></li><li><p>FLOPS   每秒浮点运算次数</p></li><li><p>CPI   执行一条指令所需时钟周期数</p></li></ul><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230222112210072.png" alt="image-20230222112210072"></p><h4 id="2-5-计算机软件的分类"><a href="#2-5-计算机软件的分类" class="headerlink" title="2.5 计算机软件的分类"></a>2.5 计算机软件的分类</h4><ol><li>系统软件和应用软件</li><li>三个级别的语言</li></ol><h3 id="3-数据的表示与运算"><a href="#3-数据的表示与运算" class="headerlink" title="3.数据的表示与运算"></a>3.数据的表示与运算</h3><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/20210118085022452.png" alt="在这里插入图片描述"></p><h4 id="3-1数制与编码"><a href="#3-1数制与编码" class="headerlink" title="3.1数制与编码"></a>3.1数制与编码</h4><h5 id="1-进位计数值及其相互转化"><a href="#1-进位计数值及其相互转化" class="headerlink" title="1.进位计数值及其相互转化"></a>1.进位计数值及其相互转化</h5><p><strong>进制之间的相互转换，乘x除x法</strong>  </p><p><strong>1位16进制数等价于4位二进制数,    一位8进制数等价于3位二进制数</strong></p><h5 id="2-BCD码-用二进制数表示十进制"><a href="#2-BCD码-用二进制数表示十进制" class="headerlink" title="2.BCD码   用二进制数表示十进制"></a>2.BCD码   用二进制数表示十进制</h5><p><strong>这是为了表示0和9，所以1010~1111都是违法的</strong></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230307230630880.png" alt="image-20230307230630880"></p><h5 id="3-字符与字符串"><a href="#3-字符与字符串" class="headerlink" title="3.字符与字符串"></a>3.字符与字符串</h5><p>英文字符的表示  ASCII码</p><p>中文字符的表示  GDB等其他编码</p><p><strong>字符串的大小端模式</strong></p><hr><ul><li>大端模式：将数据的最高有效字节存放在低地址单元中</li><li>小端模式：将数据的最高有效字节存放在高地址单元中</li></ul><h5 id="4-校验码"><a href="#4-校验码" class="headerlink" title="4.校验码"></a>4.校验码</h5><p>待补充…</p><h4 id="3-2-定点数的表示和运算"><a href="#3-2-定点数的表示和运算" class="headerlink" title="3.2 定点数的表示和运算"></a>3.2 定点数的表示和运算</h4><p><strong>定点数和浮点数是一对相对的概念</strong></p><p><strong>整数的小数点表示在最后一位数字的后面，而小数的小数点标识在真值的符号位后面</strong></p><h5 id="1-定点数的移位规则"><a href="#1-定点数的移位规则" class="headerlink" title="1.定点数的移位规则"></a>1.定点数的移位规则</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/f770pmhzm4.png" alt="img"></p><ul><li>将+26的原码、补码和反码分别左移一位 结果是：[26]原 &#x3D; [26]反 &#x3D; [26]补 &#x3D; 0,0011010，根据规则，原码、反码、补码左移一位的结果是：0,010100 </li><li>将-26的原码、反码、补码分别左移一位 [-26]原 &#x3D; 1,0011010，左移一位：1,0110100 [-26]反 &#x3D; 1,1100101,左移一位：1,1001011 [-26]补 &#x3D; 1,1100110，左移一位：1,100110</li></ul><h5 id="2-定点数的加减法"><a href="#2-定点数的加减法" class="headerlink" title="2.定点数的加减法"></a>2.定点数的加减法</h5><p><strong>加法直接加，减法先变为加法后再计算</strong></p><p>[A+B]补 &#x3D; [A+B]补，[A-B]补 &#x3D; [A]补 + [-B]补</p><ul><li>A &#x3D; -1001，B &#x3D; -0101，求[A+B]补 [A+B]补 &#x3D; [A]补 + [B]补，[A]补 &#x3D; 1,0111，[B]补 &#x3D; 1,1011，所以最终的结果是：11,0010，但是这并非我们的最终结果，最终结果应该丢掉第一个1，即1,0010.为什么呢？这涉及到一个模2运算的问题，如果不想深究只需要记住，<strong>一个数只能有一个符号位不是吗？</strong> </li><li>A &#x3D; -1001，B&#x3D;-0101.求[A-B]补 [A-B]补 &#x3D; [A]补 + [-B]补，[A]补&#x3D;1,0111，[-B]补&#x3D;0,0101（求法：**[-B]补等于[B]补符号位和各位取反，末位加一**），这样得到最终的结果，丢弃掉多余的位即可。</li></ul><p><strong>溢出的判断</strong>：如果计算机的机器字长为4，那么能够表示的真值范围在-8~+7之间，如果两个数相加减，跳出了这个范围，则为溢出</p><p><strong>判断原则：</strong></p><ul><li><strong>不论加法还是减法，只要实际参与运算的两个数的符号相同，但是与最终的结果的符号相反，则为溢出。</strong>比如我们的第一个例子，两个参与运算的数的符号相同，且和最终结果的符号也相同，则这种情况就不是溢出。</li><li><strong>最终结果的两位符号位如果相同，则无溢出，如果不同则溢出</strong>，还是第一个例子，计算后的结果是11,0010，两位符号位相同，没有溢出。</li></ul><p>补码加法硬件配置</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308102214393.png" alt="image-20230308102214393"></p><h5 id="3-定点数的乘法"><a href="#3-定点数的乘法" class="headerlink" title="3.定点数的乘法"></a>3.定点数的乘法</h5><ol><li>原码乘法</li></ol><p>一位</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308104605231.png" alt="image-20230308104605231"></p><ul><li>绝对值运算</li><li>用移位的次数判断乘法是否结束</li><li>逻辑移位</li></ul><p>两位</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/7q991tvqow.png" alt="img"></p><ol start="2"><li>补码乘法</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308105741103.png" alt="image-20230308105741103"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308110246959.png" alt="image-20230308110246959"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308110321370.png" alt="image-20230308110321370"></p><h5 id="4-定点数的除法"><a href="#4-定点数的除法" class="headerlink" title="4.定点数的除法"></a>4.定点数的除法</h5><ol><li>恢复余数法</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308112711712.png" alt="image-20230308112711712"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308112728241.png" alt="image-20230308112728241"></p><ol start="2"><li>不恢复余数法</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308113227128.png" alt="image-20230308113227128"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230308113242477.png" alt="image-20230308113242477"></p><ul><li>上商n+1次</li><li>第一次上商判溢出</li><li>移n次，加n+1次</li><li>用移位的次数判断除法是否结束</li></ul><h4 id="3-3-浮点数的表示与运算"><a href="#3-3-浮点数的表示与运算" class="headerlink" title="3.3 浮点数的表示与运算"></a>3.3 浮点数的表示与运算</h4><p>用定点数表示数字时，会约定小数点的位置固定不变，整数部分和小数部分分别转换为二进制，就是定点数的结果。</p><p>但用定点数表示小数时，存在数值范围、精度范围有限的缺点，所以在计算机中，我们一般使用「浮点数」来表示小数。</p><p>其中「定点」指的是约定小数点位置固定不变。那浮点数的「浮点」就是指，其<strong>小数点的位置</strong>是可以是<strong>漂浮不定</strong>的。</p><p>例如十进制小数 8.345，用科学计数法表示，可以有多种方式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">8.345 = 8.345 * 10^0</span><br><span class="line">8.345 = 83.45 * 10^-1</span><br><span class="line">8.345 = 834.5 * 10^-2</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>用这种科学计数法的方式表示小数时，小数点的位置就变得「漂浮不定」了，这就是相对于定点数，浮点数名字的由来。</p><p>使用同样的规则，对于二进制数，我们也可以用科学计数法表示，也就是说把基数 10 换成 2 即可。</p><h5 id="浮点数如何表示数字："><a href="#浮点数如何表示数字：" class="headerlink" title="浮点数如何表示数字："></a>浮点数如何表示数字：</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">V = (-1)^S * M * R^E</span><br></pre></td></tr></table></figure><p>其中各个变量的含义如下：</p><ul><li>S：符号位，取值 0 或 1，决定一个数字的符号，0 表示正，1 表示负</li><li>M：尾数，用小数表示，例如前面所看到的 8.345 * 10^0，8.345 就是尾数</li><li>R：基数，表示十进制数 R 就是 10，表示二进制数 R 就是 2</li><li>E：指数，用整数表示，例如前面看到的 10^-1，-1 即是指数</li></ul><p>假设现在我们用 32 bit 表示一个浮点数，把以上变量按照一定规则，填充到这些 bit 上就可以了：</p><p><a href="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090678516816.jpg"><img src="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090678516816.jpg" alt="img"></a></p><p>假设我们定义如下规则来填充这些 bit：</p><ul><li>符号位 S 占 1 bit</li><li>指数 E 占 10 bit</li><li>尾数 M 占 21 bit</li></ul><p>按照这个规则，将十进制数 25.125 转换为浮点数，转换过程就是这样的（D代表十进制，B代表二进制）：</p><ol><li>整数部分：25(D) &#x3D; 11001(B)</li><li>小数部分：0.125(D) &#x3D; 0.001(B)</li><li>用二进制科学计数法表示：25.125(D) &#x3D; 11001.001(B) &#x3D; 1.1001001 * 2^4(B)</li></ol><p>所以符号位 S &#x3D; 0，尾数 M &#x3D; 1.001001(B)，指数 E &#x3D; 4(D) &#x3D; 100(B)。</p><p>按照上面定义的规则，填充到 32 bit 上，就是这样：</p><p><a href="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090693834968.jpg"><img src="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090693834968.jpg" alt="img"></a></p><p>上述规则只是随意设置的，若按新规则来，那浮点数表示出来也可以这样：</p><p><a href="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090693834973.jpg"><img src="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090693834973.jpg" alt="img"></a></p><p>可以看到：</p><ol><li>指数位越多，尾数位则越少，其表示的范围越大，但精度就会变差，反之，指数位越少，尾数位则越多，表示的范围越小，但精度就会变好</li><li>一个数字的浮点数格式，会因为定义的规则不同，得到的结果也不同，表示的范围和精度也有差异</li></ol><p>早期人们提出浮点数定义时，就是这样的情况，当时有很多计算机厂商，例如IBM、微软等，每个计算机厂商会定义自己的浮点数规则，</p><h5 id="浮点数标准"><a href="#浮点数标准" class="headerlink" title="浮点数标准"></a>浮点数标准</h5><p>直到1985年，IEEE 组织推出了浮点数标准，就是我们经常听到的 <strong>IEEE754 浮点数标准</strong>，这个标准统一了浮点数的表示形式，并提供了 2 种浮点格式：</p><ul><li>单精度浮点数 float：32 位，符号位 S 占 1 bit，指数 E 占 8 bit，尾数 M 占 23 bit</li><li>双精度浮点数 double：64 位，符号位 S 占 1 bit，指数 E 占 11 bit，尾数 M 占 52 bit</li></ul><p>为了使其表示的数字范围、精度最大化，浮点数标准还对指数和尾数进行了规定：</p><ol><li>尾数 M 的第一位总是 1（因为 1 &lt;&#x3D; M &lt; 2），因此这个 1 可以省略不写，它是个<strong>隐藏位</strong>，这样单精度 23 位尾数可以表示了 24 位有效数字，双精度 52 位尾数可以表示 53 位有效数字</li><li>指数 E 是个无符号整数，表示 float 时，一共占 8 bit，所以它的取值范围为 0 ~ 255。但因为指数可以是负的，所以规定在存入 E 时在它原本的值加上一个<strong>中间数</strong> 127，这样 E 的取值范围为 -127 ~ 128。表示 double 时，一共占 11 bit，存入 E 时加上中间数 1023，这样取值范围为 -1023 ~ 1024。</li></ol><p>除了规定尾数和指数位，还做了以下规定：</p><ul><li>指数 E 非全 0 且非全 1：规格化数字，按上面的规则正常计算</li><li>指数 E 全 0，尾数非 0：非规格化数，尾数隐藏位不再是 1，而是 0(M &#x3D; 0.xxxxx)，这样可以表示 0 和很小的数</li><li>指数 E 全 1，尾数全 0：正无穷大&#x2F;负无穷大（正负取决于 S 符号位）</li><li>指数 E 全 1，尾数非 0：NaN(Not a Number)</li></ul><p><a href="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090749848677.jpg"><img src="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090749848677.jpg" alt="3"></a></p><p><a href="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090749848677.jpg">3</a></p><h5 id="标准浮点数的表示"><a href="#标准浮点数的表示" class="headerlink" title="标准浮点数的表示"></a>标准浮点数的表示</h5><p>有了这个统一的浮点数标准，我们再把 25.125 转换为标准的 float 浮点数：</p><ol><li>整数部分：25(D) &#x3D; 11001(B)</li><li>小数部分：0.125(D) &#x3D; 0.001(B)</li><li>用二进制科学计数法表示：25.125(D) &#x3D; 11001.001(B) &#x3D; 1.1001001 * 2^4(B)</li></ol><p>所以 S &#x3D; 0，尾数 M &#x3D; 1.001001 &#x3D; 001001(去掉1，隐藏位)，指数 E &#x3D; 4 + 127(中间数) &#x3D; 135(D) &#x3D; 10000111(B)。填充到 32 bit 中，如下：</p><p><a href="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090678516830.jpg"><img src="https://kaito-blog-1253469779.cos.ap-beijing.myqcloud.com/2020/12/16090678516830.jpg" alt="img"></a></p><p>这就是标准 32 位浮点数的结果。</p><p>如果用 double 表示，和这个规则类似，指数位 E 用 11 bit 填充，尾数位 M 用 52 bit 填充即可。</p><p><strong>注：</strong></p><blockquote><p>float 1位符号位，8位阶码位（移码表示，偏移127，取值范围为1~254，0和255表示特殊值），23位尾数（隐含1），所以能表示的最大正整数为 （1+1-2^-23）* 2^127</p><p>double 1位符号位，11位阶码位（偏移 1023），52位尾数位（隐含1），所以能表示的最大正整数为（1+1-2^52）* 2^1023</p></blockquote><h5 id="浮点数的范围和精度"><a href="#浮点数的范围和精度" class="headerlink" title="浮点数的范围和精度"></a>浮点数的范围和精度</h5><p>以单精度浮点数 float 为例，它能表示的最大二进制数为 +1.1.11111…1 * 2^127（小数点后23个1），而二进制 1.11111…1 ≈ 2，所以 float 能表示的最大数为 2^128 &#x3D; 3.4 * 10^38，即 float 的表示范围为：-3.4 * 10^38 ~ 3.4 * 10 ^38。</p><p><strong>精度：</strong></p><p>float 能表示的最小正二进制数为 0.0000….1（小数点后22个0，1个1），用十进制数表示就是 1&#x2F;2^23。</p><p>用同样的方法可以算出，double 能表示的最大二进制数为 +1.111…111（小数点后52个1） * 2^1023 ≈ 2^1024 &#x3D; 1.79 * 10^308，所以 double 能表示范围为：-1.79 * 10^308 ~ +1.79 * 10^308。</p><p>double 的最小精度为：0.0000…1(51个0，1个1)，用十进制表示就是 1&#x2F;2^52。</p><p>虽然浮点数的范围和精度也有限，但其范围和精度<strong>都已非常之大</strong></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315112408077.png" alt="image-20230315112408077"></p><p>总结</p><hr><ol><li>浮点数一般用科学计数法表示</li><li>把科学计数法中的变量，填充到固定 bit 中，即是浮点数的结果</li><li>在浮点数提出的早期，各个计算机厂商各自制定自己的浮点数规则，导致不同厂商对于同一个数字的浮点数表示各不相同，在计算时还需要先进行转换才能进行计算</li><li>后来 IEEE 组织提出了浮点数的标准，统一了浮点数的格式，并规定了单精度浮点数 float 和双精度浮点数 double，从此以后各个计算机厂商统一了浮点数的格式，一直延续至今</li><li>浮点数在表示小数时，由于十进制小数在转换为二进制时，存在无法精确转换的情况，而在固定 bit 的计算机中存储时会被截断，所以浮点数表示小数可能存在精度损失</li><li>浮点数在表示一个数字时，其范围和精度非常大，所以我们平时使用的小数，在计算机中通常用浮点数来存储</li></ol><blockquote><p>补充: <a href="https://blog.csdn.net/qq_43855740/article/details/104721619">负数补码表示范围以及规格化数</a></p></blockquote><h4 id="3-4-算数逻辑单元（ALU）"><a href="#3-4-算数逻辑单元（ALU）" class="headerlink" title="3.4 算数逻辑单元（ALU）"></a>3.4 算数逻辑单元（ALU）</h4><h5 id="1-ALU电路"><a href="#1-ALU电路" class="headerlink" title="1.ALU电路"></a>1.ALU电路</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315133712464.png" alt="image-20230315133712464"></p><p>ALU电路是一个<strong>组合逻辑电路</strong>，不含记忆功能，所以要接入寄存器</p><h5 id="2-运算器的组成"><a href="#2-运算器的组成" class="headerlink" title="2. 运算器的组成"></a>2. 运算器的组成</h5><p><img src="https://img-blog.csdnimg.cn/20210424213852316.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1F1YW50dW1Zb3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210424214104744.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1F1YW50dW1Zb3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h5 id="3-快速进位链"><a href="#3-快速进位链" class="headerlink" title="3.快速进位链"></a>3.快速进位链</h5><h6 id="a-并行加法器"><a href="#a-并行加法器" class="headerlink" title="a. 并行加法器"></a>a. 并行加法器</h6><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315134834037.png" alt="image-20230315134834037"></p><h6 id="b-串行进位链"><a href="#b-串行进位链" class="headerlink" title="b.串行进位链"></a>b.串行进位链</h6><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315135151661.png" alt="image-20230315135151661"></p><h6 id="c-并行进位链"><a href="#c-并行进位链" class="headerlink" title="c.并行进位链"></a>c.并行进位链</h6><p>先行进位链：电路复杂</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315135518513.png" alt="image-20230315135518513"></p><p>1）单重分组跳跃进位链</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315140854991.png" alt="image-20230315140854991"></p><ol start="2"><li>双重分组跳跃进位链</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315140918035.png" alt="image-20230315140918035"></p><p><strong>进位分析</strong></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315140933992.png" alt="image-20230315140933992"></p><p><strong>大组进位线路</strong></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315141040662.png" alt="image-20230315141040662"></p><p><strong>小组进位线路</strong></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315141122347.png" alt="image-20230315141122347"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315141142891.png" alt="image-20230315141142891"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315141154305.png" alt="image-20230315141154305"></p><h3 id="4-系统总线"><a href="#4-系统总线" class="headerlink" title="4. 系统总线"></a>4. 系统总线</h3><p>总线概念</p><p>如果两两单独连接，则连接的网络十分庞大，而且难以扩展</p><h4 id="1-总线是各个部件共享的传输介质"><a href="#1-总线是各个部件共享的传输介质" class="headerlink" title="1.总线是各个部件共享的传输介质"></a>1.总线是各个部件共享的传输介质</h4><p>总线上可以进行<strong>串行</strong>和<strong>并行</strong>两种传输方式</p><h4 id="2-总线结构"><a href="#2-总线结构" class="headerlink" title="2.总线结构"></a>2.总线结构</h4><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227100514706.png" alt="image-20230227100514706"></p><p>这种结构可扩展性强，但是任意时刻只能进行两个部件之间的信息传递，严重影响效率</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227100531192.png" alt="image-20230227100531192"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227100543225.png" alt="image-20230227100543225"></p><p><strong>现代技术中 通常情况下这两条总线也很难同时工作</strong></p><h4 id="3-总线分类"><a href="#3-总线分类" class="headerlink" title="3.总线分类"></a>3.总线分类</h4><ol><li>片内总线： 芯片内部的总线</li><li>系统总线： <strong>注意机器字长和存储字长的概念</strong></li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227101014280.png" alt="image-20230227101014280"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227101422395.png" alt="image-20230227101422395"></p><h4 id="4-总线特性及性能指标"><a href="#4-总线特性及性能指标" class="headerlink" title="4.总线特性及性能指标"></a>4.总线特性及性能指标</h4><ol><li>总线的物理实现</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227182408824.png" alt="image-20230227182408824"></p><p><strong>总线是印刷在主板上的</strong>，图中cpu插板等都是在主线上的接口</p><ol start="2"><li>总线的特性</li></ol><p>a. 机械特性：<strong>尺寸</strong>、形状、<strong>管脚数</strong>及<strong>排列顺序</strong></p><p>b. 电气特性： <strong>传输方向</strong> 和有效的 <strong>电平</strong>范围</p><p>c. 功能特性： 每根传输线的 <strong>功能</strong>（地址、数据、控制）</p><p>d. 时间特性： 信号的<strong>时序</strong>关系</p><ol start="3"><li>总线的性能指标</li></ol><p>总线宽度：数据线的根数</p><p>标准传输率： 每秒传输的最大字节数(MBps)</p><p>时钟同步&#x2F;异步： 同步、不同步</p><p>总线复用：地址线与数据线复用</p><p>信号线数：地址线、数据线和控制线的总和</p><p>总线控制方式：突发、自动、仲裁、逻辑、计数</p><p>其他指标：负载能力</p><ol start="4"><li>总线标准</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227195630263.png" alt="image-20230227195630263"></p><h4 id="5-多总线结构"><a href="#5-多总线结构" class="headerlink" title="5.多总线结构"></a>5.多总线结构</h4><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227201405612.png" alt="image-20230227201405612"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227205904087.png" alt="image-20230227205904087"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227210134573.png" alt="image-20230227210134573"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227210156759.png" alt="image-20230227210156759"></p><h4 id="6-总线控制"><a href="#6-总线控制" class="headerlink" title="6.总线控制"></a>6.总线控制</h4><h5 id="一、总线判优控制"><a href="#一、总线判优控制" class="headerlink" title="一、总线判优控制"></a>一、总线判优控制</h5><ol><li><p>主设备（模块） 对总线有 <strong>控制权</strong></p><p>从设备                <strong>响应</strong> 从主设备发来的总线命令</p></li></ol><pre class="mermaid">graph LRA[总线判优控制] --> B1[集中式]A --> B2[分布式]B1 --> c1[链式查询]B1 --> c2[计数器定时查询]B1 --> c3[独立请求方式]</pre><p>2.查询方式</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227214415925.png" alt="image-20230227214415925"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227214440898.png" alt="image-20230227214440898"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227221835173.png" alt="image-20230227221835173"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227221931763.png" alt="image-20230227221931763"></p><h5 id="二、总线通信控制"><a href="#二、总线通信控制" class="headerlink" title="二、总线通信控制"></a>二、总线通信控制</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227222023283.png" alt="image-20230227222023283"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227222036808.png" alt="image-20230227222036808"></p><ol><li>同步式数据输入</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227222118063.png" alt="image-20230227222118063"></p><ol start="2"><li>同步式数据输出</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227223345390.png" alt="image-20230227223345390"></p><ol start="3"><li>异步通信</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227223642412.png" alt="image-20230227223642412"></p><ol start="4"><li>半同步通信</li></ol><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227224130590.png" alt="image-20230227224130590"></p><p>在同步通信之间插入<strong>WAIT信号</strong></p><ol start="5"><li>总结上述三种通信的共同点<br>一个总线传输周期（以输入数据为例）</li></ol><ul><li>主模块发地址、命令       占用总线</li><li>从模块准备数据               不占用总线总线空闲</li><li>从模块向主模块发数据   占用总线</li></ul><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230227224751442.png" alt="image-20230227224751442"> </p><p>分离式通信特点</p><ul><li>各模块有权申请占用总线</li><li>采用同步方式通信，不等对方回答</li><li>各模块准备数据时，不占用总线</li><li>总线被占用时，无空闲</li><li><u>充分提高了总线的有效占用</u></li></ul><h3 id="5-存储器"><a href="#5-存储器" class="headerlink" title="5.存储器"></a>5.存储器</h3><h4 id="5-1存储器的分类"><a href="#5-1存储器的分类" class="headerlink" title="5.1存储器的分类"></a>5.1存储器的分类</h4><p>按在计算机中的作用对存储器分类：</p><ul><li><p>主存储器,简称<strong>主存</strong>。CPU可以直接随机地对其进行访问，也可以和高速缓存器及辅助存储器交换数据。</p></li><li><p>辅助存储器,简称<strong>辅存</strong>，不能与CPU直接相连，用来存放当前暂时不用的程序和数据</p></li><li><p><strong>高速缓冲存储器</strong>, 位于<strong>主存和CPU之间</strong>，用来存放正在执行的程序段和数据，作为cpu和主存之间的缓冲</p></li><li><p><strong>Flash Memory</strong>：可作为主存和辅存之间的临时存储器，也可以单独作为高性能存储器，为半导体元件，如U盘</p></li></ul><p>按存储介质分类：</p><p>磁表面存储器（磁盘，磁带），磁心存储器半导体存储器（MOS型存储器，双极存储器）和光存储器（光盘）。</p><p>按存取方式分类：</p><ul><li><p>随机存储器（<strong>RAM</strong>）。存储器的任何一个存储单元的内容都可以随机存取，而且存取时间与存取单元的物理位置无关，<strong>主要用作主存或高速缓冲存储器</strong>。</p></li><li><p>只读存储器（ROM）。存储器的内容只能随机读出而不能写入。即使断电，内容也不会丢失。</p></li><li><p>串行访问存储器。对存储单元进行读&#x2F;写操作时，需按其物理位置的先后顺序寻址，包括<strong>顺序存取存储器（如磁带）</strong>与<strong>直接存取存储器（如磁盘）</strong>。</p></li></ul><p>按信息的可保存性分类：</p><p>断电后，存储信息即消失的存储器，称为易失性存储器，如RAM。断电后信息仍然保持的存储器，称为非易失性存储器，如ROM，磁表面存储器和光存储器。若某个存储单元所存储的信息被读出时，原存储信息被破坏，则称为破坏性读出；若读出时，被读单元原存储信息不被破坏，则称为非破坏性读出。具有破坏性读出性能的存储器，每次读出操作后，必须紧接一个再生的操作，以便恢复被破坏的信息。</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315141745772.png" alt="image-20230315141745772"></p><h4 id="5-2存储器的性能指标"><a href="#5-2存储器的性能指标" class="headerlink" title="5.2存储器的性能指标"></a>5.2存储器的性能指标</h4><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/20210118125344491.png" alt="在这里插入图片描述"></p><h4 id="5-3存储器的层次结构"><a href="#5-3存储器的层次结构" class="headerlink" title="5.3存储器的层次结构"></a>5.3存储器的层次结构</h4><p>1.三个主要特征的关系</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315142102827.png" alt="image-20230315142102827"></p><p>2.缓存一主存层次和主存一辅存层次</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230315142136765.png" alt="image-20230315142136765"></p><p>cpu提升速度远快于主存，所以在二者之间加了一个缓存</p><p>cpu可以通过直接访问缓存从而访问主存的信息</p><blockquote><p>主存和缓存之间通过硬件直接设计连接到一起，不需要机器语言程序员考虑</p><p>而主存和辅存之间则软硬件结合</p></blockquote><h4 id="5-4主存"><a href="#5-4主存" class="headerlink" title="5.4主存"></a>5.4主存</h4><h5 id="1-主存的基本组成"><a href="#1-主存的基本组成" class="headerlink" title="1.主存的基本组成"></a>1.主存的基本组成</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230320210244821.png" alt="image-20230320210244821"></p><h5 id="2-主存和cpu的关系"><a href="#2-主存和cpu的关系" class="headerlink" title="2.主存和cpu的关系"></a>2.主存和cpu的关系</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230320214104344.png" alt="image-20230320214104344"></p><h5 id="3-主存中存储单元地址的分配"><a href="#3-主存中存储单元地址的分配" class="headerlink" title="3.主存中存储单元地址的分配"></a>3.主存中存储单元地址的分配</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230320214340051.png" alt="image-20230320214340051"></p><h5 id="4-主存的技术指标"><a href="#4-主存的技术指标" class="headerlink" title="4.主存的技术指标"></a>4.主存的技术指标</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230320214403647.png" alt="image-20230320214403647"></p><h4 id="5-5半导体存储芯片"><a href="#5-5半导体存储芯片" class="headerlink" title="5.5半导体存储芯片"></a>5.5半导体存储芯片</h4><h5 id="1-半导体存储芯片的基本结构"><a href="#1-半导体存储芯片的基本结构" class="headerlink" title="1.半导体存储芯片的基本结构"></a>1.半导体存储芯片的基本结构</h5><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230320221448301.png" alt="image-20230320221448301"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230320222022340.png" alt="image-20230320222022340"></p><h5 id="2-半导体芯片译码驱动法"><a href="#2-半导体芯片译码驱动法" class="headerlink" title="2.半导体芯片译码驱动法"></a>2.半导体芯片译码驱动法</h5><h6 id="1-线选法"><a href="#1-线选法" class="headerlink" title="1.线选法"></a>1.线选法</h6><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230321000200109.png" alt="image-20230321000200109"></p><h6 id="2-重合法"><a href="#2-重合法" class="headerlink" title="2.重合法"></a>2.重合法</h6><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230321000229610.png" alt="image-20230321000229610"></p><h5 id="3-随机存取存储器（RAM）"><a href="#3-随机存取存储器（RAM）" class="headerlink" title="3.随机存取存储器（RAM）"></a>3.随机存取存储器（RAM）</h5><h6 id="1-静态RAM-SRAM"><a href="#1-静态RAM-SRAM" class="headerlink" title="1.静态RAM(SRAM)"></a>1.静态RAM(SRAM)</h6><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230321162919992.png" alt="image-20230321162919992"></p><p><strong>读写操作</strong></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327155902669.png" alt="image-20230327155902669"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327155919892.png" alt="image-20230327155919892"></p><h6 id="2-动态RAM-DRAM"><a href="#2-动态RAM-DRAM" class="headerlink" title="2.动态RAM(DRAM)"></a>2.动态RAM(DRAM)</h6><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327161252954.png" alt="image-20230327161252954"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327162525925.png" alt="image-20230327162525925"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327162606881.png" alt="image-20230327162606881"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327162626230.png" alt="image-20230327162626230"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327162659123.png" alt="image-20230327162659123"></p><h6 id="3-DRAM和SRAM比较"><a href="#3-DRAM和SRAM比较" class="headerlink" title="3.DRAM和SRAM比较"></a>3.DRAM和SRAM比较</h6><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230327162751135.png" alt="image-20230327162751135"></p><h5 id="4-只读存储器（ROM）"><a href="#4-只读存储器（ROM）" class="headerlink" title="4.只读存储器（ROM）"></a>4.只读存储器（ROM）</h5><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230327163640770.png" alt="image-20230327163640770"></p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230327163653828.png" alt="image-20230327163653828"></p><h5 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h5><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230327164250734.png" alt="image-20230327164250734"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 计算机组成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch_learning</title>
      <link href="/2023/01/05/pytorch-learning/"/>
      <url>/2023/01/05/pytorch-learning/</url>
      
        <content type="html"><![CDATA[<p>本文参考：<a href="https://www.pytorchmaster.com/">https://www.pytorchmaster.com/</a></p><h3 id="torch函数"><a href="#torch函数" class="headerlink" title="torch函数"></a>torch函数</h3><h4 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat()"></a>torch.cat()</h4><p>函数目的： 在给定维度上对输入的张量序列seq 进行连接操作。</p><p><code>outputs = torch.cat(inputs, dim=?) → Tensor</code></p><p><strong>参数</strong></p><ul><li>inputs : 待连接的张量序列，可以是任意相同<code>Tensor</code>类型的python 序列</li><li>dim : 选择的扩维, 必须在<code>0</code>到<code>len(inputs[0])</code>之间，沿着此维连接张量序列。</li></ul><p><strong>重点</strong></p><ol><li>输入数据必须是序列，序列中数据是任意相同的<code>shape</code>的同类型<code>tensor</code></li><li>维度不可以超过输入数据的任一个张量的维度</li></ol><p><strong>例子</strong></p><ol><li>准备数据，每个的shape都是[2,3]</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x1 = torch.tensor([[<span class="number">11</span>,<span class="number">21</span>,<span class="number">31</span>],[<span class="number">21</span>,<span class="number">31</span>,<span class="number">41</span>]],dtype=torch.<span class="built_in">int</span>)</span><br><span class="line">x1.shape <span class="comment"># torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x2 = torch.tensor([[<span class="number">12</span>,<span class="number">22</span>,<span class="number">32</span>],[<span class="number">22</span>,<span class="number">32</span>,<span class="number">42</span>]],dtype=torch.<span class="built_in">int</span>)</span><br><span class="line">x2.shape  <span class="comment"># torch.Size([2, 3])</span></span><br></pre></td></tr></table></figure><ol start="2"><li>合成inputs</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27; inputs为２个形状为[2 , 3]的矩阵 &#x27;</span></span><br><span class="line">inputs = [x1, x2]</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br><span class="line"><span class="string">&#x27;打印查看&#x27;</span></span><br><span class="line">[tensor([[<span class="number">11</span>, <span class="number">21</span>, <span class="number">31</span>],</span><br><span class="line">         [<span class="number">21</span>, <span class="number">31</span>, <span class="number">41</span>]], dtype=torch.int32),</span><br><span class="line"> tensor([[<span class="number">12</span>, <span class="number">22</span>, <span class="number">32</span>],</span><br><span class="line">         [<span class="number">22</span>, <span class="number">32</span>, <span class="number">42</span>]], dtype=torch.int32)]</span><br></pre></td></tr></table></figure><ol start="3"><li>查看结果, 测试不同的dim拼接结果</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In    [<span class="number">1</span>]: torch.cat(inputs, dim=<span class="number">0</span>).shape</span><br><span class="line">Out   [<span class="number">1</span>]: torch.Size([<span class="number">4</span>,  <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">In    [<span class="number">2</span>]: torch.cat(inputs, dim=<span class="number">1</span>).shape</span><br><span class="line">Out   [<span class="number">2</span>]: torch.Size([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">In    [<span class="number">3</span>]: torch.cat(inputs, dim=<span class="number">2</span>).shape</span><br><span class="line">IndexError: Dimension out of <span class="built_in">range</span> (expected to be <span class="keyword">in</span> <span class="built_in">range</span> of [-<span class="number">2</span>, <span class="number">1</span>], but got <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h4 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack()"></a>torch.stack()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">T1 = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">T2 = torch.tensor([[<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>],</span><br><span class="line">                [<span class="number">40</span>, <span class="number">50</span>, <span class="number">60</span>],</span><br><span class="line">                [<span class="number">70</span>, <span class="number">80</span>, <span class="number">90</span>]])</span><br><span class="line"></span><br><span class="line">T3 = torch.stack((T1,T2),dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(T3.shape)</span><br><span class="line"><span class="built_in">print</span>(T3)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([2, 3, 3])</span></span><br><span class="line"><span class="string">tensor([[[ 1,  2,  3],</span></span><br><span class="line"><span class="string">         [ 4,  5,  6],</span></span><br><span class="line"><span class="string">         [ 7,  8,  9]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[10, 20, 30],</span></span><br><span class="line"><span class="string">         [40, 50, 60],</span></span><br><span class="line"><span class="string">         [70, 80, 90]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">T4 = torch.stack((T1,T2),dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(T4.shape)</span><br><span class="line"><span class="built_in">print</span>(T4)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([3, 2, 3])</span></span><br><span class="line"><span class="string">tensor([[[ 1,  2,  3],</span></span><br><span class="line"><span class="string">         [10, 20, 30]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 4,  5,  6],</span></span><br><span class="line"><span class="string">         [40, 50, 60]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 7,  8,  9],</span></span><br><span class="line"><span class="string">         [70, 80, 90]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">T5 = torch.stack((T1,T2),dim=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(T5.shape)</span><br><span class="line"><span class="built_in">print</span>(T5)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([3, 3, 2])</span></span><br><span class="line"><span class="string">tensor([[[ 1, 10],</span></span><br><span class="line"><span class="string">         [ 2, 20],</span></span><br><span class="line"><span class="string">         [ 3, 30]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 4, 40],</span></span><br><span class="line"><span class="string">         [ 5, 50],</span></span><br><span class="line"><span class="string">         [ 6, 60]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 7, 70],</span></span><br><span class="line"><span class="string">         [ 8, 80],</span></span><br><span class="line"><span class="string">         [ 9, 90]]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="torch-ones-like"><a href="#torch-ones-like" class="headerlink" title="torch.ones_like()"></a>torch.ones_like()</h4><h4 id="torch-zeros-like"><a href="#torch-zeros-like" class="headerlink" title="torch.zeros_like()"></a>torch.zeros_like()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># 生成与input形状相同、元素全为1的张量</span></span><br><span class="line">a = torch.ones_like(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 生成与input形状相同、元素全为0的张量</span></span><br><span class="line">b = torch.zeros_like(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.1362, 0.6439, 0.3817],</span></span><br><span class="line"><span class="string">        [0.0971, 0.3498, 0.8780]])</span></span><br><span class="line"><span class="string">tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]])</span></span><br><span class="line"><span class="string">tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="torch-range"><a href="#torch-range" class="headerlink" title="torch.range()"></a>torch.range()</h4><h4 id="torch-arange"><a href="#torch-arange" class="headerlink" title="torch.arange()"></a>torch.arange()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y=torch.<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.dtype</span><br><span class="line">torch.float32</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z=torch.arange(<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.dtype</span><br><span class="line">torch.int64</span><br></pre></td></tr></table></figure><p>注意：</p><p>torch.range必须有begin和end值</p><p>但是torch.arange可以只有单值或设置间隔值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; torch.arange(<span class="number">4</span>)</span><br><span class="line">&gt;&gt; tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">&gt;&gt; torch.arange(<span class="number">1</span>,<span class="number">0.6</span>,-<span class="number">0.1</span>)</span><br><span class="line">&gt;&gt; tensor([<span class="number">1.0000</span>, <span class="number">0.9000</span>, <span class="number">0.8000</span>, <span class="number">0.7000</span>])</span><br></pre></td></tr></table></figure><h4 id="torch-normal"><a href="#torch-normal" class="headerlink" title="torch.normal()"></a>torch.normal()</h4><p>原型：<code>normal(mean, std, *, generator=None, out=None)</code></p><p>该函数返回从单独的<a href="https://so.csdn.net/so/search?q=%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83&spm=1001.2101.3001.7020">正态分布</a>中提取的随机数的张量，该正态分布的均值是mean，标准差是std。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.normal(mean=torch.arange(<span class="number">4.</span>),std=torch.arange(<span class="number">1.</span>,<span class="number">0.6</span>,-<span class="number">0.1</span>)).reshape(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[-1.4455,  0.9446],</span></span><br><span class="line"><span class="string">        [ 3.2138,  3.3914]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h3><h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><p>基本上趋同于numpy.array，但是<strong>不支持str</strong>!!!   包括：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.float64(torch.double),</span><br><span class="line">torch.float32(torch.<span class="built_in">float</span>),</span><br><span class="line">torch.float16,</span><br><span class="line">torch.int64(torch.long),</span><br><span class="line">torch.int32(torch.<span class="built_in">int</span>),</span><br><span class="line">torch.int16,</span><br><span class="line">torch.int8,</span><br><span class="line">torch.uint8,</span><br><span class="line">torch.<span class="built_in">bool</span></span><br></pre></td></tr></table></figure><p>一般神经网络都是用<code>torch.float32</code>类型</p><p>几种构造方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动推断</span></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>)</span><br><span class="line"><span class="built_in">print</span>(a,a.dtype)</span><br><span class="line"><span class="comment"># tensor(2.) torch.float32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定数据类型</span></span><br><span class="line">b = torch.tensor(<span class="number">2.0</span>, dtype=torch.double)</span><br><span class="line"><span class="built_in">print</span>(b,b.dtype)</span><br><span class="line"><span class="comment"># tensor(2.,dtype=torch.float64) torch.float64</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用特定构造函数 &amp; numpt转tensor</span></span><br><span class="line">c = torch.IntTensor(<span class="number">1</span>)</span><br><span class="line">d = torch.Tensor(np.array(<span class="number">2.0</span>))  <span class="comment">#等价于torch.FloatTensor</span></span><br><span class="line">e = torch.BoolTensor(np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(c,c.dtype)</span><br><span class="line"><span class="built_in">print</span>(d,d.dtype)</span><br><span class="line"><span class="built_in">print</span>(e,e.dtype)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([5], dtype=torch.int32) torch.int32</span></span><br><span class="line"><span class="string">tensor(2.) torch.float32</span></span><br><span class="line"><span class="string">tensor([ True, False,  True, False]) torch.bool</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同类型转换</span></span><br><span class="line">i = torch.tensor(<span class="number">1</span>); <span class="built_in">print</span>(i,i.dtype)</span><br><span class="line">x = i.<span class="built_in">float</span>(); <span class="built_in">print</span>(x,x.dtype) <span class="comment">#调用 float方法转换成浮点类型</span></span><br><span class="line">y = i.<span class="built_in">type</span>(torch.<span class="built_in">float</span>); <span class="built_in">print</span>(y,y.dtype) <span class="comment">#使用type函数转换成浮点类型</span></span><br><span class="line">z = i.type_as(x);<span class="built_in">print</span>(z,z.dtype) <span class="comment">#使用type_as方法转换成某个Tensor相同类型</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(1) torch.int64</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="张量的维度"><a href="#张量的维度" class="headerlink" title="张量的维度"></a>张量的维度</h4><p>不同类型数据会是不同的维度，<strong>有几层中括号就是多少维张量</strong></p><hr><p>标量为0维张量，向量为1维张量，矩阵为2维张量，彩色图像有rgb三个通道为3维张量，视频还有时间维表示为4维张量。</p><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scalar = torch.tensor(<span class="literal">True</span>) <span class="comment"># 0维</span></span><br><span class="line">vector = torch.tensor([<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>]) <span class="comment">#向量，1维张量</span></span><br><span class="line">matrix = torch.tensor([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]]) <span class="comment">#矩阵, 2维张量</span></span><br><span class="line">tensor3 = torch.tensor([[[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]],[[<span class="number">5.0</span>,<span class="number">6.0</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]]])  <span class="comment"># 3维张量</span></span><br><span class="line">tensor4 = torch.tensor([[[[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">2.0</span>,<span class="number">2.0</span>]],[[<span class="number">3.0</span>,<span class="number">3.0</span>],[<span class="number">4.0</span>,<span class="number">4.0</span>]]],</span><br><span class="line">                        [[[<span class="number">5.0</span>,<span class="number">5.0</span>],[<span class="number">6.0</span>,<span class="number">6.0</span>]],[[<span class="number">7.0</span>,<span class="number">7.0</span>],[<span class="number">8.0</span>,<span class="number">8.0</span>]]]])  <span class="comment"># 4维张量</span></span><br></pre></td></tr></table></figure><h4 id="张量的尺寸"><a href="#张量的尺寸" class="headerlink" title="张量的尺寸"></a>张量的尺寸</h4><ul><li>使用<code>shape</code>属性或者<code>size()</code>方法查看张量在每一维的长度</li><li>使用<code>view()</code>方法改变张量的尺寸,view()和numpy中的reshape很像，所以<strong>view()失败可以直接使用reshape</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有些操作会让张量存储结构扭曲，直接使用view会失败，可以用reshape方法</span></span><br><span class="line">matrix26 = torch.arange(<span class="number">0</span>,<span class="number">12</span>).view(<span class="number">2</span>,<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(matrix26)       <span class="comment"># tensor([[ 0,  1,  2,  3,  4,  5],</span></span><br><span class="line">          <span class="comment">#[ 6,  7,  8,  9, 10, 11]])</span></span><br><span class="line"><span class="built_in">print</span>(matrix26.shape) <span class="comment"># torch.Size([2, 6])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转置操作让张量存储结构扭曲</span></span><br><span class="line">matrix62 = matrix26.t()</span><br><span class="line"><span class="built_in">print</span>(matrix62.is_contiguous()) <span class="comment"># False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接使用view方法会失败，可以使用reshape方法</span></span><br><span class="line"><span class="comment">#matrix34 = matrix62.view(3,4) #error!</span></span><br><span class="line">matrix34 = matrix62.reshape(<span class="number">3</span>,<span class="number">4</span>) <span class="comment">#等价于matrix34 = matrix62.contiguous().view(3,4)</span></span><br><span class="line"><span class="built_in">print</span>(matrix34) </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 0,  6,  1,  7],</span></span><br><span class="line"><span class="string">        [ 2,  8,  3,  9],</span></span><br><span class="line"><span class="string">        [ 4, 10,  5, 11]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><strong>is_contiguous()：Tensor底层一维数组元素的存储顺序与Tensor按行优先一维展开的元素顺序是否一致</strong></p><p>行有限列有限博客：<a href="https://zhuanlan.zhihu.com/p/64551412">https://zhuanlan.zhihu.com/p/64551412</a></p><h4 id="张量和numpy数组"><a href="#张量和numpy数组" class="headerlink" title="张量和numpy数组"></a>张量和numpy数组</h4><p>numpy –&gt; tensor</p><ul><li>用numpy方法从Tensor得到numpy数组</li><li>用torch.from_numpy从numpy数组得到Tensor</li></ul><p>这两种方法关联的tensor和numpy数组是共享数据内存的，改变一个也会改变另一个。当然需要的话可以通过张量的<code>clone</code>方法拷贝张量中断这种关联。</p><p>此外可以使用<code>item</code>方法从标量张量得到对应的Python数值</p><p>​使用<code>tolist</code>方法从张量得到对应的Python数值列表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ndarray-&gt;tensor</span></span><br><span class="line">arr = np.zeros(<span class="number">3</span>)</span><br><span class="line">tensor = torch.from_numpy(arr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor-&gt;ndarray</span></span><br><span class="line">tensor = torch.zeros(<span class="number">3</span>)</span><br><span class="line">arr = tensor.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以用clone() 方法拷贝张量，中断这种关联</span></span><br><span class="line">tensor = torch.zeros(<span class="number">3</span>)</span><br><span class="line">arr = tensor.clone().numpy() <span class="comment"># 也可以使用tensor.data.numpy()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># item方法和tolist方法可以将张量转换成Python数值和数值列表</span></span><br><span class="line">scalar = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">s = scalar.item()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(s),<span class="string">&#x27; &#x27;</span>,s)</span><br><span class="line"><span class="comment"># &lt;class &#x27;float&#x27;&gt; 1.0</span></span><br><span class="line"></span><br><span class="line">tensor = torch.rand(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">t = tensor.tolist()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(t),<span class="string">&#x27; &#x27;</span>,t)</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;list&#x27;</span>&gt; [[<span class="number">0.8211846351623535</span>, <span class="number">0.20020723342895508</span>], [<span class="number">0.011571824550628662</span>, <span class="number">0.2906131148338318</span>]]</span><br></pre></td></tr></table></figure><h3 id="autograd自动微分"><a href="#autograd自动微分" class="headerlink" title="autograd自动微分"></a>autograd自动微分</h3><p>神经网络通常依靠反向传播求梯度更新网络参数，pytorch通过反向传播backward方法实现梯度计算，可以调用torch.autograd.grad函数来实现梯度计算，这就是Pytorch的自动微分机制</p><h4 id="backward求导数"><a href="#backward求导数" class="headerlink" title="backward求导数"></a>backward求导数</h4><p>backward 方法通常在一个标量张量上调用，该方法求得的梯度将存在对应自变量张量的grad属性下。</p><p>如果调用的张量非标量，则要传入一个和它同形状 的gradient参数张量。</p><p>相当于用该gradient参数张量与调用张量作向量点乘，得到的标量结果再反向传播。</p><ol><li>标量的反向传播</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line">dy_dx = x.grad</span><br><span class="line"><span class="built_in">print</span>(dy_dx) <span class="comment"># tensor(-2.)</span></span><br></pre></td></tr></table></figure><ol start="2"><li>非标量的反向传播</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c</span></span><br><span class="line">x = torch.tensor([[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">1.0</span>,<span class="number">2.0</span>]],requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">gradient = torch.tensor([[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x:\n&quot;</span>,x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y:\n&quot;</span>,y)</span><br><span class="line">y.backward(gradient = gradient)</span><br><span class="line">x_grad = x.grad</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_grad:\n&quot;</span>,x_grad)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x:</span></span><br><span class="line"><span class="string"> tensor([[0., 0.],</span></span><br><span class="line"><span class="string">        [1., 2.]], requires_grad=True)</span></span><br><span class="line"><span class="string">y:</span></span><br><span class="line"><span class="string"> tensor([[1., 1.],</span></span><br><span class="line"><span class="string">        [0., 1.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">x_grad:</span></span><br><span class="line"><span class="string"> tensor([[-2., -2.],</span></span><br><span class="line"><span class="string">        [ 0.,  2.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ol start="3"><li>非标量的反向传播可以用标量的反向传播实现</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor([[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">1.0</span>,<span class="number">2.0</span>]],requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">gradient = torch.tensor([[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line">z = torch.<span class="built_in">sum</span>(y*gradient)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x:&quot;</span>,x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y:&quot;</span>,y)</span><br><span class="line">z.backward()</span><br><span class="line">x_grad = x.grad</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_grad:\n&quot;</span>,x_grad)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x: tensor([[0., 0.],</span></span><br><span class="line"><span class="string">        [1., 2.]], requires_grad=True)</span></span><br><span class="line"><span class="string">y: tensor([[1., 1.],</span></span><br><span class="line"><span class="string">        [0., 1.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">x_grad:</span></span><br><span class="line"><span class="string"> tensor([[-2., -2.],</span></span><br><span class="line"><span class="string">        [ 0.,  2.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><hr><ul><li>在numpy和torch.tensor中  *  都是指相同size的矩阵各个位置相乘产生新矩阵</li><li>numpy中的矩阵乘法为<code>np.dot(a,b)</code>，torch中为<code>torch.matmul(c,d)</code></li></ul><hr><h4 id="autograd-grad自动微分"><a href="#autograd-grad自动微分" class="headerlink" title="autograd.grad自动微分"></a>autograd.grad自动微分</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line"></span><br><span class="line"><span class="comment"># create_graph 设置为 True 将允许创建更高阶的导数 </span></span><br><span class="line">dy_dx = torch.autograd.grad(y,x,create_graph=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(dy_dx.data)      <span class="comment"># tensor(-2.)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求二阶导数</span></span><br><span class="line">dy2_dx2 = torch.autograd.grad(dy_dx,x)[<span class="number">0</span>] </span><br><span class="line"><span class="built_in">print</span>(dy2_dx2.data) <span class="comment"># tensor(2.)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">x1 = torch.tensor(<span class="number">1.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">x2 = torch.tensor(<span class="number">2.0</span>,requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y1 = x1*x2</span><br><span class="line">y2 = x1+x2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 允许同时对多个自变量求导数</span></span><br><span class="line">(dy1_dx1,dy1_dx2) = torch.autograd.grad(outputs=y1,inputs = [x1,x2],retain_graph = <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(dy1_dx1,dy1_dx2)</span><br><span class="line"><span class="comment"># tensor(2.) tensor(1.)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果有多个因变量，相当于把多个因变量的梯度结果求和</span></span><br><span class="line">(dy12_dx1,dy12_dx2) = torch.autograd.grad(outputs=[y1,y2],inputs = [x1,x2])</span><br><span class="line"><span class="built_in">print</span>(dy12_dx1,dy12_dx2)</span><br><span class="line"><span class="comment"># tensor(3.) tensor(2.)</span></span><br></pre></td></tr></table></figure><h4 id="利用autograd和optimizer求最小值"><a href="#利用autograd和optimizer求最小值" class="headerlink" title="利用autograd和optimizer求最小值"></a>利用autograd和optimizer求最小值</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>, requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(params=[x], lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    result = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    <span class="keyword">return</span> (result)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    y = f(x)</span><br><span class="line">    y.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;y= <span class="subst">&#123;f(x).data&#125;</span>; x= <span class="subst">&#123;x.data&#125;</span>&quot;</span>) <span class="comment"># f-string格式化</span></span><br><span class="line"><span class="comment"># y= tensor(0.) ; x= tensor(1.0000)</span></span><br></pre></td></tr></table></figure><h3 id="动态计算图"><a href="#动态计算图" class="headerlink" title="动态计算图"></a>动态计算图</h3><p>之前的一些深度学习框架使用静态计算图，而pytorch使用动态计算图，有它的优点</p><h4 id="动态图简介"><a href="#动态图简介" class="headerlink" title="动态图简介"></a>动态图简介</h4><p><img src="https://www.pytorchmaster.com/data/torch%E5%8A%A8%E6%80%81%E5%9B%BE.gif" alt="img"></p><p>pytorch的计算图由<strong>节点</strong>和<strong>边</strong>组成，节点表示<strong>张量</strong>或<strong>Function</strong>,边表示张量和Function之间的依赖关系</p><p>动态有两重含义</p><ul><li>计算图正向传播是立即执行的，无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到的计算结果。</li><li>计算图在反向传播后立即销毁。下次调用需要重新构建计算图。如果在程序中使用了backward方法执行了反向传播，或者利用torch.autograd.grad方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建</li></ul><p>1.计算图的正向传播是立即执行的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.randn(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">Y = torch.randn(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">Y_hat = X@w.t() + b  <span class="comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span></span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss.data)</span><br><span class="line"><span class="built_in">print</span>(Y_hat.data)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(17.8969)</span></span><br><span class="line"><span class="string">tensor([[3.2613],</span></span><br><span class="line"><span class="string">        [4.7322],</span></span><br><span class="line"><span class="string">        [4.5037],</span></span><br><span class="line"><span class="string">        [7.5899],</span></span><br><span class="line"><span class="string">        [7.0973],</span></span><br><span class="line"><span class="string">        [1.3287],</span></span><br><span class="line"><span class="string">        [6.1473],</span></span><br><span class="line"><span class="string">        [1.3492],</span></span><br><span class="line"><span class="string">        [1.3911],</span></span><br><span class="line"><span class="string">        [1.2150]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>2.计算图在反向传播后立即销毁</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.randn(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">Y = torch.randn(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">Y_hat = X@w.t() + b  <span class="comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span></span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算图在反向传播后立即销毁，如果需要保留计算图, 需要设置retain_graph = True</span></span><br><span class="line">loss.backward()  <span class="comment">#loss.backward(retain_graph = True) </span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss.backward() <span class="comment">#如果再次执行反向传播将报错</span></span><br></pre></td></tr></table></figure><p>RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph&#x3D;True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.</p><p>第二次尝试向后遍历图形（或在张量已被释放后直接访问已保存的张量）。当您调用 .backward() 或 autograd.grad() 时，图形的已保存中间值将被释放。如果您需要第二次向后遍历图形，或者如果您需要在向后调用后访问保存的张量，请指定 retain_graph&#x3D;True。</p><h4 id="计算图中的Function"><a href="#计算图中的Function" class="headerlink" title="计算图中的Function"></a>计算图中的Function</h4><p>计算图中除了张量的另外一种节点是<code>Function</code>, 实际上就是 Pytorch中各种对张量操作的函数</p><p>这些Function和我们python中的函数有一个较大的区别，那就是它<strong>同时包含正向计算逻辑和反向传播逻辑</strong></p><p>我们可以通过继承torch.autograd.Function来创建这种支持反向传播的Function</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyReLU</span>(torch.autograd.Function):</span><br><span class="line">    <span class="comment"># 正向传播逻辑，可以用ctx存储一些值，供反向传播使用</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span></span>):</span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span>.clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播逻辑</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        <span class="built_in">input</span>, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[<span class="built_in">input</span> &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.tensor([[-<span class="number">1.0</span>,-<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>,<span class="number">3.0</span>]])</span><br><span class="line"></span><br><span class="line">relu = MyReLU.apply <span class="comment"># relu现在也可以具有正向传播和反向传播功能</span></span><br><span class="line">Y_hat = relu(X@w.t() + b)</span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(w.grad)<span class="comment">#tensor([[4.5000, 4.5000]])</span></span><br><span class="line"><span class="built_in">print</span>(b.grad)<span class="comment">#tensor([[4.5000]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Y_hat的梯度函数即是我们自己所定义的 MyReLU.backward</span></span><br><span class="line"><span class="built_in">print</span>(Y_hat.grad_fn)<span class="comment">#&lt;torch.autograd.function.MyReLUBackward object at 0x1205a46c8&gt;</span></span><br></pre></td></tr></table></figure><h4 id="计算图与反向传播"><a href="#计算图与反向传播" class="headerlink" title="计算图与反向传播"></a>计算图与反向传播</h4><p>简单理解反向传播的原理和过程（链式法则）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x + <span class="number">1</span></span><br><span class="line">y2 = <span class="number">2</span>*x</span><br><span class="line">loss = (y1-y2)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure><p>loss.backward()语句调用后，依次发生以下计算过程。</p><ol><li><p>loss自己的grad梯度赋值为1，即对自身的梯度为1。</p></li><li><p>loss根据其自身梯度以及关联的backward方法，计算出其对应的自变量即y1和y2的梯度，将该值赋值到y1.grad和y2.grad。</p></li><li><p>y2和y1根据其自身梯度以及关联的backward方法, 分别计算出其对应的自变量x的梯度，x.grad将其收到的多个梯度值累加。</p></li></ol><p>（注意，1,2,3步骤的求梯度顺序和对多个梯度值的累加规则恰好是求导链式法则的程序表述）</p><p>正因为求导链式法则衍生的梯度累加规则，张量的grad梯度不会自动清零，在需要的时候需要手动置零。</p><h4 id="叶子节点和非叶子节点"><a href="#叶子节点和非叶子节点" class="headerlink" title="叶子节点和非叶子节点"></a>叶子节点和非叶子节点</h4><p>执行<a href="#jump">上述代码</a>，我们会发现loss.grad并不是我们期望的1，而是None。类似地 y1.grad 以及 y2.grad也是 None。</p><p>这是<strong>由于它们不是叶子节点张量</strong>。</p><p>在反向传播过程中，只有 <code>is_leaf=True</code> 的叶子节点，需要求导的张量的导数结果才会被最后保留下来。</p><p>那么什么是叶子节点张量呢？叶子节点张量需要满足两个条件。</p><p>1，叶子节点张量是由用户直接创建的张量，而非由某个Function通过计算得到的张量。</p><p>2，叶子节点张量的 requires_grad属性必须为True.</p><p>Pytorch设计这样的规则主要是为了节约内存或者显存空间，因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。</p><p>所有依赖于叶子节点张量的张量, 其requires_grad 属性必定是True的，但其梯度值只在计算过程中被用到，不会最终存储到grad属性中。</p><p>如果需要保留中间计算结果的梯度到grad属性中，可以使用 retain_grad方法。 如果仅仅是为了调试代码查看梯度值，可以利用register_hook打印日志。<span id="jump"></span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x + <span class="number">1</span></span><br><span class="line">y2 = <span class="number">2</span>*x</span><br><span class="line">loss = (y1-y2)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;loss.grad:&quot;</span>, loss.grad) <span class="comment"># loss.grad: None</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y1.grad:&quot;</span>, y1.grad)<span class="comment"># y1.grad: None</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y2.grad:&quot;</span>, y2.grad)<span class="comment"># y2.grad: None</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)<span class="comment"># tensor(4.)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.is_leaf) <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(y1.is_leaf) <span class="comment"># False</span></span><br><span class="line"><span class="built_in">print</span>(y2.is_leaf) <span class="comment"># False</span></span><br><span class="line"><span class="built_in">print</span>(loss.is_leaf)<span class="comment"># False</span></span><br></pre></td></tr></table></figure><p>利用retain_grad可以保留非叶子节点的梯度值，利用register_hook可以查看非叶子节点的梯度值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment">#正向传播</span></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x + <span class="number">1</span></span><br><span class="line">y2 = <span class="number">2</span>*x</span><br><span class="line">loss = (y1-y2)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#非叶子节点梯度显示控制</span></span><br><span class="line">y1.register_hook(<span class="keyword">lambda</span> grad: <span class="built_in">print</span>(<span class="string">&#x27;y1 grad: &#x27;</span>, grad))<span class="comment"># y2 grad:  tensor(4.)</span></span><br><span class="line">y2.register_hook(<span class="keyword">lambda</span> grad: <span class="built_in">print</span>(<span class="string">&#x27;y2 grad: &#x27;</span>, grad))<span class="comment"># y1 grad:  tensor(-4.)</span></span><br><span class="line">loss.retain_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;loss.grad:&quot;</span>, loss.grad)<span class="comment"># loss.grad: tensor(1.)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x.grad:&quot;</span>, x.grad) <span class="comment"># loss.grad: tensor(1.)</span></span><br></pre></td></tr></table></figure><h4 id="计算图在TensorBoard中的可视化"><a href="#计算图在TensorBoard中的可视化" class="headerlink" title="计算图在TensorBoard中的可视化"></a>计算图在TensorBoard中的可视化</h4><p>可以利用 torch.utils.tensorboard 将计算图导出到 TensorBoard进行可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.w = nn.Parameter(torch.randn(<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">        self.b = nn.Parameter(torch.zeros(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = x@self.w + self.b</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./data/tensorboard&#x27;</span>)</span><br><span class="line">writer.add_graph(net,input_to_model = torch.rand(<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line">%load_ext tensorboard</span><br><span class="line"><span class="comment">#%tensorboard --logdir ./data/tensorboard</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorboard <span class="keyword">import</span> notebook</span><br><span class="line">notebook.<span class="built_in">list</span>() </span><br><span class="line"></span><br><span class="line"><span class="comment">#在tensorboard中查看模型</span></span><br><span class="line">notebook.start(<span class="string">&quot;--logdir ./data/tensorboard&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://www.pytorchmaster.com/data/2-3-%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt="img"></p><h3 id="pytorch的层次结构"><a href="#pytorch的层次结构" class="headerlink" title="pytorch的层次结构"></a>pytorch的层次结构</h3><p>5个不同的层次结构：即硬件层，内核层，低阶API，中阶API，高阶API [torchkeras]</p><p>Pytorch的层次结构从低到高可以分成如下五层。</p><p>最底层为硬件层，Pytorch支持CPU、GPU加入计算资源池。</p><p>第二层为C++实现的内核。</p><p>第三层为Python实现的操作符，提供了封装C++内核的低级API指令，主要包括各种张量操作算子、自动微分、变量管理. 如torch.tensor,torch.cat,torch.autograd.grad,nn.Module. 如果把模型比作一个房子，那么第三层API就是【模型之砖】。</p><p>第四层为Python实现的模型组件，对低级API进行了函数封装，主要包括各种模型层，损失函数，优化器，数据管道等等。 如torch.nn.Linear,torch.nn.BCE,torch.optim.Adam,torch.utils.data.DataLoader. 如果把模型比作一个房子，那么第四层API就是【模型之墙】。</p><p>第五层为Python实现的模型接口。Pytorch没有官方的高阶API。为了便于训练模型，<a href="https://github.com/lyhue1991/eat_pytorch_in_20_days">博客作者</a>仿照keras中的模型接口，使用了不到300行代码，封装了Pytorch的高阶模型接口torchkeras.Model。如果把模型比作一个房子，那么第五层API就是模型本身，即【模型之屋】。</p><h4 id="低阶API示范"><a href="#低阶API示范" class="headerlink" title="低阶API示范"></a>低阶API示范</h4><p>下面的范例使用Pytorch的低阶API实现线性回归模型和DNN二分类模型</p><p>低阶API主要包括<strong>张量操作</strong>，<strong>计算图</strong>和<strong>自动微分</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">printbar</span>():</span><br><span class="line">    nowtime = datetime.datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;==========&quot;</span>*<span class="number">8</span> + <span class="string">&quot;%s&quot;</span>%nowtime)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mac系统上pytorch和matplotlib在jupyter中同时跑需要更改环境变量</span></span><br><span class="line"><span class="comment"># os.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;]=&quot;TRUE&quot; </span></span><br></pre></td></tr></table></figure><h5 id="一、线性回归模型"><a href="#一、线性回归模型" class="headerlink" title="一、线性回归模型"></a>一、线性回归模型</h5><ol><li>data prepare</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#样本数量</span></span><br><span class="line">n = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成测试用数据集</span></span><br><span class="line">X = <span class="number">10</span>*torch.rand([n,<span class="number">2</span>])-<span class="number">5.0</span>  <span class="comment">#torch.rand是均匀分布 </span></span><br><span class="line">w0 = torch.tensor([[<span class="number">2.0</span>],[-<span class="number">3.0</span>]])</span><br><span class="line">b0 = torch.tensor([[<span class="number">10.0</span>]])</span><br><span class="line">Y = X@w0 + b0 + torch.normal( <span class="number">0.0</span>,<span class="number">2.0</span>,size = [n,<span class="number">1</span>])  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;svg&#x27;</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))  <span class="comment"># 图形窗口的宽度为 12 英寸，高度为 5 英寸</span></span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line"><span class="comment">#画散点图 </span></span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>].numpy(),Y[:,<span class="number">0</span>].numpy(), c = <span class="string">&quot;b&quot;</span>,label = <span class="string">&quot;samples&quot;</span>)</span><br><span class="line"><span class="comment">#设置图例</span></span><br><span class="line">ax1.legend()  <span class="comment"># 没有此不会有samples图例</span></span><br><span class="line">plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>].numpy(),Y[:,<span class="number">0</span>].numpy(), c = <span class="string">&quot;g&quot;</span>,label = <span class="string">&quot;samples&quot;</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>,rotation = <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230119174857563.png" alt="image-20230119174857563"></p><p>注：**<code>plt.subplot(121)</code> 是 matplotlib 库中用于创建多个子图的函数之一。其中，<code>121</code> 是参数，表示将整个图分成 1 行 2 列，并在第 1 个位置创建子图。因此，这条语句将创建一个 1 行 2 列的子图网格，并在第一个子图中绘制图形。**</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建数据管道迭代器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">features, labels, batch_size=<span class="number">8</span></span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    np.random.shuffle(indices)  <span class="comment">#样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        indexs = torch.LongTensor(indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span>  features.index_select(<span class="number">0</span>, indexs), labels.index_select(<span class="number">0</span>, indexs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据管道效果   </span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">(features,labels) = <span class="built_in">next</span>(data_iter(X,Y,batch_size))</span><br><span class="line"><span class="built_in">print</span>(features)</span><br><span class="line"><span class="built_in">print</span>(labels)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 1.0449, -0.3581],</span></span><br><span class="line"><span class="string">        [-3.0645, -2.9230],</span></span><br><span class="line"><span class="string">        [ 3.7969, -4.5846],</span></span><br><span class="line"><span class="string">        [-0.2429,  0.5349],</span></span><br><span class="line"><span class="string">        [ 2.2708,  0.1713],</span></span><br><span class="line"><span class="string">        [ 4.6910,  4.3684],</span></span><br><span class="line"><span class="string">        [ 2.1360, -4.7411],</span></span><br><span class="line"><span class="string">        [ 3.3687,  0.3648]])</span></span><br><span class="line"><span class="string">tensor([[15.6397],</span></span><br><span class="line"><span class="string">        [14.1632],</span></span><br><span class="line"><span class="string">        [31.6240],</span></span><br><span class="line"><span class="string">        [ 7.4723],</span></span><br><span class="line"><span class="string">        [11.8881],</span></span><br><span class="line"><span class="string">        [ 6.8064],</span></span><br><span class="line"><span class="string">        [30.4618],</span></span><br><span class="line"><span class="string">        [15.2579]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ol start="2"><li>define the model</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegression</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.w = torch.randn_like(w0,requires_grad=<span class="literal">True</span>)</span><br><span class="line">        self.b = torch.zeros_like(b0,requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">#正向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>): </span><br><span class="line">        <span class="keyword">return</span> x@self.w + self.b</span><br><span class="line">    <span class="comment"># 损失函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss_func</span>(<span class="params">self,y_pred,y_true</span>):  </span><br><span class="line">        <span class="keyword">return</span> torch.mean((y_pred - y_true)**<span class="number">2</span>/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model = LinearRegression()</span><br></pre></td></tr></table></figure><ol start="3"><li>model training</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">model, features, labels</span>):</span><br><span class="line">    predictions = model.forward(features)</span><br><span class="line">    loss = model.loss_func(predictions,labels)</span><br><span class="line">    <span class="comment"># 反向传播求梯度</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用torch.no_grad()避免梯度记录，也可以通过操作 model.w.data 实现避免梯度记录 </span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 梯度下降法更新参数</span></span><br><span class="line">        model.w -= <span class="number">0.001</span>*model.w.grad</span><br><span class="line">        model.b -= <span class="number">0.001</span>*model.b.grad</span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        model.w.grad.zero_()</span><br><span class="line">        model.b.grad.zero_()</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试train_step效果</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">(features,labels) = <span class="built_in">next</span>(data_iter(X,Y,batch_size))</span><br><span class="line">train_step(model,features,labels)</span><br></pre></td></tr></table></figure><p>Out[]: tensor(141.2280, grad_fn&#x3D;<MeanBackward0>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">model,epochs</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> data_iter(X,Y,<span class="number">10</span>):</span><br><span class="line">            loss = train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">200</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;epoch =&quot;</span>,epoch,<span class="string">&quot;loss = &quot;</span>,loss.item())</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;model.w =&quot;</span>,model.w.data)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;model.b =&quot;</span>,model.b.data)</span><br><span class="line"></span><br><span class="line">train_model(model,epochs = <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">================================================================================2023-01-20 17:03:37</span><br><span class="line">epoch = 200 loss =  2.0936481952667236</span><br><span class="line">model.w = tensor([[ 2.0185],</span><br><span class="line">        [-2.9519]])</span><br><span class="line">model.b = tensor([[10.0203]])</span><br><span class="line"></span><br><span class="line">================================================================================2023-01-20 17:03:38</span><br><span class="line">epoch = 400 loss =  1.3962600231170654</span><br><span class="line">model.w = tensor([[ 2.0212],</span><br><span class="line">        [-2.9512]])</span><br><span class="line">model.b = tensor([[10.0241]])</span><br><span class="line"></span><br><span class="line">================================================================================2023-01-20 17:03:39</span><br><span class="line">epoch = 600 loss =  0.3997553586959839</span><br><span class="line">model.w = tensor([[ 2.0167],</span><br><span class="line">        [-2.9528]])</span><br><span class="line">model.b = tensor([[10.0237]])</span><br><span class="line"></span><br><span class="line">================================================================================2023-01-20 17:03:41</span><br><span class="line">epoch = 800 loss =  1.8717002868652344</span><br><span class="line">model.w = tensor([[ 2.0194],</span><br><span class="line">        [-2.9495]])</span><br><span class="line">model.b = tensor([[10.0240]])</span><br><span class="line"></span><br><span class="line">================================================================================2023-01-20 17:03:42</span><br><span class="line">epoch = 1000 loss =  1.8542640209197998</span><br><span class="line">model.w = tensor([[ 2.0176],</span><br><span class="line">        [-2.9507]])</span><br><span class="line">model.b = tensor([[10.0237]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>].numpy(),Y[:,<span class="number">0</span>].numpy(), c = <span class="string">&quot;b&quot;</span>,label = <span class="string">&quot;samples&quot;</span>)</span><br><span class="line">ax1.plot(X[:,<span class="number">0</span>].numpy(),(model.w[<span class="number">0</span>].data*X[:,<span class="number">0</span>]+model.b[<span class="number">0</span>].data).numpy(),<span class="string">&quot;-r&quot;</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">&quot;model&quot;</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>].numpy(),Y[:,<span class="number">0</span>].numpy(), c = <span class="string">&quot;g&quot;</span>,label = <span class="string">&quot;samples&quot;</span>)</span><br><span class="line">ax2.plot(X[:,<span class="number">1</span>].numpy(),(model.w[<span class="number">1</span>].data*X[:,<span class="number">1</span>]+model.b[<span class="number">0</span>].data).numpy(),<span class="string">&quot;-r&quot;</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">&quot;model&quot;</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230120170539911.png" alt="image-20230120170539911"></p><h5 id="二、DNN二分类模型"><a href="#二、DNN二分类模型" class="headerlink" title="二、DNN二分类模型"></a>二、DNN二分类模型</h5><ol><li>data prepare</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;svg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正负样本数量</span></span><br><span class="line">n_positive,n_negative = <span class="number">2000</span>,<span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正样本, 小圆环分布</span></span><br><span class="line">r_p = <span class="number">5.0</span> + torch.normal(<span class="number">0.0</span>,<span class="number">1.0</span>,size = [n_positive,<span class="number">1</span>]) </span><br><span class="line">theta_p = <span class="number">2</span>*np.pi*torch.rand([n_positive,<span class="number">1</span>])</span><br><span class="line">Xp = torch.cat([r_p*torch.cos(theta_p),r_p*torch.sin(theta_p)],axis = <span class="number">1</span>)</span><br><span class="line">Yp = torch.ones_like(r_p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成负样本, 大圆环分布</span></span><br><span class="line">r_n = <span class="number">8.0</span> + torch.normal(<span class="number">0.0</span>,<span class="number">1.0</span>,size = [n_negative,<span class="number">1</span>]) </span><br><span class="line">theta_n = <span class="number">2</span>*np.pi*torch.rand([n_negative,<span class="number">1</span>])</span><br><span class="line">Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = <span class="number">1</span>)</span><br><span class="line">Yn = torch.zeros_like(r_n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#汇总样本</span></span><br><span class="line">X = torch.cat([Xp,Xn],axis = <span class="number">0</span>)</span><br><span class="line">Y = torch.cat([Yp,Yn],axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize = (<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">&quot;r&quot;</span>)</span><br><span class="line">plt.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">&quot;g&quot;</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;positive&quot;</span>,<span class="string">&quot;negative&quot;</span>])</span><br></pre></td></tr></table></figure><p><img src="https://www.pytorchmaster.com/data/3-1-%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt="img"></p>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python小知识点</title>
      <link href="/2023/01/04/python%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
      <url>/2023/01/04/python%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p>python小知识点</p><h4 id="可变类型拷贝"><a href="#可变类型拷贝" class="headerlink" title="可变类型拷贝"></a>可变类型拷贝</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;raw data.txt&quot;</span>,<span class="string">&quot;r&quot;</span>)<span class="keyword">as</span> f:</span><br><span class="line">raw data f.readlines()</span><br><span class="line">raw_data [[<span class="built_in">int</span>(i)<span class="keyword">for</span> i <span class="keyword">in</span> data.strip().split(<span class="string">&quot;&quot;</span>)<span class="keyword">for</span> data <span class="keyword">in</span> raw data]</span><br><span class="line"><span class="comment">#[[1,2,3,4,5],[6,7,8,9,0],[2,2,2,2,2],[4,4,4,4,4],[6,7,1,9,8]]</span></span><br><span class="line"></span><br><span class="line">raw_data_copy = rawdata.copy() <span class="comment"># 一维数据copy样拷贝还会改变原数据</span></span><br><span class="line">raw_data_copy = copy.deepcopy(raw_data)  <span class="comment">#多维数据拷贝</span></span><br><span class="line">fraw_data_copy[<span class="number">0</span>][<span class="number">0</span>]<span class="number">99999</span></span><br><span class="line"><span class="built_in">print</span>(raw_data)</span><br><span class="line"><span class="comment">#[1,2,3,4,5],[6,7,8,9,0],[2,2,2,2,2],[4,4,</span></span><br><span class="line"><span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>],<span class="number">6</span>,<span class="number">7</span>,<span class="number">1</span>,<span class="number">9</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>函数默认参数为可变类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_fruit</span>(<span class="params">fruit,fruit <span class="built_in">list</span>=[]</span>):</span><br><span class="line">fruit_list.append(fruit)</span><br><span class="line"><span class="built_in">print</span>(fruit_list)</span><br><span class="line">fruits=[<span class="string">&#x27;banana&#x27;</span>,<span class="string">&#x27;apple&#x27;</span>]</span><br><span class="line">add_fruit(<span class="string">&#x27;watermelon&#x27;</span>,fruits)</span><br><span class="line">[<span class="string">&#x27;banana&#x27;</span>,<span class="string">&#x27;apple&#x27;</span>,<span class="string">&#x27;watermelon&#x27;</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_fruit</span>(<span class="params">fruit,fruit <span class="built_in">list</span>=[]</span>):</span><br><span class="line">fruit_list.append(fruit)</span><br><span class="line"><span class="built_in">print</span>(fruit_list)</span><br><span class="line"></span><br><span class="line">add fruit(<span class="string">&#x27;watermelon&#x27;</span>) <span class="comment">#[&#x27;watermelon&#x27;]</span></span><br><span class="line">add fruit(<span class="string">&#x27;banana&#x27;</span>)  <span class="comment">#[&#x27;watermelon&#x27;, &#x27;banana&#x27;]</span></span><br></pre></td></tr></table></figure><p>由于可变，所以fruit_list会在前面的基础上调用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_fruit</span>(<span class="params">fruit,fruit <span class="built_in">list</span>=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> fruit_list <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        fruit_list = []</span><br><span class="line">fruit_list.append(fruit)</span><br><span class="line"><span class="built_in">print</span>(fruit_list)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(add_fruit.__defaults__ ) <span class="comment"># (None, )</span></span><br><span class="line">add_fruit(<span class="string">&#x27;watermelon&#x27;</span>) <span class="comment"># [&#x27;watermelon&#x27;]</span></span><br><span class="line"><span class="built_in">print</span>(add_fruit.__defaults__ ) <span class="comment"># (None, )</span></span><br><span class="line">add_ fruit( <span class="string">&#x27;banana&#x27;</span> ) <span class="comment">#[&#x27;banana&#x27;]</span></span><br><span class="line"><span class="built_in">print</span>(add_fruit.__defaults__ ) <span class="comment"># (None, )</span></span><br></pre></td></tr></table></figure><p>所以尽可能避免将可变类型作为默认参数，而是在内部判断</p><h4 id="函数默认值属性"><a href="#函数默认值属性" class="headerlink" title="函数默认值属性"></a>函数默认值属性</h4><p><strong>在定义时就被确定了</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">display_time</span>(<span class="params">data=datetime.now(<span class="params"> </span>)</span>) :</span><br><span class="line"><span class="built_in">print</span>(data.strftime(<span class="string">&#x27;%B %d, %Y %H:%M:%S&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span> (display_time.__defaults__ ) <span class="comment"># (datetime.datetime (2022,11,26,17, 4, 53, 360783), )</span></span><br><span class="line">display_time() <span class="comment"># November 26, 2022 17 :04:53</span></span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line">display_time() <span class="comment"># November 26, 2022 17 :04:53 </span></span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line">display_time() <span class="comment"># November 26, 2022 17 :04:53</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>修改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">display_time</span>(<span class="params">data=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> data <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        data = datatime.Now()</span><br><span class="line"><span class="built_in">print</span>(data.strftime(<span class="string">&#x27;%B %d, %Y %H:%M:%S&#x27;</span>))</span><br></pre></td></tr></table></figure><h4 id="下划线的含义"><a href="#下划线的含义" class="headerlink" title="下划线的含义"></a>下划线的含义</h4><p>· 单引号下划线： <code>_var</code></p><p>单下划线是一种Python命名约定，表示某个名称是供内部使用的，只是对程序员的提示，不会有多余的效果</p><p>· 单尾划线： <code>var_</code></p><p>一个变量最合适的名字已经被一个关键字代替了等情况，打破命名冲突</p><p>· 双领先下划线： <code>__var</code></p><p>双下划线前缀导致Python解释器重写属性名，以避免子类中的命名冲突</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Test</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.foo = <span class="number">11</span></span><br><span class="line">        self._bar = <span class="number">23</span></span><br><span class="line">        self.__baz = <span class="number">23</span></span><br><span class="line">        </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = Test()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">dir</span>(t)</span><br><span class="line">[<span class="string">&#x27;_Test__baz&#x27;</span>, <span class="string">&#x27;__class__&#x27;</span>, <span class="string">&#x27;__delattr__&#x27;</span>, <span class="string">&#x27;__dict__&#x27;</span>, <span class="string">&#x27;__dir__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__doc__&#x27;</span>, <span class="string">&#x27;__eq__&#x27;</span>, <span class="string">&#x27;__format__&#x27;</span>, <span class="string">&#x27;__ge__&#x27;</span>, <span class="string">&#x27;__getattribute__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__gt__&#x27;</span>, <span class="string">&#x27;__hash__&#x27;</span>, <span class="string">&#x27;__init__&#x27;</span>, <span class="string">&#x27;__le__&#x27;</span>, <span class="string">&#x27;__lt__&#x27;</span>, <span class="string">&#x27;__module__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__ne__&#x27;</span>, <span class="string">&#x27;__new__&#x27;</span>, <span class="string">&#x27;__reduce__&#x27;</span>, <span class="string">&#x27;__reduce_ex__&#x27;</span>, <span class="string">&#x27;__repr__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__setattr__&#x27;</span>, <span class="string">&#x27;__sizeof__&#x27;</span>, <span class="string">&#x27;__str__&#x27;</span>, <span class="string">&#x27;__subclasshook__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__weakref__&#x27;</span>, <span class="string">&#x27;_bar&#x27;</span>, <span class="string">&#x27;foo&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t.__dict__</span><br><span class="line">&#123;<span class="string">&#x27;foo&#x27;</span>: <span class="number">11</span>, <span class="string">&#x27;_bar&#x27;</span>: <span class="number">23</span>, <span class="string">&#x27;_Test__baz&#x27;</span>: <span class="number">23</span>&#125;</span><br></pre></td></tr></table></figure><p>补充<code>__dict__</code>的博客：<a href="http://c.biancheng.net/view/2374.html">http://c.biancheng.net/view/2374.html</a></p><p>可以看到<code>foo</code>和<code>_bar</code>的变量属性均未被修改，但是<code>__baz</code>被修改为<code>_Test__baz</code>,为了保护变量不被子类覆盖</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ManglingTest</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.__mangled = <span class="string">&#x27;hello&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_mangled</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.__mangled</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ManglingTest().get_mangled()</span><br><span class="line"><span class="string">&#x27;hello&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ManglingTest().__mangled</span><br><span class="line">AttributeError: <span class="string">&quot;&#x27;ManglingTest&#x27; object has no attribute &#x27;__mangled&#x27;&quot;</span></span><br></pre></td></tr></table></figure><p>· 领先和落后双下划线： <code>__var__</code></p><p>Python中存在一些特殊的方法，有些方法以双下划线<code>__</code>开头和结尾，它们是Python的魔法函数，比如<code>__init__()</code>和<code>__str__</code>等等。<strong>不用要这种方式命名自己的变量或者函数</strong></p><hr><p>魔法函数是指类内部以双下划线开头，并且以双下划线结尾的函数，在特定时刻，Python会自动调用这些函数</p><hr><p><a href="https://www.cnblogs.com/chenhuabin/p/13752770.html#_label0">https://www.cnblogs.com/chenhuabin/p/13752770.html#_label0</a></p><p>· 单下划线： <code>_</code></p><p>一个单独的下划线有时用作一个名称，表示一个变量是临时的或是不重要的</p><h4 id="filter、map、reduce、apply"><a href="#filter、map、reduce、apply" class="headerlink" title="filter、map、reduce、apply"></a>filter、map、reduce、apply</h4><ul><li>filter(function，sequence)</li></ul><p><strong>过滤掉序列中不符合函数条件的元素</strong>,返回迭代器,需要list转列表等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x:x%<span class="number">2</span>==<span class="number">0</span>,x))) <span class="comment"># 找出偶数。python3.*之后filter函数返回的不再是列表而是迭代器，所以需要用list转换。</span></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">[<span class="number">2</span>, <span class="number">4</span>]</span><br></pre></td></tr></table></figure><ul><li>map(function,iterable1,iterable2)</li></ul><p><strong>求一个序列或者多个序列进行函数映射之后的值</strong>，就该想到map这个函数,返回迭代器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">y = [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x,y:(x*y)+<span class="number">2</span>,x,y)))</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">[<span class="number">4</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">22</span>, <span class="number">32</span>]</span><br></pre></td></tr></table></figure><ul><li>reduce（function，iterable）</li></ul><p><strong>对一个序列进行压缩运算，得到一个值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line">arr = [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">reduce(<span class="keyword">lambda</span> x,y: x + y,arr) <span class="comment"># 直接返回一个值</span></span><br><span class="line"><span class="comment"># 20</span></span><br></pre></td></tr></table></figure><p>其计算原理：<br>先计算头两个元素：f(2, 3)，结果为5；<br>再把结果和第3个元素计算：f(5, 4)，结果为9；<br>再把结果和第4个元素计算：f(9, 5)，结果为14；<br>再把结果和第5个元素计算：f(14, 6)，结果为20；<br>由于没有更多的元素了，计算结束，返回结果20。</p><ul><li>apply(function,axis)</li></ul><p>pandas中的函数，eg:   <code>data.apply(lambda x:x*10)</code></p><hr><p>filter和map都是python内置的函数，可以直接调用，reduce在functools模块，apply在pandas模块</p><hr><h4 id="yield"><a href="#yield" class="headerlink" title="yield"></a>yield</h4><p>带有 yield 的函数在 Python 中被称之为 generator（生成器）</p><p>博客：<a href="https://blog.csdn.net/mieleizhi0522/article/details/82142856">https://blog.csdn.net/mieleizhi0522/article/details/82142856</a></p><h4 id="python数值"><a href="#python数值" class="headerlink" title="python数值"></a>python数值</h4><ul><li><strong>整型(int)</strong> - 通常被称为是整型或整数，是正或负整数，不带小数点。Python3 整型是没有限制大小的，可以当作 Long 类型使用，所以 Python3 没有 Python2 的 Long 类型。布尔(bool)是整型的子类型。</li><li><strong>浮点型(float)</strong> - 浮点型由整数部分与小数部分组成，浮点型也可以使用科学计数法表示（2.5e2 &#x3D; 2.5 x 102 &#x3D; 250）</li><li><strong>复数( (complex))</strong> - 复数由实数部分和虚数部分构成，可以用a + bj,或者complex(a,b)表示， 复数的实部a和虚部b都是浮点型。</li></ul><h4 id="list-append-无返回值"><a href="#list-append-无返回值" class="headerlink" title="list.append()无返回值"></a>list.append()无返回值</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>case1 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(case1.append(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># list.append没有返回值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 还有clear, insert, sort, reverse, remove, extend</span></span><br></pre></td></tr></table></figure><p>所以<code>case = case.append(1)</code>这样的操作不可行</p><h4 id="列表是可变的"><a href="#列表是可变的" class="headerlink" title="列表是可变的"></a>列表是可变的</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> = [<span class="number">9</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">case</span>:</span><br><span class="line">   <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">     <span class="keyword">case</span>.remove(i)</span><br><span class="line">     </span><br><span class="line"><span class="built_in">print</span>(<span class="keyword">case</span>)</span><br><span class="line"></span><br><span class="line">&gt;&gt; [<span class="number">9</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>因为列表是可变对象，当第一个8被删除后，第二个8补上了前面的位置，自然而然就被跳过了</p><p>修改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> = [<span class="number">9</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">case1 = [x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="keyword">case</span> <span class="keyword">if</span> x%<span class="number">2</span> != <span class="number">0</span>]</span><br></pre></td></tr></table></figure><h4 id="字符串常量用空格连接"><a href="#字符串常量用空格连接" class="headerlink" title="字符串常量用空格连接"></a>字符串常量用空格连接</h4><p>表示字符串合并</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> = <span class="string">&#x27;a&#x27;</span> <span class="string">&#x27;b&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="keyword">case</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ab</span></span><br></pre></td></tr></table></figure><h4 id="tuple只有一个元素要在末尾加逗号"><a href="#tuple只有一个元素要在末尾加逗号" class="headerlink" title="tuple只有一个元素要在末尾加逗号"></a>tuple只有一个元素要在末尾加逗号</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">isinstance</span>((<span class="string">&#x27;bilibili&#x27;</span>), <span class="built_in">tuple</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">isinstance</span>((<span class="string">&#x27;bilibili&#x27;</span>,), <span class="built_in">tuple</span>)</span><br><span class="line"></span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"></span><br><span class="line">a = (<span class="string">&#x27;bilibili&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line">b</span><br><span class="line">i</span><br><span class="line">l</span><br><span class="line">i</span><br><span class="line">b</span><br><span class="line">i</span><br><span class="line">l</span><br><span class="line">i</span><br><span class="line"></span><br><span class="line">b = (<span class="string">&#x27;bilibli&#x27;</span>,)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> b:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line">bilibli</span><br></pre></td></tr></table></figure><h4 id="if-else表达式优先级高于逗号"><a href="#if-else表达式优先级高于逗号" class="headerlink" title="if-else表达式优先级高于逗号"></a>if-else表达式优先级高于逗号</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x, y = (<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">if</span> <span class="literal">True</span> <span class="keyword">else</span> <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure><h4 id="使用enumerate遍历："><a href="#使用enumerate遍历：" class="headerlink" title="使用enumerate遍历："></a>使用enumerate遍历：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line"><span class="keyword">for</span> idx, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">    <span class="keyword">if</span> num % <span class="number">2</span>:</span><br><span class="line">        data[idx] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure><h4 id="字典的get方法"><a href="#字典的get方法" class="headerlink" title="字典的get方法"></a>字典的get方法</h4><p>在工程文件中经常会注意到使用get方法，避免因键不存在而引起的程序崩溃，若索引不到，将返回在第二个位置定义的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = &#123;<span class="string">&quot;name&quot;</span> : <span class="string">&quot;sds&quot;</span>, <span class="string">&quot;age&quot;</span> : <span class="string">&quot;18&quot;</span>&#125;</span><br><span class="line">uid = data.get(<span class="string">&quot;uid&quot;</span>, <span class="string">&quot;6688&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(uid)</span><br><span class="line"></span><br><span class="line"><span class="number">6688</span></span><br></pre></td></tr></table></figure><h4 id="f-string新格式化方法"><a href="#f-string新格式化方法" class="headerlink" title="f-string新格式化方法"></a>f-string新格式化方法</h4><p>Python3.6开始支持的新的格式化操作，相比以前更简洁方便。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">9</span></span><br><span class="line">data = <span class="string">f&quot;<span class="subst">&#123;i&#125;</span> * <span class="subst">&#123;i&#125;</span> = <span class="subst">&#123;i * i&#125;</span>&quot;</span></span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">9</span> * <span class="number">9</span> = <span class="number">81</span></span><br></pre></td></tr></table></figure><p>我们也常常这样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">&#x27;xhm&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;hello,&#x27;</span>+name+<span class="string">&#x27;!&#x27;</span>)</span><br></pre></td></tr></table></figure><p>改为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;hello,<span class="subst">&#123;name&#125;</span>!&#x27;</span>)</span><br><span class="line"><span class="comment"># hello,xhm!</span></span><br></pre></td></tr></table></figure><h4 id="合并两个字典"><a href="#合并两个字典" class="headerlink" title="合并两个字典"></a>合并两个字典</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data_1 = &#123;<span class="string">&quot;name&quot;</span> : <span class="string">&quot;sds&quot;</span>, <span class="string">&quot;age&quot;</span> : <span class="string">&quot;18&quot;</span>&#125;</span><br><span class="line">data_2 = &#123;<span class="string">&quot;name&quot;</span> : <span class="string">&quot;sds&quot;</span>, <span class="string">&quot;uid&quot;</span> : <span class="string">&quot;6688&quot;</span>&#125;</span><br><span class="line">out_data = &#123;**data_1, **data_2&#125;</span><br><span class="line"><span class="built_in">print</span>(out_data)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;sds&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="string">&#x27;18&#x27;</span>, <span class="string">&#x27;uid&#x27;</span>: <span class="string">&#x27;6688&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><h4 id="判断某对象是否为某些值"><a href="#判断某对象是否为某些值" class="headerlink" title="判断某对象是否为某些值"></a>判断某对象是否为某些值</h4><p>如果需要在if中将某对象与多个其他对象进行对比判断，你可能会进行如下定义</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = <span class="string">&quot;a&quot;</span></span><br><span class="line"><span class="keyword">if</span> data == <span class="string">&quot;a&quot;</span> <span class="keyword">or</span> data == <span class="string">&quot;b&quot;</span> <span class="keyword">or</span> data == <span class="string">&quot;c&quot;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;HHH&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>HHH</span><br></pre></td></tr></table></figure><p>简化修改为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">datas = [<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>]</span><br><span class="line">data = <span class="string">&quot;a&quot;</span></span><br><span class="line"><span class="keyword">if</span> data <span class="keyword">in</span> datas:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;HHH&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>HHH</span><br></pre></td></tr></table></figure><h4 id="注意-和-的区别"><a href="#注意-和-的区别" class="headerlink" title="注意^和**的区别"></a>注意^和**的区别</h4><p>按位异或  和   次幂</p><h4 id="写文件时请用with"><a href="#写文件时请用with" class="headerlink" title="写文件时请用with"></a>写文件时请用with</h4><p>当写入出错时会报错，文件会关闭</p><h4 id="数字中的下划线"><a href="#数字中的下划线" class="headerlink" title="数字中的下划线"></a>数字中的下划线</h4><p><code>x=10000000</code>和<code>x=10_000_000</code>等价，但是后者明显更加清晰</p><h4 id="没穿衣服的元组"><a href="#没穿衣服的元组" class="headerlink" title="没穿衣服的元组"></a>没穿衣服的元组</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">1</span>,<span class="number">2</span></span><br><span class="line">d1 = x[<span class="number">0</span>]</span><br><span class="line">d2 = x[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h4 id="使用isinstance代替-x3D-x3D-号检查类型"><a href="#使用isinstance代替-x3D-x3D-号检查类型" class="headerlink" title="使用isinstance代替&#x3D;&#x3D;号检查类型"></a>使用isinstance代替&#x3D;&#x3D;号检查类型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">type</span>(name) == <span class="built_in">tuple</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">isinstance</span>(name, <span class="built_in">tuple</span>)</span><br></pre></td></tr></table></figure><h4 id="b-a-x3D-a-b快速值交换"><a href="#b-a-x3D-a-b快速值交换" class="headerlink" title="b,a  &#x3D; a,b快速值交换"></a>b,a  &#x3D; a,b快速值交换</h4>]]></content>
      
      
      
        <tags>
            
            <tag> python小知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python三剑客及一些常用函数</title>
      <link href="/2023/01/04/python%E4%B8%89%E5%89%91%E5%AE%A2%E5%8F%8A%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/"/>
      <url>/2023/01/04/python%E4%B8%89%E5%89%91%E5%AE%A2%E5%8F%8A%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h3><h4 id="str"><a href="#str" class="headerlink" title="str"></a>str</h4><ol><li>string.count(str, beg&#x3D;0, end&#x3D;len(string))    返回 str 在 string 里面出现的次数，如果 beg 或者 end 指定则返回指定范围内 str 出现的次数</li><li>string.find(str, beg&#x3D;0, end&#x3D;len(string))       检测str是否包含在string中，如果beg和end指定范围，则检查是否包含在指定范围内，如果是返回开始的索引值，否则返回-1</li><li>string.format()</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="string">&quot;&#123;&#125; &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot;hello&quot;</span>, <span class="string">&quot;world&quot;</span>)    <span class="comment"># 不设置指定位置，按默认顺序</span></span><br><span class="line"><span class="string">&#x27;hello world&#x27;</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&quot;&#123;0&#125; &#123;1&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot;hello&quot;</span>, <span class="string">&quot;world&quot;</span>)  <span class="comment"># 设置指定位置</span></span><br><span class="line"><span class="string">&#x27;hello world&#x27;</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&quot;&#123;1&#125; &#123;0&#125; &#123;1&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot;hello&quot;</span>, <span class="string">&quot;world&quot;</span>)  <span class="comment"># 设置指定位置</span></span><br><span class="line"><span class="string">&#x27;world hello world&#x27;</span></span><br></pre></td></tr></table></figure><p>也可以设置参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;网站名：&#123;name&#125;, 地址 &#123;url&#125;&quot;</span>.<span class="built_in">format</span>(name=<span class="string">&quot;菜鸟教程&quot;</span>, url=<span class="string">&quot;www.runoob.com&quot;</span>))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 通过字典设置参数</span></span><br><span class="line">site = &#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;菜鸟教程&quot;</span>, <span class="string">&quot;url&quot;</span>: <span class="string">&quot;www.runoob.com&quot;</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;网站名：&#123;name&#125;, 地址 &#123;url&#125;&quot;</span>.<span class="built_in">format</span>(**site))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过列表索引设置参数</span></span><br><span class="line">my_list = [<span class="string">&#x27;菜鸟教程&#x27;</span>, <span class="string">&#x27;www.runoob.com&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;网站名：&#123;0[0]&#125;, 地址 &#123;0[1]&#125;&quot;</span>.<span class="built_in">format</span>(my_list))  <span class="comment"># &quot;0&quot; 是必须的</span></span><br></pre></td></tr></table></figure><p>也可以向 <strong>str.format()</strong> 传入对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AssignValue</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, value</span>):</span><br><span class="line">        self.value = value</span><br><span class="line">my_value = AssignValue(<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;value 为: &#123;0.value&#125;&#x27;</span>.<span class="built_in">format</span>(my_value))  <span class="comment"># &quot;0&quot; 是可选的</span></span><br></pre></td></tr></table></figure><p>格式化数字的方法</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230102114624443.png" alt="image-20230102114624443"></p><p><code>d</code> means expecting an int:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&quot;&#123;:d&#125;&quot;</span>.<span class="built_in">format</span>(<span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;3&#x27;</span></span><br></pre></td></tr></table></figure><p><code>2d</code> means formats to 2 characters using padding (whitespace by default)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&quot;&#123;:2d&#125;&quot;</span>.<span class="built_in">format</span>(<span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27; 3&#x27;</span></span><br></pre></td></tr></table></figure><p><code>0&gt;</code> means using <code>0</code> as padding, and right adjust the result:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&quot;&#123;:0&gt;2d&#125;&quot;</span>.<span class="built_in">format</span>(<span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;03&#x27;</span></span><br></pre></td></tr></table></figure><ol start="4"><li>string.join(seq)  以 string 作为分隔符，将 seq 中所有的元素(的字符串表示)<strong>合并为一个新的字符串</strong></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_seq = <span class="string">&#x27; &#x27;</span>.join(seq)</span><br></pre></td></tr></table></figure><ol start="5"><li>string.replace(str1, str2, num&#x3D;string.count(str1))   把string中<strong>str1替换为str2</strong>，如果num指定，则替换不超过num次</li><li>string.split(str&#x3D;””, num&#x3D;string.count(str))  以 str 为分隔符<strong>切片 string为列表</strong>，如果 num 有指定值，则仅分隔 <strong>num+1</strong> 个子字符串</li><li>string.strip([obj])    在 string 上执行 lstrip()和 rstrip()</li></ol><h4 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h4><ol><li>list(seq)     将元组转换成列表</li><li>list.append()    在列表末尾添加新的对象</li><li>list.count()     统计某个元素在列表中出现的次数</li><li>list.extend(seq) 在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）</li></ol><h4 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h4><p>tuple(iterable)   将可迭代系列转换为元组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>list1= [<span class="string">&#x27;Google&#x27;</span>, <span class="string">&#x27;Taobao&#x27;</span>, <span class="string">&#x27;Runoob&#x27;</span>, <span class="string">&#x27;Baidu&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tuple1=<span class="built_in">tuple</span>(list1)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tuple1</span><br><span class="line">(<span class="string">&#x27;Google&#x27;</span>, <span class="string">&#x27;Taobao&#x27;</span>, <span class="string">&#x27;Runoob&#x27;</span>, <span class="string">&#x27;Baidu&#x27;</span>)</span><br></pre></td></tr></table></figure><p>元组不变是指     <strong>元组所指向的内存中的内容不可变</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tup = (<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tup[<span class="number">0</span>] = <span class="string">&#x27;g&#x27;</span>     <span class="comment"># 不支持修改元素</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">TypeError: <span class="string">&#x27;tuple&#x27;</span> <span class="built_in">object</span> does <span class="keyword">not</span> support item assignment</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">id</span>(tup)     <span class="comment"># 查看内存地址</span></span><br><span class="line"><span class="number">4440687904</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tup = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">id</span>(tup)</span><br><span class="line"><span class="number">4441088800</span>    <span class="comment"># 内存地址不一样了</span></span><br></pre></td></tr></table></figure><h4 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">d=&#123;<span class="number">1</span>:<span class="string">&quot;a&quot;</span>,<span class="number">2</span>:<span class="string">&quot;b&quot;</span>,<span class="number">3</span>:<span class="string">&quot;c&quot;</span>&#125;</span><br><span class="line">result=[]</span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> d.items():</span><br><span class="line">    result.append(k)</span><br><span class="line">    result.append(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line">&gt;&gt; [<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>, <span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>, <span class="number">3</span>, <span class="string">&#x27;c&#x27;</span>]</span><br></pre></td></tr></table></figure><h4 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h4><p>集合运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data_1 = &#123;<span class="string">&#x27;Mathematics&#x27;</span>, <span class="string">&#x27;Chinese&#x27;</span>, <span class="string">&#x27;English&#x27;</span>, <span class="string">&#x27;Physics&#x27;</span>, <span class="string">&#x27;Chemistry&#x27;</span>, <span class="string">&#x27;Biology&#x27;</span>&#125;</span><br><span class="line">data_2 = &#123;<span class="string">&#x27;Mathematics&#x27;</span>, <span class="string">&#x27;Chinese&#x27;</span>, <span class="string">&#x27;English&#x27;</span>, <span class="string">&#x27;Politics&#x27;</span>, <span class="string">&#x27;Geography&#x27;</span>, <span class="string">&#x27;History&#x27;</span>&#125;</span><br><span class="line"><span class="comment"># 交集</span></span><br><span class="line">data_1 &amp; data_2</span><br><span class="line"><span class="comment"># 并集</span></span><br><span class="line">data_1 | data_2</span><br><span class="line"><span class="comment"># 差集</span></span><br><span class="line">data_1 - data_2</span><br><span class="line"><span class="comment"># 异或（不同时包含于两集合中的数据）</span></span><br><span class="line">data_1 ^ data_2</span><br></pre></td></tr></table></figure><h4 id="sorted"><a href="#sorted" class="headerlink" title="sorted()"></a>sorted()</h4><p>如果你需要对可迭代对象进行排序，比如列表、元组、字典，首先以列表为例子，可以直接使用内置函数sorted完成任务</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = [-<span class="number">1</span>, -<span class="number">10</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">5</span>]</span><br><span class="line">new_data = <span class="built_in">sorted</span>(data)</span><br><span class="line"><span class="comment"># new_data = sorted(data, reverse=True)降序</span></span><br><span class="line"><span class="built_in">print</span>(new_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [-10, -1, 0, 5, 9]</span></span><br></pre></td></tr></table></figure><p>对元组使用之后输出类型会变成列表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = (-<span class="number">1</span>, -<span class="number">10</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">5</span>)</span><br><span class="line">new_data = <span class="built_in">sorted</span>(data, reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(new_data)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="number">9</span>, <span class="number">5</span>, <span class="number">0</span>, -<span class="number">1</span>, -<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p>字典</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = [</span><br><span class="line">    &#123;<span class="string">&quot;name&quot;</span> : <span class="string">&quot;jia&quot;</span>, <span class="string">&quot;age&quot;</span> : <span class="number">18</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;name&quot;</span> : <span class="string">&quot;yi&quot;</span>, <span class="string">&quot;age&quot;</span> : <span class="number">60</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;name&quot;</span> : <span class="string">&quot;bing&quot;</span>, <span class="string">&quot;age&quot;</span> : <span class="number">20</span>&#125;</span><br><span class="line">]</span><br><span class="line">new_data = <span class="built_in">sorted</span>(data, key=<span class="keyword">lambda</span> x: x[<span class="string">&quot;age&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(new_data)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;jia&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">18</span>&#125;, &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;bing&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>&#125;, &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;yi&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">60</span>&#125;]</span><br></pre></td></tr></table></figure><h3 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h3><h4 id="get-dummies"><a href="#get-dummies" class="headerlink" title="get_dummies()"></a>get_dummies()</h4><p>pandas实现one hot encode的方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pandas.get_dummies(data, prefix=<span class="literal">None</span>, prefix_sep=<span class="string">&#x27;_&#x27;</span>, dummy_na=<span class="literal">False</span>, columns=<span class="literal">None</span>, sparse=<span class="literal">False</span>, drop_first=<span class="literal">False</span>)[source]</span><br></pre></td></tr></table></figure><p>例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame([</span><br><span class="line">    [<span class="string">&#x27;green&#x27;</span>, <span class="string">&#x27;A&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;B&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;A&#x27;</span>]</span><br><span class="line">])</span><br><span class="line">df.columns = [<span class="string">&#x27;color&#x27;</span>, <span class="string">&#x27;class&#x27;</span>]</span><br><span class="line">df = pd.get_dummies(df)</span><br></pre></td></tr></table></figure><p>get_dummies前</p><table><thead><tr><th></th><th>color</th><th>class</th></tr></thead><tbody><tr><td>0</td><td>green</td><td>A</td></tr><tr><td>1</td><td>red</td><td>B</td></tr><tr><td>2</td><td>blue</td><td>A</td></tr></tbody></table><p>get_dummies后</p><table><thead><tr><th align="left"></th><th align="center">color_blue</th><th align="center">color_green</th><th align="center">color_red</th><th align="center">color_A</th><th align="center">color_B</th></tr></thead><tbody><tr><td align="left">0</td><td align="center">0</td><td align="center">1</td><td align="center">0</td><td align="center">1</td><td align="center">0</td></tr><tr><td align="left">1</td><td align="center">0</td><td align="center">0</td><td align="center">1</td><td align="center">0</td><td align="center">1</td></tr><tr><td align="left">2</td><td align="center">1</td><td align="center">0</td><td align="center">0</td><td align="center">1</td><td align="center">0</td></tr></tbody></table><p>可以对指定列进行get_dummies</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.get_dummies(df.color)</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>blue</th><th>green</th><th>red</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>1</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><td>2</td><td>1</td><td>0</td><td>0</td></tr></tbody></table><p>将指定列进行get_dummies 后合并到元数据中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.join(pd.get_dummies(df.color))</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>color</th><th>class</th><th>blue</th><th>green</th><th>red</th></tr></thead><tbody><tr><td>0</td><td>green</td><td>A</td><td>0</td><td>1</td><td>0</td></tr><tr><td>1</td><td>red</td><td>B</td><td>0</td><td>0</td><td>1</td></tr><tr><td>2</td><td>blue</td><td>A</td><td>1</td><td>0</td><td>0</td></tr></tbody></table><h4 id="concat"><a href="#concat" class="headerlink" title="concat()"></a>concat()</h4><p><strong>连接内容 objs</strong></p><p>需要连接的数据，可以是多个 DataFrame 或者 Series。必传参数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Series 或 DataFrame 对象的序列或映射</span><br><span class="line">s1 = pd.Series([&#x27;a&#x27;, &#x27;b&#x27;])</span><br><span class="line">s2 = pd.Series([&#x27;c&#x27;, &#x27;d&#x27;])</span><br><span class="line">pd.concat([s1, s2])</span><br><span class="line"></span><br><span class="line"># df</span><br><span class="line">df1 = pd.DataFrame([[&#x27;a&#x27;, 1], [&#x27;b&#x27;, 2]], columns=[&#x27;letter&#x27;, &#x27;number&#x27;])</span><br><span class="line">df2 = pd.DataFrame([[&#x27;c&#x27;, 3], [&#x27;d&#x27;, 4]], columns=[&#x27;letter&#x27;, &#x27;number&#x27;])</span><br><span class="line">pd.concat([df1, df2])</span><br></pre></td></tr></table></figure><p><strong>轴方向 axis</strong></p><p>连接轴的方法，默认是 0，按行连接，追加在行后边，为 1 时追加到列后边。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># &#123;0/’index’, 1/’columns’&#125;, default 0</span><br><span class="line">pd.concat([df1, df4], axis=1) # 按列</span><br></pre></td></tr></table></figure><p><strong>合并方式 join</strong></p><p>其他轴上的数据是按交集（inner）还是并集（outer）进行合并。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># &#123;‘inner’, ‘outer’&#125;, default ‘outer’</span><br><span class="line">pd.concat([df1, df3], join=&quot;inner&quot;) # 按交集</span><br></pre></td></tr></table></figure><p><strong>保留索引 ignore_index</strong></p><p>是否保留原表索引，默认保留，为 True 会自动增加自然索引。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># bool, default False</span><br><span class="line">pd.concat([df1, df3], ignore_index=True) # 不保留索引</span><br></pre></td></tr></table></figure><h4 id="dropna"><a href="#dropna" class="headerlink" title="dropna()"></a>dropna()</h4><p>DataFrame.dropna(axis&#x3D;0, how&#x3D;’any’, thresh&#x3D;None, subset&#x3D;None, inplace&#x3D;False)</p><p>axis:</p><ul><li>axis&#x3D;0: 删除包含缺失值的行</li><li>axis&#x3D;1: 删除包含缺失值的列</li></ul><p>how: 与axis配合使用</p><ul><li>how&#x3D;‘any’ :只要有缺失值出现，就删除该行货列</li><li>how&#x3D;‘all’: 所有的值都缺失，才删除行或列</li></ul><p>thresh： axis中至少有thresh个非缺失值，否则删除<br>比如 axis&#x3D;0，thresh&#x3D;10：标识如果该行中非缺失值的数量小于10，将删除改行</p><p>subset: list<br>在哪些列中查看是否有缺失值</p><p>inplace: 是否在原数据上操作。如果为真，返回None否则返回新的copy，去掉了缺失值</p><h4 id="drop"><a href="#drop" class="headerlink" title="drop()"></a>drop()</h4><p>DataFrame.drop(labels&#x3D;None, axis&#x3D;0, index&#x3D;None, columns&#x3D;None, level&#x3D;None, inplace&#x3D;False, errors&#x3D;’raise’)</p><ul><li>labels: 要删除行或列的列表</li><li>axis: 0 行 ；1 列</li></ul><h4 id="fillna"><a href="#fillna" class="headerlink" title="fillna()"></a>fillna()</h4><p>DataFrame.fillna(value&#x3D;None, method&#x3D;None, axis&#x3D;None, inplace&#x3D;False, limit&#x3D;None, downcast&#x3D;None, **kwargs)</p><ul><li><p>value: scalar, dict, Series, or DataFrame<br>dict 可以指定每一行或列用什么值填充</p></li><li><p>method： {‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}, default None<br>在列上操作</p><ul><li>ffill &#x2F; pad: 使用前一个值来填充缺失值</li><li>backfill &#x2F; bfill :使用后一个值来填充缺失值</li></ul></li><li><p>limit 填充的缺失值个数限制。应该不怎么用</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用0代替所有的缺失值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.fillna(<span class="number">0</span>)</span><br><span class="line">    A   B   C   D</span><br><span class="line"><span class="number">0</span>   <span class="number">0.0</span> <span class="number">2.0</span> <span class="number">0.0</span> <span class="number">0</span></span><br><span class="line"><span class="number">1</span>   <span class="number">3.0</span> <span class="number">4.0</span> <span class="number">0.0</span> <span class="number">1</span></span><br><span class="line"><span class="number">2</span>   <span class="number">0.0</span> <span class="number">0.0</span> <span class="number">0.0</span> <span class="number">5</span></span><br><span class="line"><span class="number">3</span>   <span class="number">0.0</span> <span class="number">3.0</span> <span class="number">0.0</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用后边或前边的值填充缺失值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.fillna(method=<span class="string">&#x27;ffill&#x27;</span>)</span><br><span class="line">    A   B   C   D</span><br><span class="line"><span class="number">0</span>   NaN <span class="number">2.0</span> NaN <span class="number">0</span></span><br><span class="line"><span class="number">1</span>   <span class="number">3.0</span> <span class="number">4.0</span> NaN <span class="number">1</span></span><br><span class="line"><span class="number">2</span>   <span class="number">3.0</span> <span class="number">4.0</span> NaN <span class="number">5</span></span><br><span class="line"><span class="number">3</span>   <span class="number">3.0</span> <span class="number">3.0</span> NaN <span class="number">4</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;df.fillna(method=<span class="string">&#x27;bfill&#x27;</span>)</span><br><span class="line">     ABCD</span><br><span class="line"><span class="number">0</span><span class="number">3.0</span><span class="number">2.0</span>NaN<span class="number">0</span></span><br><span class="line"><span class="number">1</span><span class="number">3.0</span><span class="number">4.0</span>NaN<span class="number">1</span></span><br><span class="line"><span class="number">2</span>NaN<span class="number">3.0</span>NaN<span class="number">5</span></span><br><span class="line"><span class="number">3</span>NaN<span class="number">3.0</span>NaN<span class="number">4</span></span><br></pre></td></tr></table></figure><h4 id="csv操作"><a href="#csv操作" class="headerlink" title="csv操作"></a>csv操作</h4><ol><li>读csv不要索引</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&quot;filename.csv&quot;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>,index_col=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><ol start="2"><li>写csv不要索引</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.to_csv(<span class="string">&quot;xxx.csv&quot;</span>,index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><ol start="3"><li>删除有空值的行</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1 = df.dropna(subset=[<span class="string">&#x27;列名&#x27;</span>])</span><br></pre></td></tr></table></figure><ol start="4"><li>先把带有时间的列转为date_time格式，再进行排序。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1[<span class="string">&#x27;time&#x27;</span>] = pd.to_datetime(df1[<span class="string">&#x27;time&#x27;</span>])</span><br><span class="line">df1.sort_values(<span class="string">&#x27;time&#x27;</span>, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>inplace代表是否更改数据，默认是False，要保存结果的话需要inplace&#x3D;True。</p><ol start="5"><li>增加一列并赋值</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;xxx number&#x27;</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure><ol start="6"><li>插入列</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1.insert(<span class="number">3</span>, <span class="string">&#x27;users number&#x27;</span>, df2[<span class="string">&#x27;users number&#x27;</span>])</span><br><span class="line">//df.insert(插入到哪一列, <span class="string">&#x27;列名&#x27;</span>, another_df[<span class="string">&#x27;需要被插入的那一列&#x27;</span>])</span><br></pre></td></tr></table></figure><ol start="7"><li>Pandas sample()用于从DataFrame中随机选择行和列。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrame.sample(n=<span class="literal">None</span>, frac=<span class="literal">None</span>, replace=<span class="literal">False</span>, weights=<span class="literal">None</span>, random_state=<span class="literal">None</span>, axis=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p> 参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">n：这是一个可选参数, 由整数值组成, 并定义生成的随机行数。</span><br><span class="line">frac：它也是一个可选参数, 由浮点值组成, 并返回浮点值*数据帧值的长度。不能与参数n一起使用。</span><br><span class="line">replace：由布尔值组成。如果为true, 则返回带有替换的样本。替换的默认值为false。</span><br><span class="line">权重：它也是一个可选参数, 由类似于<span class="built_in">str</span>或ndarray的参数组成。默认值”无”将导致相等的概率加权。</span><br><span class="line">如果正在通过系列赛；它将与索引上的目标对象对齐。在采样对象中找不到的权重索引值将被忽略, 而在采样对象中没有权重的索引值将被分配零权重。</span><br><span class="line">如果在轴= <span class="number">0</span>时正在传递DataFrame, 则返回<span class="number">0</span>。它将接受列的名称。</span><br><span class="line">如果权重是系列；然后, 权重必须与被采样轴的长度相同。</span><br><span class="line">如果权重不等于<span class="number">1</span>；它将被标准化为<span class="number">1</span>的总和。</span><br><span class="line">权重列中的缺失值被视为零。</span><br><span class="line">权重栏中不允许无穷大。</span><br><span class="line">random_state：它也是一个可选参数, 由整数或numpy.random.RandomState组成。如果值为<span class="built_in">int</span>, 则为随机数生成器或numpy RandomState对象设置种子。</span><br><span class="line">axis：它也是由整数或字符串值组成的可选参数。 <span class="number">0</span>或”行”和<span class="number">1</span>或”列”。</span><br></pre></td></tr></table></figure><ol start="8"><li>通过.fillna()填充空值。</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs = inputs.fillna(inputs.mean())</span><br></pre></td></tr></table></figure><h4 id="pandas按列遍历Dataframe"><a href="#pandas按列遍历Dataframe" class="headerlink" title="pandas按列遍历Dataframe"></a>pandas按列遍历Dataframe</h4><ul><li>iterrows(): 按行遍历，将DataFrame的每一行迭代为(index, Series)对，可以通过row[name]对元素进行访问。</li><li>itertuples(): 按行遍历，将DataFrame的每一行迭代为元祖，可以通过row[name]对元素进行访问，比iterrows()效率高。</li><li>iteritems():按列遍历，将DataFrame的每一列迭代为(列名, Series)对，可以通过row[index]对元素进行访问。</li></ul><p>示例数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">inp = [&#123;<span class="string">&#x27;c1&#x27;</span>:<span class="number">10</span>, <span class="string">&#x27;c2&#x27;</span>:<span class="number">100</span>&#125;, &#123;<span class="string">&#x27;c1&#x27;</span>:<span class="number">11</span>, <span class="string">&#x27;c2&#x27;</span>:<span class="number">110</span>&#125;, &#123;<span class="string">&#x27;c1&#x27;</span>:<span class="number">12</span>, <span class="string">&#x27;c2&#x27;</span>:<span class="number">123</span>&#125;]</span><br><span class="line">df = pd.DataFrame(inp)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190227143422984.png" alt="在这里插入图片描述"></p><p><strong>按行遍历iterrows():</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index, row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    <span class="built_in">print</span>(index) <span class="comment"># 输出每行的索引值</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190227143612870.png" alt="在这里插入图片描述"></p><p>row[‘name’]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于每一行，通过列名name访问对应的元素</span></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    <span class="built_in">print</span>(row[<span class="string">&#x27;c1&#x27;</span>], row[<span class="string">&#x27;c2&#x27;</span>]) <span class="comment"># 输出每一行</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190227143716567.png" alt="在这里插入图片描述"></p><p><strong>按行遍历itertuples():</strong><br>getattr(row, ‘name’)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df.itertuples():</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">getattr</span>(row, <span class="string">&#x27;c1&#x27;</span>), <span class="built_in">getattr</span>(row, <span class="string">&#x27;c2&#x27;</span>)) <span class="comment"># 输出每一行</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190227143835738.png" alt="在这里插入图片描述"></p><p><strong>按列遍历iteritems():</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index, row <span class="keyword">in</span> df.iteritems():</span><br><span class="line">    <span class="built_in">print</span>(index) <span class="comment"># 输出列名</span></span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>c1<br>c2</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df.iteritems():</span><br><span class="line">    <span class="built_in">print</span>(row[<span class="number">0</span>], row[<span class="number">1</span>], row[<span class="number">2</span>]) <span class="comment"># 输出各列</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190227144037269.png" alt="在这里插入图片描述"></p><h3 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h3><h4 id="axis理解"><a href="#axis理解" class="headerlink" title="axis理解"></a>axis理解</h4><p>参考： <a href="https://zhuanlan.zhihu.com/p/31275071">https://zhuanlan.zhihu.com/p/31275071</a></p><p>简单来说就是：</p><ul><li><strong>Axis就是数组层级</strong></li><li><strong>设axis&#x3D;i，则Numpy沿着第i个下标变化的方向进行操作</strong></li></ul><h4 id="reshape"><a href="#reshape" class="headerlink" title="reshape()"></a>reshape()</h4><p>重新定义矩阵的形状</p><p><strong>相当于pytorch中的view()</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">v1 = torch.<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">16</span>)</span><br><span class="line">v2 = v1.view(<span class="number">4</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>参数使用-1，<strong>view中一个参数定为-1，代表动态调整这个维度上的元素个数，以保证元素的总数不变</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">v1 = torch.<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">16</span>)</span><br><span class="line">v2 = v1.view(-<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 代码效果同上</span></span><br></pre></td></tr></table></figure><h4 id="np-r-和-np-c"><a href="#np-r-和-np-c" class="headerlink" title="np.r_ 和 np.c_"></a>np.r_ 和 np.c_</h4><p>np.r_是按列连接两个<a href="https://so.csdn.net/so/search?q=%E7%9F%A9%E9%98%B5&spm=1001.2101.3001.7020">矩阵</a>，就是把两矩阵上下相加，要求列数相等。</p><p>np.c_是按行连接两个矩阵，就是把两矩阵左右相加，要求行数相等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line">b=np.array([[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line"> </span><br><span class="line">&gt;&gt;a</span><br><span class="line">Out[<span class="number">4</span>]: </span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"> </span><br><span class="line">&gt;&gt;b</span><br><span class="line">Out[<span class="number">5</span>]: </span><br><span class="line">array([[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line"> </span><br><span class="line">c=np.c_[a,b]</span><br><span class="line"> </span><br><span class="line">&gt;&gt;c</span><br><span class="line">Out[<span class="number">7</span>]: </span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">       [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure><h4 id="eye"><a href="#eye" class="headerlink" title="eye()"></a>eye()</h4><p>numpy.eye(N,M&#x3D;None,k&#x3D;0,dtype&#x3D;&lt;class ‘float’&gt;,order&#x3D;’C)</p><p>返回的是一个二维2的数组(N,M)，对角线的地方为1，其余的地方为0.</p><p>参数介绍：</p><p>（1）N:int型，表示的是输出的行数</p><p>（2）M：int型，可选项，输出的列数，如果没有就默认为N</p><p>（3）k：int型，可选项，对角线的下标，默认为0表示的是主对角线，负数表示的是低对角，正数表示的是高对角。</p><p>（4）dtype：数据的类型，可选项，返回的数据的数据类型</p><p>（5）order：{‘C’，‘F’}，可选项，也就是输出的数组的形式是按照C语言的行优先’C’，还是按照Fortran形式的列优先‘F’存储在内存中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line">a=np.eye(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"> </span><br><span class="line">a=np.eye(<span class="number">4</span>,k=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"> </span><br><span class="line">a=np.eye(<span class="number">4</span>,k=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"> </span><br><span class="line">a=np.eye(<span class="number">4</span>,k=-<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">[[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]]</span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure><h5 id="深度学习高级用法"><a href="#深度学习高级用法" class="headerlink" title="深度学习高级用法"></a>深度学习高级用法</h5><p>将数组转化为 one-hot形式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">labels = np.array([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">0</span>],[<span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;label的大小:&#x27;</span>,labels.shape,<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#因为我们的类别是从0-2，所以这里是3个类</span></span><br><span class="line">a=np.eye(<span class="number">3</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;如果对应的类别号是1，那么转成one-hot的形式&quot;</span>,a,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"> </span><br><span class="line">a=np.eye(<span class="number">3</span>)[<span class="number">2</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;如果对应的类别号是2，那么转成one-hot的形式&quot;</span>,a,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"> </span><br><span class="line">a=np.eye(<span class="number">3</span>)[<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1转成one-hot的数组的第一个数字是：&quot;</span>,a,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#这里和上面的结果的区别，注意!!!</span></span><br><span class="line">a=np.eye(<span class="number">3</span>)[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>]]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;如果对应的类别号是1,2,0,1，那么转成one-hot的形式\n&quot;</span>,a)</span><br><span class="line"> </span><br><span class="line">res=np.eye(<span class="number">3</span>)[labels.reshape(-<span class="number">1</span>)]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;labels转成one-hot形式的结果：\n&quot;</span>,res,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;labels转化成one-hot后的大小：&quot;</span>,res.shape)</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">labels的大小： (<span class="number">4</span>, <span class="number">1</span>) </span><br><span class="line"> </span><br><span class="line">如果对应的类别号是<span class="number">1</span>，那么转成one-hot的形式 [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>] </span><br><span class="line"> </span><br><span class="line">如果对应的类别号是<span class="number">2</span>，那么转成one-hot的形式 [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>] </span><br><span class="line"> </span><br><span class="line"><span class="number">1</span>转成one-hot的数组的第一个数字是： <span class="number">0.0</span> </span><br><span class="line"> </span><br><span class="line">如果对应的类别号是<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>，那么转成one-hot的形式</span><br><span class="line"> [[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]]</span><br><span class="line">labels转成one-hot形式的结果：</span><br><span class="line"> [[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]] </span><br><span class="line"> </span><br><span class="line">labels转化成one-hot后的大小： (<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#注：</span></span><br><span class="line">label.reshape(-<span class="number">1</span>)</span><br><span class="line">--&gt;   array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>])变成了一维数组</span><br></pre></td></tr></table></figure><h4 id="identity"><a href="#identity" class="headerlink" title="identity()"></a>identity()</h4><p>与eye()的区别在于只能创建方阵</p><h4 id="np-linalg-norm求范数"><a href="#np-linalg-norm求范数" class="headerlink" title="np.linalg.norm求范数"></a>np.linalg.norm求范数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_norm=np.linalg.norm(x, <span class="built_in">ord</span>=<span class="literal">None</span>, axis=<span class="literal">None</span>, keepdims=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">6</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="comment">#默认参数ord=None，axis=None，keepdims=False</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;默认参数(矩阵整体元素平方和开根号，不保留矩阵二维特性)：&quot;</span>,np.linalg.norm(x)</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;矩阵整体元素平方和开根号，保留矩阵二维特性：&quot;</span>,np.linalg.norm(x,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;矩阵每个行向量求向量的2范数：&quot;</span>,np.linalg.norm(x,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;矩阵每个列向量求向量的2范数：&quot;</span>,np.linalg.norm(x,axis=<span class="number">0</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;矩阵1范数：&quot;</span>,np.linalg.norm(x,<span class="built_in">ord</span>=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;矩阵2范数：&quot;</span>,np.linalg.norm(x,<span class="built_in">ord</span>=<span class="number">2</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;矩阵∞范数：&quot;</span>,np.linalg.norm(x,<span class="built_in">ord</span>=np.inf,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;矩阵每个行向量求向量的1范数：&quot;</span>,np.linalg.norm(x,<span class="built_in">ord</span>=<span class="number">1</span>,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="squeeze"><a href="#squeeze" class="headerlink" title="squeeze()"></a>squeeze()</h4><p><strong>作用</strong>：从数组的形状中删除单维度条目，即把shape中为1的维度去掉,<strong>对非单维的维度不起作用</strong></p><p>np.squeeze(a, axis &#x3D; None)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1）a表示输入的数组；</span><br><span class="line">2）axis用于指定需要删除的维度，但是指定的维度必须为单维度，否则将会报错；</span><br><span class="line">3）axis的取值可为None 或 int 或 tuple of ints, 可选。若axis为空，则删除所有单维度的条目；</span><br><span class="line">4）返回值：数组</span><br><span class="line">5) 不会修改原数组；</span><br></pre></td></tr></table></figure><p>eg:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(10).reshape(1, 10)</span><br><span class="line"># array([[0,1,2,3,4,5,6,7,8,9]])</span><br><span class="line"></span><br><span class="line">a.shape</span><br><span class="line"># (1,10)</span><br><span class="line"></span><br><span class="line">b = np.squeeze(a)</span><br><span class="line"># array([0,1,2,3,4,5,6,7,8,9])</span><br><span class="line"></span><br><span class="line">b.shape</span><br><span class="line"># (10,)</span><br></pre></td></tr></table></figure><h4 id="dot"><a href="#dot" class="headerlink" title="dot()"></a>dot()</h4><p>向量点积    和     多维矩阵乘法</p><h4 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h4><p><strong>一维数组</strong></p><p>通过冒号分隔切片参数 start:stop:step 来进行切片操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = a[<span class="number">2</span>:<span class="number">7</span>:<span class="number">2</span>]   <span class="comment"># 从索引 2 开始到索引 7 停止，间隔为 2</span></span><br></pre></td></tr></table></figure><p>冒号 : 的解释：如果只放置一个参数，如 [2]，将返回与该索引相对应的单个元素。如果为 [2:]，表示从该索引开始以后的所有项都将被提取。如果使用了两个参数，如 [2:7]，那么则提取两个索引(不包括停止索引)之间的项。</p><p><strong>注意1：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">7</span>:]</span><br><span class="line">array([<span class="number">8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">7</span>]</span><br><span class="line"><span class="number">8</span></span><br></pre></td></tr></table></figure><p><strong>注意2：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(a[<span class="number">1</span>:<span class="number">3</span>])  <span class="comment">#从索引1开始，也就是第二个元素2，到索引3，不包括索引3</span></span><br><span class="line">[<span class="number">2</span> <span class="number">3</span>]</span><br></pre></td></tr></table></figure><p><strong>二维数组</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>]</span><br><span class="line">array([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>:]</span><br><span class="line">array([[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">      [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[:<span class="number">2</span>]</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">      [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>:<span class="number">2</span>]</span><br><span class="line">array([[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#进阶</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>,]</span><br><span class="line">array([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>:,]</span><br><span class="line">array([[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">      [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[:<span class="number">2</span>,]</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">      [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>:<span class="number">2</span>,]</span><br><span class="line">array([[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>总结：</p><hr><p>这是numpy的切片操作，一般结构如num[a:b,c:d]，分析时以逗号为分隔符，<br>逗号之前为要取的num行的下标范围(a到b-1)，逗号之后为要取的num列的下标范围(c到d-1)；<br>前面是行索引，后面是列索引。<br>如果是这种num[:b,c:d]，a的值未指定，那么a为最小值0；<br>如果是这种num[a:,c:d]，b的值未指定，那么b为最大值；c、d的情况同理可得。</p><hr><p>所以重点就是看逗号，没逗号，就是看行了，冒号呢，就看成一维数组的形式啦。那上面逗号后面没有树，也就是不对列操作咯。<br>当然也可以这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a[:<span class="number">2</span>:<span class="number">1</span>]</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure><p>首先没有逗号，那切片就是只看行了，这里的意思是，从0开始到2（2不算），间隔为1。</p><h3 id="结合matplotlib画图"><a href="#结合matplotlib画图" class="headerlink" title="结合matplotlib画图"></a>结合matplotlib画图</h3><p><code>%matplotlib inline</code> 可以在Ipython编译器里直接使用，功能是可以内嵌绘图，并且可以省略掉plt.show()这一步。</p><h4 id="pyplot库"><a href="#pyplot库" class="headerlink" title="pyplot库"></a>pyplot库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p>绘制直线</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">xpoints = np.array([<span class="number">0</span>, <span class="number">6</span>])</span><br><span class="line">ypoints = np.array([<span class="number">0</span>, <span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;TITLE&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x - label&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y - label&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(xpoints, ypoints)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.grid()        网格线</span></span><br><span class="line"><span class="comment"># plt.grid(axis=&#x27;x&#x27;)        设置y轴方向显示网格线</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>plt.plot()函数是绘制二维函数的最基本函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>plot(x, y)        <span class="comment"># 创建 y 中数据与 x 中对应值的二维线图，使用默认样式</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>plot(x, y, <span class="string">&#x27;bo&#x27;</span>)  <span class="comment"># 创建 y 中数据与 x 中对应值的二维线图，使用蓝色实心圈绘制</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>plot(y)           <span class="comment"># x 的值为 0..N-1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>plot(y, <span class="string">&#x27;r+&#x27;</span>)     <span class="comment"># 使用红色 + 号</span></span><br></pre></td></tr></table></figure><h4 id="绘制多图"><a href="#绘制多图" class="headerlink" title="绘制多图"></a>绘制多图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot 1:</span></span><br><span class="line">x = np.array([<span class="number">0</span>, <span class="number">6</span>])</span><br><span class="line">y = np.array([<span class="number">0</span>, <span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">&quot;plot 1&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot 2:</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">&quot;plot 2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot 3:</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">&quot;plot 3&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot 4:</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">&quot;plot 4&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.suptitle(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/%E4%B8%8B%E8%BD%BD.png" alt="plt"></p><p>注： <code>plt.subplot(2,2,1)</code> &lt;&#x3D;&gt; <code>plt.subplot(221)</code></p><h4 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">23</span>, <span class="number">18</span>])</span><br><span class="line">sizes = np.array([<span class="number">20</span>,<span class="number">50</span>,<span class="number">100</span>,<span class="number">200</span>,<span class="number">500</span>,<span class="number">1000</span>,<span class="number">60</span>,<span class="number">90</span>])</span><br><span class="line">plt.scatter(x, y, s=sizes)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://www.runoob.com/wp-content/uploads/2021/07/pl-scatter-5.png" alt="img"></p><h4 id="柱形图"><a href="#柱形图" class="headerlink" title="柱形图"></a>柱形图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="string">&quot;num-1&quot;</span>, <span class="string">&quot;num-2&quot;</span>, <span class="string">&quot;num-3&quot;</span>, <span class="string">&quot;num-4&quot;</span>])</span><br><span class="line">y = np.array([<span class="number">12</span>, <span class="number">22</span>, <span class="number">6</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">plt.bar(x,y)</span><br><span class="line"><span class="comment"># plt.barh(x,y)水平柱状图</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="饼图"><a href="#饼图" class="headerlink" title="饼图"></a>饼图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">y = np.array([<span class="number">35</span>, <span class="number">25</span>, <span class="number">25</span>, <span class="number">15</span>])</span><br><span class="line"></span><br><span class="line">plt.pie(y,</span><br><span class="line">        labels=[<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;C&#x27;</span>,<span class="string">&#x27;D&#x27;</span>], <span class="comment"># 设置饼图标签</span></span><br><span class="line">        colors=[<span class="string">&quot;#d5695d&quot;</span>, <span class="string">&quot;#5d8ca8&quot;</span>, <span class="string">&quot;#65a479&quot;</span>, <span class="string">&quot;#a564c9&quot;</span>], <span class="comment"># 设置饼图颜色</span></span><br><span class="line">       )</span><br><span class="line">plt.title(<span class="string">&quot;Pie Test&quot;</span>) <span class="comment"># 设置标题</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="三维图"><a href="#三维图" class="headerlink" title="三维图"></a>三维图</h4><p>最基本的三维图是线图与<a href="https://so.csdn.net/so/search?q=%E6%95%A3%E7%82%B9%E5%9B%BE&spm=1001.2101.3001.7020">散点图</a>，可以用<code>ax.plot3D</code>和<code>ax.scatter3D</code>函数来创建</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制三角螺旋线</span></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ax = plt.axes(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#三维线的数据</span></span><br><span class="line">zline = np.linspace(<span class="number">0</span>, <span class="number">15</span>, <span class="number">1000</span>)</span><br><span class="line">xline = np.sin(zline)</span><br><span class="line">yline = np.cos(zline)</span><br><span class="line">ax.plot3D(xline, yline, zline, <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 三维散点的数据</span></span><br><span class="line">zdata = <span class="number">15</span> * np.random.random(<span class="number">100</span>)</span><br><span class="line">xdata = np.sin(zdata) + <span class="number">0.1</span> * np.random.randn(<span class="number">100</span>)</span><br><span class="line">ydata = np.cos(zdata) + <span class="number">0.1</span> * np.random.randn(<span class="number">100</span>)</span><br><span class="line">ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap=<span class="string">&#x27;Greens&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20230104104737680.png" alt="image-20230104104737680"></p><h4 id="热图"><a href="#热图" class="headerlink" title="热图"></a>热图</h4><p>imshow()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">plt.imshow(X)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20230104164258648.png" alt="image-20230104164258648"></p><p>博客：<a href="https://blog.csdn.net/qq_21763381/article/details/100169288">https://blog.csdn.net/qq_21763381/article/details/100169288</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> python三剑客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>machine_code（1）</title>
      <link href="/2022/12/27/csapp-machine-code-md/"/>
      <url>/2022/12/27/csapp-machine-code-md/</url>
      
        <content type="html"><![CDATA[<h4 id="处理器模型发展"><a href="#处理器模型发展" class="headerlink" title="处理器模型发展"></a>处理器模型发展</h4><p><a href="https://hansimov.gitbook.io/csapp/part1/ch03-machine-level-representing-of-programs/3.1-a-historial-perspective">https://hansimov.gitbook.io/csapp/part1/ch03-machine-level-representing-of-programs/3.1-a-historial-perspective</a></p><h4 id="摩尔定律"><a href="#摩尔定律" class="headerlink" title="摩尔定律"></a>摩尔定律</h4><h4 id="芯片构造"><a href="#芯片构造" class="headerlink" title="芯片构造"></a>芯片构造</h4><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/image-20221227205856257.png" alt="image-20221227205856257"></p><p>当时标准桌面型号有四个核心，服务器级别的机器有八个核心（上图）</p><p>芯片周围连接外围设备的接口：</p><ul><li>DDR是连接到主存的方式，即所谓的DRAM（Dynamic动态 RAM随机访问机）</li><li>PCI是一种同步的独立于处理器的32位或64位局部总线，主要用于连接显示卡、网卡、声卡</li><li>SATA是与不同类型盘连接</li><li>USB接口与USB设备连接</li><li>ethernet网络连接</li></ul><p>集成到芯片上的不止是处理器还有很多逻辑单元</p><h4 id="处理器架构"><a href="#处理器架构" class="headerlink" title="处理器架构"></a>处理器架构</h4><table><thead><tr><th><strong>架构</strong></th><th><strong>特点</strong></th><th><strong>代表性的厂商</strong></th><th><strong>运营机构</strong></th></tr></thead><tbody><tr><td><strong>X86</strong></td><td><strong>性能高，速度快，兼容性好</strong></td><td><strong>英特尔，AMD</strong></td><td><strong>英特尔</strong></td></tr><tr><td><strong>ARM</strong></td><td><strong>成本低，低功耗</strong></td><td><strong>苹果，谷歌，IBM，华为</strong></td><td><strong>英国ARM公司</strong></td></tr><tr><td><strong>RISC-V</strong></td><td><strong>模块化，极简，可拓展</strong></td><td><strong>三星，英伟达，西部数据</strong></td><td><strong>RISC-V基金会</strong></td></tr><tr><td><strong>MIPS</strong></td><td><strong>简洁，优化方便，高拓展性</strong></td><td><strong>龙芯</strong></td><td><strong>MIPS科技公司</strong></td></tr></tbody></table><p><strong>X86在PC上占据大部分份额，ARM在手机处理器上占据绝对份额</strong></p><h4 id="c代码运行的过程"><a href="#c代码运行的过程" class="headerlink" title="c代码运行的过程"></a>c代码运行的过程</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gcc -Og -S sum.c</span><br><span class="line"><span class="comment">// 将c代码转换为assembly代码</span></span><br></pre></td></tr></table></figure><p>-s为-stop停在把c转化为汇编的时刻</p><p>-Og是我希望编译器做什么样的优化的规范，这样才能读懂</p><p>具体过程：<a href="https://www.cnblogs.com/carpenterlee/p/5994681.html">https://www.cnblogs.com/carpenterlee/p/5994681.html</a></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objdump -d sum &gt; sum.d</span><br></pre></td></tr></table></figure><p>反汇编，sum.d的内容</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a.out:     file format elf64-x86<span class="number">-64</span></span><br><span class="line">     </span><br><span class="line">Disassembly of section .init:</span><br><span class="line"></span><br><span class="line"><span class="number">0000000000001000</span> &lt;_init&gt;:</span><br><span class="line">    <span class="number">1000</span>:   f3 <span class="number">0f</span> <span class="number">1</span>e fa             endbr64</span><br><span class="line">    <span class="number">1004</span>:   <span class="number">48</span> <span class="number">83</span> ec <span class="number">08</span>             sub    $<span class="number">0x8</span>,%rsp</span><br><span class="line">    <span class="number">1008</span>:   <span class="number">48</span> <span class="number">8b</span> <span class="number">05</span> d9 <span class="number">2f</span> <span class="number">00</span> <span class="number">00</span>    mov    <span class="number">0x2fd9</span>(%rip),%rax        # <span class="number">3f</span>e8 &lt;__gmon_start__&gt;</span><br><span class="line">    <span class="number">100f</span>:   <span class="number">48</span> <span class="number">85</span> c0                test   %rax,%rax</span><br><span class="line">    <span class="number">1012</span>:   <span class="number">74</span> <span class="number">02</span>                   je     <span class="number">1016</span> &lt;_init+<span class="number">0x16</span>&gt;</span><br><span class="line">    <span class="number">1014</span>:   ff d0                   callq  *%rax</span><br><span class="line">    <span class="number">1016</span>:   <span class="number">48</span> <span class="number">83</span> c4 <span class="number">08</span>             add    $<span class="number">0x8</span>,%rsp</span><br><span class="line">    <span class="number">101</span>a:   c3                      retq</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p><strong>反汇编程序无法访问源代码，甚至无法访问汇编代码，它只是通过实际目标代码文件中的字节来辨别出来的</strong></p><p>或者使用GDB</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; gdb</span><br><span class="line">...</span><br><span class="line">(gdb)</span><br><span class="line">(gdb) disassemble bitXor</span><br><span class="line">Dump of assembler code <span class="keyword">for</span> function bitXor:</span><br><span class="line">   <span class="number">0x0000000000001149</span> &lt;+<span class="number">0</span>&gt;:     endbr64</span><br><span class="line">   <span class="number">0x000000000000114d</span> &lt;+<span class="number">4</span>&gt;:     push   %rbp</span><br><span class="line">   <span class="number">0x000000000000114e</span> &lt;+<span class="number">5</span>&gt;:     mov    %rsp,%rbp</span><br><span class="line">   <span class="number">0x0000000000001151</span> &lt;+<span class="number">8</span>&gt;:     mov    %edi,<span class="number">-0x4</span>(%rbp)</span><br><span class="line">   <span class="number">0x0000000000001154</span> &lt;+<span class="number">11</span>&gt;:    mov    %esi,<span class="number">-0x8</span>(%rbp)</span><br><span class="line">   <span class="number">0x0000000000001157</span> &lt;+<span class="number">14</span>&gt;:    mov    <span class="number">-0x4</span>(%rbp),%eax</span><br><span class="line">   <span class="number">0x000000000000115a</span> &lt;+<span class="number">17</span>&gt;:    xor    <span class="number">-0x8</span>(%rbp),%eax</span><br><span class="line">   <span class="number">0x000000000000115d</span> &lt;+<span class="number">20</span>&gt;:    pop    %rbp</span><br><span class="line">   <span class="number">0x000000000000115e</span> &lt;+<span class="number">21</span>&gt;:    retq</span><br><span class="line">End of assembler dump.</span><br></pre></td></tr></table></figure><p><strong>此方法前面显示的是16进制地址而非像objdum那样的字节级编码</strong></p><h4 id="Assembly-Characteristics-Data-Types-汇编语言特性"><a href="#Assembly-Characteristics-Data-Types-汇编语言特性" class="headerlink" title="Assembly Characteristics: Data Types 汇编语言特性"></a>Assembly Characteristics: Data Types 汇编语言特性</h4><ul><li>“Integer” data type of 1,2,4,or 8 bytes，不区分unsigned和signed</li><li>floating point 有4,8,10,bytes</li><li>没有数组以及一些数据结构，只是内存中连续存储的单元</li></ul><h4 id="x86-64-Integer-Registers"><a href="#x86-64-Integer-Registers" class="headerlink" title="x86-64 Integer Registers"></a>x86-64 Integer Registers</h4><p>有16个寄存器</p><p><img src="https://img-blog.csdnimg.cn/20190723112517340.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p>注意到 %r代表62位操作 %e代表了32位的操作，%e版本只是%r实体的低32位。</p><p><strong>%rsp寄存器存的是栈指针，它能告诉你程序执行到哪儿了</strong></p><h4 id="移动数据"><a href="#移动数据" class="headerlink" title="移动数据"></a>移动数据</h4><h5 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">moveq Source Dest</span><br></pre></td></tr></table></figure><h5 id="操作数类型"><a href="#操作数类型" class="headerlink" title="操作数类型"></a>操作数类型</h5><p><img src="https://img-blog.csdnimg.cn/20190723113325695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><h5 id="操作数组合"><a href="#操作数组合" class="headerlink" title="操作数组合"></a>操作数组合</h5><p><img src="https://img-blog.csdnimg.cn/20190723113456800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><h4 id="理解Swap-函数"><a href="#理解Swap-函数" class="headerlink" title="理解Swap()函数"></a>理解Swap()函数</h4><p><img src="https://img-blog.csdnimg.cn/20190723125635539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p>使用x86-64的时候，函数参数总是出现在某些特定的寄存器中，**%rdi将是第一个参数寄存器，%rsi将是第二个参数寄存器**。最多可以有6个</p><h4 id="完整的内存地址模式"><a href="#完整的内存地址模式" class="headerlink" title="完整的内存地址模式"></a>完整的内存地址模式</h4><p><img src="https://img-blog.csdnimg.cn/20190723125841927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p>下面是内存完整模式的一个例子</p><p><img src="https://img-blog.csdnimg.cn/20190723130008460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><h4 id="地址计算"><a href="#地址计算" class="headerlink" title="地址计算"></a>地址计算</h4><p><img src="https://img-blog.csdnimg.cn/20190723131714576.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20221229121353910.png" alt="image-20221229121353910"></p><p><img src="C:\Users\xhm\AppData\Roaming\Typora\typora-user-images\image-20221229121547636.png" alt="image-20221229121547636"></p><p><img src="https://img-blog.csdnimg.cn/20190723131745599.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><h3 id="control"><a href="#control" class="headerlink" title="control"></a>control</h3><h4 id="处理器状态-x86-64-Partial"><a href="#处理器状态-x86-64-Partial" class="headerlink" title="处理器状态 (x86-64, Partial)"></a>处理器状态 (x86-64, Partial)</h4><p><img src="https://img-blog.csdnimg.cn/20190724111040785.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p>图上的CF,ZF,SF,OF就是微机学过的状态位，其中各自代表的意思如下</p><p><img src="https://img-blog.csdnimg.cn/20190724111626651.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20190724112028471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20190724112037452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/2019072411205365.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5kZXpodXRp,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20190724112113371.png" alt="img"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>csapp_CouseOverview</title>
      <link href="/2022/12/23/csapp-CouseOverview-md/"/>
      <url>/2022/12/23/csapp-CouseOverview-md/</url>
      
        <content type="html"><![CDATA[<hr><hr><p>写在前面; 今天是12.23，经历了新冠的抗争后开始学习csapp，教材课程及练习都使用CMU 15-213。希望能在这个寒假学习完毕</p><hr><h3 id="Ints-are-not-Interagers-Float-are-not-Reals"><a href="#Ints-are-not-Interagers-Float-are-not-Reals" class="headerlink" title="Ints are not Interagers, Float are not Reals"></a>Ints are not Interagers, Float are not Reals</h3><p>Example 1:</p><ul><li>Float’s : Yes!</li><li>Int’s:<ul><li>40000 * 40000 -&gt; 1600000000</li><li>50000 * 50000 -&gt; ??</li></ul></li></ul><p>Example 2:  Is (x + y) + z &#x3D; x + (y  + z )   ?</p><ul><li>Int’s:: Yes!</li><li>Float’s : Not Sure!!</li></ul><p>Float will throw the more number!</p><h3 id="Memory-Referencing-Bug-Example"><a href="#Memory-Referencing-Bug-Example" class="headerlink" title="Memory Referencing Bug Example"></a>Memory Referencing Bug Example</h3><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202212231500118.png" alt="image-20221223150035984"></p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202212231513136.png" alt="image-20221223151356105"></p><p><strong>For sure, this is influenced by your gcc version and IDE</strong>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> csapp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch深度学习总结</title>
      <link href="/2022/11/09/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93-md/"/>
      <url>/2022/11/09/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93-md/</url>
      
        <content type="html"><![CDATA[<h2 id="深度学习流程"><a href="#深度学习流程" class="headerlink" title="深度学习流程"></a>深度学习流程</h2><h3 id="流程简述"><a href="#流程简述" class="headerlink" title="流程简述"></a>流程简述</h3><pre class="mermaid">graph TD a[Build the dataset]-->b[preprocessing] b[preprocessing]-->c[training and validation] c-->d[Parameter tuning && optimization]</pre><h3 id="构建数据集："><a href="#构建数据集：" class="headerlink" title="构建数据集："></a>构建数据集：</h3><ol><li>流程</li></ol><p>first step: 收集数据：去大量抽样调查收集，爬虫（<a href="https://github.com/MrS0m30n3/youtube-dl-gui">youtube爬虫工具</a>），众包（<del>花钱找工具人</del>）等等</p><p>second: 数据格式整理</p><p>third: 导入代码进行处理</p><ol start="2"><li><p>code</p><p>一般处理csv或者json等格式的文本格式文件，下为举例</p><p>sklearn</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_json(<span class="string">&#x27;data.json&#x27;</span>)</span><br><span class="line"><span class="comment"># data = pd.read_csv(&#x27;data.csv&#x27;)</span></span><br></pre></td></tr></table></figure><p>pytorch</p><p>​pytorch的项目预处理的时候可以用pandas、json等库处理，之后生成新的文件在构建模型前构造DataSet和DataLoader时直接读取数据集来load</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = MyDataset(csv_file=<span class="string">&#x27;../data/data.csv&#x27;</span>,</span><br><span class="line">root_dir=<span class="string">&#x27;../data&#x27;</span>,</span><br><span class="line">transform=torchvision.transforms.ToTensor())</span><br></pre></td></tr></table></figure></li></ol><h3 id="数据集划分："><a href="#数据集划分：" class="headerlink" title="数据集划分："></a>数据集划分：</h3><ol><li>基本知识</li></ol><ul><li><p>训练集（Train Set）: 模型用于训练和调整模型参数</p></li><li><p>验证集（Validation Set）: 用来验证模型精度和调整</p></li><li><p>测试集（Test Set）: 验证模型的泛化能力</p></li></ul><blockquote><p>训练集和验证集有时候是从同一数据集中分开的，但是在划分验证集时需要注意验证集的分布需和测试集尽量保持一致，保证其泛化性</p></blockquote><p>几种划分方式：</p><ul><li><p>留出法（Hold-Out）：直接将训练集划分为新的训练集和验证集。优点简单。缺点只得到了一份验证集，有可能导致模型在验证集上过拟合，适用于数据量比较大点的情况。</p></li><li><p>交叉验证法（Cross Validation,CV）：将训练集划分成K份，将其中的K-1份作为训练集，剩余的1份作为验证集，循环K训练。这种划分方式是所有的训练集都是验证集，最终模型验证精度是K份平均得到。这种方式的优点是验证集精度比较可靠，训练K次可以得到K个有多样性差异的模型；CV验证的缺点是需要训练K次，不适合数据量很大的情况。</p></li><li><p>自助采样法（BootStrap）：通过有放回的采样方式得到新的训练集和验证集，每次的训练集和验证集都是有区别的。这种划分方式一般适用于数据量较小的情况。</p></li></ul><ol start="2"><li><p>code: </p><p>sklearn</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line">os.chdir(<span class="string">&#x27;breast_cancer_data&#x27;</span>)</span><br><span class="line">data = pd.read_csv(<span class="string">r&#x27;data.csv&#x27;</span>)</span><br><span class="line">data.drop(<span class="string">&#x27;Unnamed: 32&#x27;</span>,inplace = <span class="literal">True</span>,axis = <span class="number">1</span>)</span><br><span class="line">data.drop(<span class="string">&#x27;id&#x27;</span>,inplace = <span class="literal">True</span>,axis=<span class="number">1</span>)</span><br><span class="line">y = data[<span class="string">&#x27;diagnosis&#x27;</span>]</span><br><span class="line">x = data.drop(<span class="string">&#x27;diagnosis&#x27;</span>,axis = <span class="number">1</span>)</span><br><span class="line">model = RandomForestClassifier()</span><br></pre></td></tr></table></figure><p>留出法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">14x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><p>k折交叉验证：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line">kf = KFold(n_splits = <span class="number">10</span>)</span><br><span class="line">accuracy = []</span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> kf.split(x):</span><br><span class="line">     x_train, x_test = x.loc[train_index],x.loc[test_index]</span><br><span class="line">     y_train, y_test = y.loc[train_index],y.loc[test_index]</span><br><span class="line">     model.fit(x_train, y_train)</span><br><span class="line">     prediction = model.predict(x_test)</span><br><span class="line">     acc=metrics.accuracy_score(predocton, y_test)</span><br><span class="line">     accuracy,append(acc)</span><br><span class="line"><span class="built_in">print</span>(accuracy)</span><br><span class="line"><span class="built_in">print</span>(np.average(accuracy))</span><br></pre></td></tr></table></figure><p>pytorch:</p><p><strong>torch.utils.data.Subset</strong>或者<strong>random_split</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])</span><br></pre></td></tr></table></figure><p>或者自定义分类数据集</p><p>eg: 文本分类中可以根据文本数字先进行排序然后按照顺序每10个前9个放入训练集后1个放入测试集(若为9:1)，然后训练时再进行shuffle，这样保证了分布均匀的问题</p></li></ol><h3 id="模型训练和验证"><a href="#模型训练和验证" class="headerlink" title="模型训练和验证"></a>模型训练和验证</h3><ol><li><p>仔细检查数据：</p><p>花时间去检查数据是一件比较重要的工作。因为数据中往往可能存在异常值，而且了解它们的分布可以有利于我们找到一个更好的模型。同时也可以对数据进行一开始的手动调整。</p></li><li><p>搭建模型并开始训练验证</p><p>评估框架提示</p><ul><li>固定随机种子：始终使用固定的随机种子来确保两次运行代码时，您将获得相同的结果。</li><li>简化：去除不必要的一些操作</li><li>验证损失：验证损失是否从正确的损失值开始</li><li>设定一个好的初始化</li><li>人类基线：监控除损失之外的指标，这些指标是人类可以解释和检查的（例如准确性）。尽可能评估自己（人类）的准确性并与之进行比较。</li><li>可视化预测动态。训练过程中可视化固定测试批次上的模型预测对模型调整有很大帮助。</li></ul></li><li><p>过度拟合</p></li></ol><p>找到一个好的模型的方法有两个阶段：首先获得一个足够大的模型以使其可以过度拟合（即专注于训练损失），然后适当地对其进行正则化（放弃一些训练损失以提高验证损失）。</p><p>此阶段的一些提示和技巧：</p><ul><li>选择模型：为了减少训练损失，您需要为数据选择合适的体系结构。</li><li>Adam是安全的。在设定基准的早期阶段，我喜欢以3e-4的学习率使用Adam 。以我的经验，亚当更宽容超参数，包括不良的学习速度。对于ConvNets，调整良好的SGD几乎总是比Adam稍胜一筹，但是最佳学习率区域要狭窄得多且针对特定问题。</li><li>一次只使一个复杂化。如果您有多个信号要插入您的分类器，我建议您将它们一个接一个地插入，并每次确保获得预期的性能提升。</li><li>不要相信学习率衰减的默认值。如果您要重新使用其他领域的代码，请务必小心学习率。</li></ul><p><strong>4. 正则化</strong></p><p>此阶段的一些提示和技巧：</p><ul><li>获取更多数据</li><li>数据扩充</li><li>创意增强：如果半假数据没有做到这一点，伪造数据也可能会有所作为。人们正在寻找扩展数据集的创新方法。例如，领域随机化，模拟的使用，巧妙的混合，例如将（潜在模拟的）数据插入场景，甚至GAN。</li><li>使用预训练网络</li><li>坚持监督学习</li><li>减小输入维数</li><li>减小模型尺寸</li><li>减小批量大小</li><li>Dropout</li><li>提早停止训练。根据您测得的验证损失提前停止训练，以在模型快要过拟合的时候捕获模型。</li><li>尝试更大的模型。大型模型大多数最终会过拟合，但是它们的“早期停止”性能通常会比小型模型好得多。</li></ul><p><strong>5. 微调</strong></p><p>此阶段的一些提示和技巧：</p><ul><li>随机网格搜索</li><li>超参数优化</li></ul><p><strong>6. 进一步提高精确率</strong></p><ul><li>模型集成</li></ul><p>代码参考搭建过的一些项目</p>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>github fork仓库向主工程提交代码</title>
      <link href="/2022/09/22/github-fork%E4%BB%93%E5%BA%93%E5%90%91%E4%B8%BB%E5%B7%A5%E7%A8%8B%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%81/"/>
      <url>/2022/09/22/github-fork%E4%BB%93%E5%BA%93%E5%90%91%E4%B8%BB%E5%B7%A5%E7%A8%8B%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h2 id="1-fork并关联本地"><a href="#1-fork并关联本地" class="headerlink" title="1. fork并关联本地"></a>1. fork并关联本地</h2><p>进入我的主页，找到这个仓库</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209220208551.png" alt="image-20220922020848487"></p><p>点击右上角的fork，然后你的主页里就多了一个同样的仓库了，相当于做了一个镜像开了个分支</p><p>然后在本地合适位置（最好别带中文）建立一个同名文件夹（名字不影响，但是为了一致嘛），然后在文件夹中打开git bash(path配置好了的话，powershell也可以)，然后按照如下流程输入（有梯子的话最好打开梯子）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 克隆fork后仓库到本地,yourname为你的github名</span></span><br><span class="line">git <span class="built_in">clone</span> （fork后的url）</span><br></pre></td></tr></table></figure><p>然后你的文件夹下就会出现本项目已有所有文件，然后你就可以在本地仓库的对应文件夹（你的名字）添加你的学习文件了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add到本地暂存区, .是add所有新文件的意思</span></span><br><span class="line">git add .</span><br><span class="line"></span><br><span class="line"><span class="comment"># commit到本地仓库</span></span><br><span class="line">git commit -m <span class="string">&quot;first_commit&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 关联到你的远程仓库</span></span><br><span class="line">git remote add origin your_url</span><br><span class="line"></span><br><span class="line"><span class="comment"># push到你的远程仓库</span></span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209220214651.webp" alt="img"></p><p>然后你fork的仓库会出现你的新增文件</p><h2 id="2-关联主工程"><a href="#2-关联主工程" class="headerlink" title="2.关联主工程"></a>2.关联主工程</h2><p>关联主工程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote add okex(自定义分支名) (主工程的git url)</span><br><span class="line"><span class="comment"># 查看关联情况</span></span><br><span class="line">git remote -v</span><br></pre></td></tr></table></figure><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209220224884.png" alt="在这里插入图片描述"></p><p>拉取主工程各分支信息到本地：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git fetch okex(自定义分支名)</span><br></pre></td></tr></table></figure><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209220224876.png" alt="在这里插入图片描述"></p><p>在本地切换到主分支的某分支（比如develop）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout develop</span><br></pre></td></tr></table></figure><p>在此分支的基础上创建一个自己的分支：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b michael.w</span><br></pre></td></tr></table></figure><p>开始做代码修改。</p><p>代码commit后向自己的repo push代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push</span><br></pre></td></tr></table></figure><p>这里可能报错，请根据报错内容自行纠正</p><p><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209220224883.png" alt="在这里插入图片描述"></p><ol><li>从自己的repo中向主工程发起request pull：<br><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209220224879.png" alt="在这里插入图片描述"><br>选择要提交的目标分支：<br><img src="https://haoming2003.oss-cn-hangzhou.aliyuncs.com/202209220224980.png" alt="在这里插入图片描述"></li></ol><h3 id="如何将主分支的更新进度同步到我的repo中"><a href="#如何将主分支的更新进度同步到我的repo中" class="headerlink" title="如何将主分支的更新进度同步到我的repo中"></a>如何将主分支的更新进度同步到我的repo中</h3><p>假设主工程的开发分支时main</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">切到本地的main分支</span></span><br><span class="line">git checkout main</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将okex的的main分支拉取下来并与本地现在所处分支合并</span></span><br><span class="line">git pull okex main</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">推到我的repo</span></span><br><span class="line">git push</span><br></pre></td></tr></table></figure><hr><blockquote><p><strong>本文参考了<a href="https://blog.csdn.net/michael_wgy_/article/details/104589800">wgy的博客</a>，侵删</strong></p></blockquote><blockquote><p><strong>由于github默认分支改变，以上master记得改为main</strong></p></blockquote><hr>]]></content>
      
      
      
        <tags>
            
            <tag> github代码提交 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>github+hexo+butterfly搭建博客</title>
      <link href="/2022/09/21/github-hexo-butterfly%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/2022/09/21/github-hexo-butterfly%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<hr><p>突然想到搭建一个博客玩，其实之前也在csdn上发过一点，但是没坚持下来，太失败了</p><p>希望这次可以坚持下来，下面记录一下搭建过程</p><hr><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><ol><li><p>github账号</p></li><li><p>nodejs, npm（版本别太低）</p></li></ol><p>上网搜具体的安装教程，肯定比我写得好</p><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><h4 id="创建username-github-io的项目"><a href="#创建username-github-io的项目" class="headerlink" title="创建username.github.io的项目"></a>创建<strong>username.github.io</strong>的项目</h4><p>（记住<strong>username</strong>跟你<strong>github</strong>名称同名）</p><p>在合适的地方新建一个文件夹，用来存放自己的博客文件，我的放在<code>D:\blog</code>下</p><p><strong>在该目录下</strong></p><h4 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装<strong>Hexo</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-cli -g</span><br></pre></td></tr></table></figure><p>可能会有几个报错，忽略</p><p>安装完后用 <strong>hexo -v</strong> 验证是否安装成功</p><h4 id="初始化并生成网页"><a href="#初始化并生成网页" class="headerlink" title="初始化并生成网页"></a><strong>初始化</strong>并生成网页</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br><span class="line"></span><br><span class="line">npm install <span class="comment"># 安装必备组件</span></span><br><span class="line"></span><br><span class="line">hexo g <span class="comment"># 生成静态网页</span></span><br><span class="line"></span><br><span class="line">hexo s <span class="comment"># 打开本地服务器,打开http://localhost:4000/,就有效果了</span></span><br></pre></td></tr></table></figure><p><strong>ctrl</strong> + <strong>c</strong>关闭本地服务器</p><h4 id="连接github和本地"><a href="#连接github和本地" class="headerlink" title="连接github和本地"></a>连接github和本地</h4><p>在根目录下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name <span class="string">&quot;HaomingX&quot;</span></span><br><span class="line">git config --global user.email <span class="string">&quot;978545377@qq.com&quot;</span></span><br><span class="line"><span class="comment"># 根据你注册github的信息替换成你自己的</span></span><br></pre></td></tr></table></figure><p>生成密钥SSH key</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;978545377@qq.com&quot;</span></span><br></pre></td></tr></table></figure><p>打开<a href="http://github.com/">github</a>，点击<code>settings</code>，再点击<code>SSH and GPG keys</code>，新建一个SSH，名字任意</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure><p>复制到ssh密匙框中，保存</p><p>输入<code>ssh -T git@github.com</code>，如果说了Hi 用户名!,你就成功了</p><p>打开博客根目录下的<code>_config.yml</code>文件，这是博客的配置文件</p><p>修改最后一行的配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repository: https://github.com/HaomingX/HaomingX.github.io</span><br><span class="line">  branch: main</span><br></pre></td></tr></table></figure><h4 id="写文章"><a href="#写文章" class="headerlink" title="写文章"></a>写文章</h4><p>根目录下安装扩展</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-deployer-git</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文章</span></span><br><span class="line">hexo new post <span class="string">&quot;文章名&quot;</span></span><br></pre></td></tr></table></figure><p>打开<code>D:\blog\source\_posts</code>的目录，可以发现下面多了一个<code>.md</code>文件</p><p>编写完后</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br><span class="line">hexo s</span><br><span class="line"></span><br><span class="line">hexo d <span class="comment"># 上传到github</span></span><br></pre></td></tr></table></figure><p>打开你的<a href="https://github.io/">github.io</a>主页就能看到发布的文章</p><h3 id="butterfly美化"><a href="#butterfly美化" class="headerlink" title="butterfly美化"></a>butterfly美化</h3><p><a href="https://tzy1997.com/articles/hexo1603/">可以跟这个博主的教程走，写得很好</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 博客搭建 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
